---
title: "Longitudinal Data Analysis in R (Part 1)"
output:
  html_document:
    df_print: paged
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = ">#")
```

## Load Packages and Import Data

```{r load-pkg, message=FALSE}
# To install a package, run the following ONCE (and only once on your computer)
# install.packages("psych")  
library(here)  # makes reading data more consistent
library(tidyverse)  # for data manipulation and plotting
library(haven)  # for importing SPSS/SAS/Stata data
library(brms)  # for Bayesian multilevel analysis
library(lattice)  # for dotplot (working with lme4)
library(sjPlot)  # for plotting effects
library(broom.mixed)  # for summarizing results
library(interactions)  # for plotting interactions
library(modelsummary)  # for making tables
# Add the following so that the LOO will be included in the msummary table
msummary_mixed <- function(models, coef_map = NULL, add_rows = NULL, ...) {
  if (is.null(coef_map)) {
    if (!"list" %in% class(models)) {
      models <- list(models)
    }
    for (model in models) {
      coef_map <- union(coef_map, tidy(model)$term)
    }
  }
  ranef_index <- grep("^(sd|cor)__", x = coef_map)
  coef_map <- c(coef_map[-ranef_index], coef_map[ranef_index])
  names(coef_map) <- coef_map
  rows <- data.frame(term = c("Fixed Effects", "Random Effects"))
  rows <- cbind(rows, rbind(rep("", length(models)), 
                            rep("", length(models))))
  length_fes <- length(coef_map) - length(ranef_index)
  attr(rows, 'position') <- c(1, (length_fes + 1) * 2)
  modelsummary::msummary(models, coef_map = coef_map, add_rows = rows, ...)
}
theme_set(theme_bw())  # Theme; just my personal preference
```

Because Bayesian estimation is a bit more intensive, you may want to set up parallel processing if your computers have two cores or more. 

```{r setup-brms}
# (Optional) Set multiple cores in global option
options(mc.cores = min(parallel::detectCores() - 1, 2L))
```

```{r import_sav, message=FALSE}
# Read in the data from gitHub (need Internet access)
curran_wide <- read_sav("https://github.com/MultiLevelAnalysis/Datasets-third-edition-Multilevel-book/raw/master/chapter%205/Curran/CurranData.sav")
# Make id a factor
curran_wide  # print the data
```

### Wide to Long

To perform multilevel analysis, we need to restructure the data from a wide
format to a long format:

```{r curran_long}
# Using the new `tidyr::pivot_longer()` function
curran_long <- curran_wide %>% 
  pivot_longer(
    c(anti1:anti4, read1:read4),  # variables that are repeated measures
    # Convert 8 columns to 3: 2 columns each for anti/read (.value), and 
    # one column for time
    names_to = c(".value", "time"),
    # Extract the names "anti"/"read" from the names of the variables for the 
    # value columns, and then the number to the "time" column
    names_pattern = "(anti|read)([1-4])", 
    # Convert the "time" column to integers
    names_transform = list(time = as.integer)
  )
curran_long %>% 
  select(id, anti, read, time, everything())
```

# Data Exploration

```{r pairs-data}
curran_long %>% 
    select(time, kidage, homecog, anti, read) %>% 
    psych::pairs.panels(jiggle = TRUE, factor = 0.5, ellipses = FALSE, 
                        cex.cor = 1, cex = 0.5)
```

What distinguishes longitudinal data from usual cross-sectional multilevel data
is the *temporal ordering*. In the example of students nested within schools, 
we don't say that student 1 is naturally before student 2, and it doesn't really
matter if one just reorder the students. However, in longitudinal data such
temporal sequence is very important, and we cannot simply rearrange the 
observation at time 2 to be at time 1. A related concern is the presence of
autocorrelation, with which observations closer in time will be more similar to 
each other. 

## Spaghetti Plot

```{r p1}
# Plotting
p1 <- ggplot(curran_long, aes(x = time, y = read)) + 
  geom_point() + 
  geom_line(aes(group = id)) +  # add lines to connect the data for each person
  # add a mean trajectory
  stat_summary(fun = "mean", col = "red", size = 1, geom = "line")
p1
```

We can see that on average there is an increasing trend across time, but also
there is a lot of variations in each individual's starting point and the 
trajectory of change. 

We can also plot the trajectory for a random sample of individuals:

```{r facet-traj}
curran_long %>% 
  # randomly sample 40 individuals
  filter(id %in% sample(unique(id), 40)) %>% 
  ggplot(aes(x = time, y = read)) + 
  geom_point() + 
  geom_line() +  # add lines to connect the data for each person
  facet_wrap( ~ id, ncol = 10)
```

## Temporal Covariances and Correlations

```{r cov-cor-read}
# Easier with the wide data set
# Covariance matrix:
curran_wide %>% 
  select(starts_with("read")) %>% 
  cov(use = "complete") %>%   # listwise deletion
  round(2L)  # two decimal places
# Correlation matrix
curran_wide %>% 
  select(starts_with("read")) %>% 
  cor(use = "complete") %>%   # listwise deletion
  round(2L)  # two decimal places
```

You can see the variances increase over time, and the correlation is generally
stronger for observations closer in time. 

## Attrition Analyses

To see whether those who dropped out or had missing data differed in some characteristics as those who did not drop out, let's identify the group with complete data on `read`, and the group without complete data on `read`

```{r mutate-complete}
# Create grouping
curran_wide <- curran_wide %>% 
  # Compute summaries by rows
  rowwise() %>% 
  # First compute the number of missing occasions
  mutate(nmis_read = sum(is.na(c_across(read1:read4))), 
         # Complete only when nmis_read = 0
         complete = if_else(nmis_read == 0, "complete", "incomplete")) %>% 
  ungroup()
# Compare the differences
datasummary((anti1 + read1 + kidgen + momage + kidage + homecog + homeemo) ~ 
              complete * (Mean + SD), data = curran_wide)
```

The two groups are pretty similar, except that the group with missing seem to have higher baseline antisocial score, and lower `homecog`. In actually analyses, you may want to adjust for these variables by including them in the model (as covariates), if you suspect the results may be confounded by higher probability of dropping out. 

Of course, the two groups could still be different in important characteristics that are not included in the data. It requires careful investigation of why the data were missing

# Unconditional Model

```{r m00, message=FALSE, cache=TRUE, results='hide'}
m00 <- brm(read ~ (1 | id), data = curran_long, 
           seed = 2152)
```

```{r summary-m00}
summary(m00)
```


```{r icc-m00}
post_samples <- posterior_samples(m00, pars = c("sd", "sigma"))
# Posterior of ICC: tau_0^2 / (tau_0^2 + sigma^2)
icc_samples <- post_samples$sd_id__Intercept^2 / 
  (post_samples$sd_id__Intercept^2 + post_samples$sigma^2)
# Posterior mean of ICC
mean(icc_samples)
```

# Linear Growth Models

## Linear Model With Time

Level 1: Within-Person

$$\text{read}_{ti} = \beta_{0i} + \beta_{1i} \text{time}_{ti} + e_{ti}$$

Level 2: Between-Person

$$
  \begin{aligned}
    \beta_{0i} = \gamma_{00} + u_{0i}  \\
    \beta_{1i} = \gamma_{10} + u_{1i}
  \end{aligned}
$$


```{r m_gca, results='hide', cache=TRUE}
# Make 0 the initial time
curran_long <- curran_long %>% 
  mutate(time = time - 1)
# Fit a linear growth model with no random slopes (not commonly done)
m_gca <- brm(read ~ time + (time | id), data = curran_long, 
             seed = 2155)
```

```{r summary-m_gca}
summary(m_gca)
```


```{r plot-m_gca}
random_persons <- sample(unique(curran_long$id), size = 24)
plot(
  conditional_effects(
    m_gca, 
    type = "pred", 
    re_formula = NULL, 
    conditions = tibble(id = random_persons)
  ), 
  points = TRUE, 
  ncol = 6  # six columns in the plot
)
```

# Piecewise Growth Model

A piecewise linear growth assumes that there are two or more phases of linear change across time. For example, we can assume that, for our data, phase 1 is from Time 0 to Time 1, and phase 2 is from Time 1 to Time 3. 

Because we're estimating two slopes, we need two predictors: `phase1` represents the initial slope from Time 0 to Time 1, and `phase2` represents the slope from Time 1 to Time 3. The coding is shown below:

$$\begin{bmatrix}
    \textrm{phase1} & \textrm{phase2} \\
    0 & 0 \\
    1 & 0 \\
    1 & 1 \\
    1 & 2
  \end{bmatrix}$$
  
To understand the coding, in `phase1` the line changes from Time 0 to Time 1, but stays there. `phase2` has 0 from Time 0 to Time 1 as nothing should have happened. Then, From Time 1 to Time 3 it starts to increase linearly. 

> One way to check whether you've specified the coding correctly is to sum the numbers in every row, which you should get back 0, 1, 2, 3, . . .

Below is an example where intercept = 1, growth in phase 1 = 0.5, growth in 
phase 2 = 0.8. The dashed line shows the contribution from phase1 (plus the intercept). 
The red dotted line shows the contribution from phase2. 

```{r piecewise_demo}
demo_df <- tibble(TIME = c(0, 1, 2, 3))
ggplot(demo_df, aes(x = TIME)) + 
  stat_function(fun = function(x) 1 + 0.5 * pmin(x, 1), 
                linetype = "dashed", size = 1.5) + 
  stat_function(fun = function(x) 0.8 * pmax(x - 1, 0), col = "red", 
                linetype = "dotted", size = 1.5) + 
  stat_function(fun = function(x) 1 + 0.5 * pmin(x, 1) + 0.8 * pmax(x - 1, 0))
```

Now, we assume Time 0 to Time 1 has one slope, and Time 1 to Time 3 has another slope.

```{r phase1phase2}
# Compute phase1 and phase2
curran_long <- curran_long %>% 
  mutate(phase1 = pmin(time, 1),  # anything bigger than 1 becomes 1
         phase2 = pmax(time - 1, 0))  # set time to start at TIME 1, and then make
# anything smaller than 0 to 0
# Check the coding:
curran_long %>% 
  select(time, phase1, phase2) %>% 
  distinct()
```


```{r m_pw, cache=TRUE, results='hide'}
# Fit the piecewise growth model
m_pw <- brm(read ~ phase1 + phase2 + (phase1 + phase2 | id), 
            data = curran_long,
            seed = 2152)
```

```{r summary-m_pw}
summary(m_pw)
```


```{r plot-m_pw}
# Because time is splitted into two pieces, 
random_persons <- sample(unique(curran_long$id), size = 24)
plot(
  conditional_effects(
    m_pw, 
    type = "pred", 
    re_formula = NULL, 
    conditions = tibble(id = random_persons)
  ), 
  points = TRUE, 
  ncol = 6  # six columns in the plot
)
```

### Plotting the predicted trajectories

Average predicted trajectory on the spaghetti plot:

```{r ave-traj-pw}
# Plot the predicted growth shape (in blue):
pred_pw <- tibble(time = 1:4, 
                  phase1 = c(0, 1, 1, 1), 
                  phase2 = c(0, 0, 1, 2))
pred_pw <- pred_pw %>% 
  mutate(read = predict(m_pw, newdata = pred_pw, 
                        re_formula = NA)[ , "Estimate"])
p1 + 
  geom_line(data = pred_pw, col = "blue", size = 2, alpha = 0.7)
```

Predicted trajectories at the individual level

```{r pred-traj}
# Add predicted lines to individual trajectories
augment(m_pw) %>% 
  # compute the original time
  mutate(time = phase1 + phase2) %>% 
  # randomly sample 40 individuals
  filter(id %in% sample(unique(id), 32)) %>% 
  ggplot(aes(x = time, y = read)) + 
  geom_point() + 
  geom_line() +  # add lines to connect the data for each person
  # add predicted line with `geom_line()`
  geom_line(aes(y = .fitted), col = "purple") + 
  facet_wrap( ~ id, ncol = 8)
```

# Time-Invariant Covariates

Time-invariant covariate = Another way to say a level-2 variable

You can add `homecog` and its interaction to the model with age as predictor. Just that now there are two interaction terms: one with the first piece and the other with the second piece.

```{r m_pw_homecog, cache=TRUE, results='hide'}
# Add `homecog` and its interaction with growth
# First, center homecog to 9
curran_long$homecog9 <- curran_long$homecog - 9
m_pw_homecog <- brm(read ~ (phase1 + phase2) * homecog9 + 
                      (phase1 + phase2 | id), 
                    data = curran_long,
                    seed = 2152)
```

```{r summary-m_pw_homecog}
summary(m_pw_homecog)
```

```{r interact-plot}
# Interaction plot (phase1:homecog)
interact_plot(m_pw_homecog, 
              pred = "phase1",
              modx = "homecog9",
              modx.values = c(1, 9, 14) - 9, 
              modx.labels = c(1, 9, 14), 
              plot.points = TRUE, 
              point.size = 0.5, 
              point.alpha = 0.2, 
              jitter = 0.02, 
              x.label = "Phase 1", 
              y.label = "Reading Score")
# Interaction plot (phase2:homecog)
interact_plot(m_pw_homecog, 
              pred = "phase2",
              modx = "homecog9",
              modx.values = c(1, 9, 14) - 9, 
              modx.labels = c(1, 9, 14), 
              plot.points = TRUE, 
              point.size = 0.5, 
              point.alpha = 0.2, 
              jitter = 0.02, 
              x.label = "Phase 2", 
              y.label = "Reading Score")
```

## Table of Coefficients

```{r tab-models}
msummary_mixed(list("Compound Symmetry" = m00, 
                    "GCA" = m_gca, 
                    "Piecewise" = m_pw, 
                    "Piecewise + homecog" = m_pw_homecog), 
               statistic = "conf.int", 
               looic = TRUE)
```

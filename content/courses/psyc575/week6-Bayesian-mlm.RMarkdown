---
title: "Bayesian Multilevel Models in R with `brms`"
output:
  html_document:
    df_print: paged
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = ">#")
```

## Load Packages and Import Data

```{r load-pkg, message=FALSE}
# To install a package, run the following ONCE (and only once on your computer)
# install.packages("psych")  
library(here)  # makes reading data more consistent
library(tidyverse)  # for data manipulation and plotting
library(haven)  # for importing SPSS/SAS/Stata data
library(brms)  # for Bayesian multilevel analysis
library(lattice)  # for dotplot (working with lme4)
library(sjPlot)  # for plotting effects
library(broom.mixed)  # for summarizing results
library(modelsummary)  # for making tables
# Add the following so that the LOO will be included in the msummary table
msummary_mixed <- function(models, coef_map = NULL, ...) {
  if (is.null(coef_map)) {
    if (!"list" %in% class(models)) {
      models <- list(models)
    }
    for (model in models) {
      coef_map <- union(coef_map, tidy(model)$term)
    }
  }
  ranef_index <- grep("^(sd|cor)__", x = coef_map)
  coef_map <- c(coef_map[-ranef_index], coef_map[ranef_index])
  names(coef_map) <- coef_map
  modelsummary::msummary(models, coef_map = coef_map, ...)
}
glance_custom.brmsfit <- function(x) {
  broom.mixed::glance(x, looic = TRUE)
}
theme_set(theme_bw())  # Theme; just my personal preference
```

Because Bayesian estimation is a bit more intensive, you may want to set up parallel processing if your computers have two cores or more. 

```{r setup-brms}
# (Optional) Set multiple cores in global option
options(mc.cores = min(parallel::detectCores() - 1, 2L))
```

```{r import_sav, message=FALSE}
# Read in the data (pay attention to the directory)
hsball <- read_sav(here("data_files", "hsball.sav"))
hsball  # print the data
```

Once again I will use a subset of 16 schools, which keep the computational time shorter (the computational time with Bayesian depends on the size of the data and the complexity of the model).

```{r hsbsub}
# Randomly select 16 school ids
set.seed(840)  # use the same seed so that the same 16 schools are selected
random_ids <- sample(unique(hsball$id), size = 16)
hsbsub <- hsball %>%
    filter(id %in% random_ids)
```

# Basic Models

```{r cmc}
hsbsub <- hsbsub %>% 
  group_by(id) %>%   # operate within schools
  mutate(ses_cm = mean(ses),   # create cluster means (the same as `meanses`)
         ses_cmc = ses - ses_cm) %>%   # cluster-mean centered
  ungroup()  # exit the "editing within groups" mode
```

```{r ran_int, results='hide', echo=TRUE, message=FALSE, cache=TRUE}
# You simply need to replace `lmer()` with `brm()`
# This takes about a minute to compile, and about 20 seconds for sampling
ran_int <- brm(mathach ~ 1 + (1 | id), data = hsbsub, 
               # I used 2 chains (instead of 4, the default) for simpler models
               chains = 2)
summary(ran_int)
# With a level-2 predictor
m_mses <- brm(mathach ~ meanses + (1 | id), data = hsbsub, 
              chains = 2)
summary(m_mses)
```

# Example: Random-Coefficients Model

## Default priors from `brms`:

```{r prior-ran_slp}
get_prior(mathach ~ meanses + ses_cmc + (ses_cmc | id), 
          data = hsbsub)
```

- Flat (Uniform) priors on the $\gamma$s (i.e., `class = b`), except for the intercept
- Weakly informative half-$t$ prior on $\tau_0$ and $\tau_1$ (it's "half-" because $\tau_0$ and $\tau_1$ are non-negative)
- Flat (Uniform) prior on the correlation between the random intercepts and the random slopes (`lkj(1)`)

```{r ran_slp, message=FALSE, cache=TRUE}
# This takes a few minutes. Use the `cache=TRUE` option to avoid rerunning
# when knitting the Rmd. 
ran_slp <- brm(mathach ~ meanses + ses_cmc + (ses_cmc | id), 
               data = hsbsub, 
               seed = 1429)  # setting the seed makes the results reproducible
```

You can summarize the results using
```{r summary-ran_slp}
summary(ran_slp)
```
which shows the posterior means (labelled `Estimate`) and SDs (labelled `Est.Error`).

## Plot Posterior Density

```{r plot-ran_int, fig.asp=.3}
plot(ran_slp, N = 1)
```

## Convergence

- Some minimum requirements
    * Rhat < 1.01 for all parameters
    * No divergence transition
- In addition
    * The Bulk effective sample size (ESS) and the tail ESS are sufficient (you will get a warning if they are too low)
- If convergence not achieved:
    * Follow the suggestions from `brms`
    * Make sure the model is identified
    * Run more iterations (e.g., `iter = 2000` to `iter = 4000`)
    * Increase `adapt_delta` to closer to 1 (.9, .95, .99, .999, etc)
    * Use stronger priors (especially in smaller samples)

### Sample language for describing the Bayesian analysis

> The multilevel models were fitted using the `brms` package [(BÃ¼rkner, 2017)](https://doi.org/10.18637/jss.v080.i01) in R, which performs Markdov Chain Monte Carlo approximation with the No U-Turn Sampler to approximate the posterior distributions of the model parameters. For each model, we used 4 chains, each with 2,000 iterations (1,000 warmup). The default priors from `brms` were used, which include uniform non-informative priors on the fixed-effect parameters and weakly informative half-Student-$t$ priors on the standard deviations of the random effects (i.e., $\tau$s and $\sigma$). For all models, $\hat R < 1.01$ [(Vehtari et al., 2020)](https://doi.org/10.1214/20-BA1221), indicating convergence of the chains to a stationary posterior distributions. The posterior distributions of the model parameters are summarized using the posterior means and the 95% equal-tailed credible intervals. 

See also the [JARS Table by APA](https://apastyle.apa.org/jars/quant-table-8.pdf)

## Posterior Predictive Check

A handy function to check whether the distributional assumption of the model is reasonable

```{r pp_check-ran_slp, fig.width=4, fig.height=3}
pp_check(ran_slp)
```

The plot above shows the model prediction (in lighter lines, labelled y~rep~) is a bit off compared to the original outcome variable (labelled y in a darker line). This is most likely due to the outcome, `mathach`, having a maximum of around 25. This means that the model can be improved by relaxing the normality assumption. The inference, however, should still be okay with this relatively mild misspecification, as the fixed-effect coefficients are generally robust to a mild degree of violations of the normality assumption (e.g., [Maas & Hox, 2004](https://dspace.library.uu.nl/bitstream/handle/1874/30372/sn04.pdf?sequence=2)). 

## Model comparisons

You can use the `loo()` function to compare the models, which computes the leave-one-out cross-validation criterion (LOO; which we'll further discuss when we talk about information criteria). For now, just know that the model with a smaller LOO should be preferred.

Let's compared the model with and without random slopes

```{r m_bw, cache=TRUE}
# Without random slopes
m_bw <- brm(mathach ~ meanses + ses_cmc + (1 | id), 
            data = hsbsub, 
            seed = 1429)
```

Compare their LOO

```{r loo-ran_slp}
loo(ran_slp, m_bw)
```

The LOO is lower for the model with random slopes, indicating that random slopes should be included. In general with Bayesian modeling, it's recommended that you include all random slopes, as it does not usually have convergence issues as with frequentist methods. 

## Plotting the conditional effects

```{r plot-ran-slp}
# Between and average within
plots_ran_slp <- plot(
  conditional_effects(ran_slp, type = "pred"), 
  points = TRUE, 
  point_args = list(size = 0.5), 
  plot = FALSE
)
# Plot the graphs together (need the gridExtra package)
gridExtra::grid.arrange(grobs = plots_ran_slp, ncol = 2)
```

## Tabulate

You can again use `msummary_mixed()`

```{r tab-models}
msummary_mixed(list("Unconditional" = ran_int, 
                    "meanses" = m_mses, 
                    "Random slope" = ran_slp), 
               # `statistic = "conf.int"` show the credible intervals
               statistic = "conf.int")
```

# Using `brms` to Relax Assumptions

## Heteroscedasticity

### Level-1

You can allow $\sigma$ to be different across clusters. If heteroscedasticity is present, it may affect power using regular MLM that assumes homoscedasticity. 

```{r m_mses_heter, results='hide', echo=TRUE, message=FALSE, cache=TRUE}
# This took about 3 minutes on my computer
m_mses_heter <- brm(
  bf(mathach ~ meanses + (1 | c | id), 
     sigma ~ (1 | c | id)), 
  data = hsbsub)
```

```{r summ-m_mses_heter}
summary(m_mses_heter)
```

#### Model comparisons

```{r loo-heter, cache=TRUE}
# `msummary` currently does not support models with random sigma. 
loo(m_mses, m_mses_heter)
# Compare the fixed effects
fixef(m_mses)
fixef(m_mses_heter)  # smaller estimate, with larger posterior SD
```

So the results indicate that allowing $\sigma$ to vary across clusters should be preferred, and it affects the fixed effect estimate.

### Level-2

Allowing the $\tau$s to be different across types of schools (`sector`): 

```{r crlv_int_heter, results='hide', echo=TRUE, message=FALSE, cache=TRUE}
# Regular model
crlv_int <- brm(mathach ~ meanses + sector * ses_cmc +
                  (ses_cmc | id),
                data = hsbsub)
# Allowing separate variances for public and private schools
crlv_int_heter <- brm(mathach ~ meanses + sector * ses_cmc +
                        (ses_cmc | gr(id, by = sector)),
                      data = hsbsub)
```

```{r tab-heter2}
msummary_mixed(list("Equal var" = crlv_int, 
                    "Heter var" = crlv_int_heter), 
               statistic = "conf.int")
```

The results indicated that the intercept variance might be larger for Catholic schools than for public schools, but the LOO did not provide particularly strong evidence as it was lower for the model assuming equal variances across sectors. 

## Outlier-Robust Model

If there are potential outliers in the data, it may affect the parameter estimates in the model. This tends to be less a problem when the sample size is large, but if you suspect that your model may be affected by the presence of outliers, you can replace the normality assumption with the assumption that the level-1 and level-2 deviations (i.e., the $e_{ij}$s and the $u$s) follow a Student's $t$ distribution instead. The $t$ distribution has heavier tails than a normal distribution, and will converge to a normal distribution when the degrees of freedom parameter ($\nu$) is large (larger than 30). 

With `brms`, we can specify a Student's $t$ model and estimate the degrees of freedom; if the results are not affected by outliers, you will see that the estimates and credible intervals from the two models are similar, which give you more confidence on your original model results. If the results look different, then you may want to evaluate whether some observations have unusual values that are correctable (such as coding error). If there are some true outliers, then you can present results on the Student's $t$ model. 

```{r crlv_int_t, results='hide', echo=TRUE, message=FALSE, cache=TRUE}
# Allowing separate variances for public and private schools
# This takes a minute
crlv_int_t <- brm(mathach ~ meanses + sector * ses_cmc +
                    (ses_cmc | gr(id, dist = "student")),
                  family = student(),
                  data = hsbsub,
                  # Increase adapt_delta (from .8 to .9, .95, .99, .995, etc)
                  # due to divergent transition
                  control = list(adapt_delta = .995),
                  seed = 1706)
# Original warnings:
# Warning messages:
# There were 13 divergent transitions after warmup. See
# http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup
# to find out why this is a problem and how to eliminate them.Examine the pairs() plot to diagnose sampling problems
# Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
# Running the chains for more iterations may help. See
# http://mc-stan.org/misc/warnings.html#bulk-ess
```

```{r tab-student-t}
msummary_mixed(list("Normal" = crlv_int, 
                    "Student-t" = crlv_int_t), 
               statistic = "conf.int")
```

The results were almost identical with the two models, and LOO did not indicate that using a Student's $t$ distribution improves the fit. So it suggests our results may not be affected by the presence of potential outliers. 

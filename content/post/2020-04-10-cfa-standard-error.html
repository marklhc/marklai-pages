---
title: Asymptotic Standard Errors in CFA
author: Mark Lai
date: '2020-04-11'
slug: cfa-standard-error
output:
  blogdown::html_page:
    toc: true
categories:
  - Statistics
tags:
  - SEM
  - R
references:
- id: Maydeu-Olivares2017
  title: 'Maximum Likelihood Estimation of Structural Equation Models for Continuous Data: Standard Errors and Goodness of Fit'
  author:
  - family: 'Maydeu-Olivares'
    given: 'Alberto'
  container-title: 'Structural Equation Modeling: A Multidisciplinary Journal'
  volume: 24
  URL: 'http://doi.org/10.1080/10705511.2016.1269606'
  DOI: 10.1080/10705511.2016.1269606
  issue: 3
  page: 383-394
  type: article-journal
  issued:
    year: 2017
header-includes:
- \DeclareMathOperator{\E}{E}
- \DeclareMathOperator{\Cov}{Cov}
- \DeclareMathOperator{\Var}{Var}
- \newcommand{\SD}{\mathit{SD}}
- \newcommand{\SE}{\mathit{SE}}
- \DeclareMathOperator{\MSE}{MSE}
- \DeclareMathOperator{\RE}{RE}
- \DeclareMathOperator{\tr}{tr}
- \DeclareMathOperator{\acov}{acov}
- \DeclareMathOperator{\vect}{vec}
- \newcommand{\bv}[1]{\boldsymbol{\mathbf{#1}}}
---













<div id="TOC">
<ul>
<li><a href="#define-true-model-and-simulate-some-data">Define True Model and Simulate Some Data</a></li>
<li><a href="#define-log-likelihood-function">Define Log-Likelihood Function</a><ul>
<li><a href="#defining-mathcallboldsymbolmathbfsigma-s-in-r">Defining <span class="math inline">\(\mathcal{l}(\boldsymbol{\mathbf{\Sigma}}; S)\)</span> in R:</a></li>
</ul></li>
<li><a href="#mle">MLE</a></li>
<li><a href="#asymptotic-standard-errors">Asymptotic Standard Errors</a><ul>
<li><a href="#using-expected-information">Using expected information</a></li>
<li><a href="#observed-information-using-hessian">Observed information (using Hessian)</a></li>
</ul></li>
<li><a href="#mlmmlmv">MLM/MLMV</a></li>
<li><a href="#mlr">MLR</a></li>
<li><a href="#bibliography">Bibliography</a></li>
</ul>
</div>

<p>In our lab meeting I’m going to present the article by <span class="citation">Maydeu-Olivares (2017)</span>, which talked about standard errors (SEs) of the maximum likelihood estimators (MLEs) in SEM models. As I’m also working on something relevant, and I haven’t found some good references (I’m sure there are some, just too lazy to do a deep search), I decided to write this demo to show how the SEs can be obtained in R and compared that to the <code>lavaan</code> package, which automates these computations.</p>
<div id="define-true-model-and-simulate-some-data" class="section level2">
<h2>Define True Model and Simulate Some Data</h2>
<p>To keep things simple, I’ll use a one-factor model with four indicators (without mean structure):</p>
<p><span class="math display">\[\mathbf{y} = \boldsymbol{\Lambda} \boldsymbol{\eta} + \boldsymbol{\varepsilon}\]</span></p>
<ul>
<li><span class="math inline">\(\mathbf{y}\)</span> = observed scores</li>
<li><span class="math inline">\(\boldsymbol{\Lambda}\)</span> = loading matrix (vector in this case)</li>
<li><span class="math inline">\(\boldsymbol{\eta}\)</span> = latent score (true score)</li>
<li><span class="math inline">\(\boldsymbol{\varepsilon}\)</span> = random measurement error/unique factor scores</li>
</ul>
<pre class="r"><code>library(lavaan)</code></pre>
<pre><code>&gt;# This is lavaan 0.6-5</code></pre>
<pre><code>&gt;# lavaan is BETA software! Please report any bugs.</code></pre>
<pre class="r"><code>LAM &lt;- rep(.7, 4)  # loading vector
THETA &lt;- diag(1 - LAM^2)
N &lt;- 100
set.seed(1)  # Set seed</code></pre>
<p>I’ll simulate data that follows the normal distribution:</p>
<pre class="r"><code>y &lt;- mvnfast::rmvn(n = N, mu = rep(0, 4), sigma = tcrossprod(LAM) + THETA)
# # Check kurtosis
# apply(y, 2, e1071::kurtosis)</code></pre>
</div>
<div id="define-log-likelihood-function" class="section level2">
<h2>Define Log-Likelihood Function</h2>
<p>The model parameters are <span class="math inline">\((\boldsymbol{\lambda}, \boldsymbol{\theta})\)</span>, which include four loadings and four uniquenesses, with latent factor variance fixed to 1. Assuming normality, the sample covariance matrix, <span class="math inline">\(\mathbf{S}\)</span> (<code>Sy</code> in the code), is sufficient statistic:</p>
<pre class="r"><code># Sample covariance S
(Sy &lt;- cov(y) * (N - 1) / N)</code></pre>
<pre><code>&gt;#           [,1]      [,2]      [,3]      [,4]
&gt;# [1,] 0.9962082 0.4808498 0.3465193 0.3733755
&gt;# [2,] 0.4808498 1.0076039 0.4208120 0.5062814
&gt;# [3,] 0.3465193 0.4208120 0.9332648 0.3749744
&gt;# [4,] 0.3733755 0.5062814 0.3749744 0.9388328</code></pre>
<p>Note that I divided by <span class="math inline">\(N\)</span> instead of <span class="math inline">\(N - 1\)</span>, which is the default for <code>lavaan</code> (<span class="math inline">\(N - 1\)</span> corresponds to <a href="http://lavaan.ugent.be/tutorial/est.html">Wishart likelihood</a>). When normality holds, <span class="math inline">\(\mathbf{S}\)</span> follows a <a href="https://en.wikipedia.org/wiki/Wishart_distribution">Wishart distribution</a> with degrees of freedom <span class="math inline">\(N\)</span>. Denote <span class="math inline">\(\boldsymbol{\mathbf{\Sigma }}= \boldsymbol{\mathbf{\Sigma}}(\boldsymbol{\lambda}, \boldsymbol{\theta})\)</span> as the model-implied covariance matrix. The likelihood function is
<span class="math display">\[\mathcal{L}(\boldsymbol{\mathbf{\Sigma}}; S) \propto 
  \frac{\exp\left[- \frac{1}{2}\mathop{\mathrm{tr}}(\boldsymbol{\mathbf{\Sigma}}^{-1} N \boldsymbol{\mathbf{S}})\right]}
       {|\Sigma|^\frac{N}{2}}\]</span>
and so the log-likelihood function is
<span class="math display">\[\mathcal{l}(\boldsymbol{\mathbf{\Sigma}}; S) =
  - \frac{N}{2}\left[\mathop{\mathrm{tr}}(\boldsymbol{\mathbf{\Sigma}}^{-1} \boldsymbol{\mathbf{S}}) + 
                     \log |\Sigma|\right] + C\]</span>
where <span class="math inline">\(C\)</span> is some constant that does not involve <span class="math inline">\(\boldsymbol{\mathbf{\Sigma}}\)</span>.</p>
<div id="defining-mathcallboldsymbolmathbfsigma-s-in-r" class="section level3">
<h3>Defining <span class="math inline">\(\mathcal{l}(\boldsymbol{\mathbf{\Sigma}}; S)\)</span> in R:</h3>
<p>This is pretty straight forward in R</p>
<pre class="r"><code>loglikSigma &lt;- function(pars, S = Sy, N = 100) {
  # pars: First four as lambdas, then thetas
  Sigma &lt;- tcrossprod(pars[1:4]) + diag(pars[5:8])
  - N / 2 * (determinant(Sigma)$modulus[1] + 
       sum(diag(solve(Sigma, S))))
}</code></pre>
</div>
</div>
<div id="mle" class="section level2">
<h2>MLE</h2>
<p>In maximum likelihood estimation, we find the parameter values that would maximize the above log-likelihood function (as log is monotonic). As an example, we consider these two sets of estimates:</p>
<pre class="r"><code># Set 1
lam1 &lt;- c(.6, .6, .8, .7); theta1 &lt;- c(.5, .4, .4, .5)
loglikSigma(c(lam1, theta1))</code></pre>
<pre><code>&gt;# [1] -158.8054</code></pre>
<pre class="r"><code># Set 2
lam2 &lt;- c(.7, .7, .8, .7); theta2 &lt;- c(.5, .5, .4, .3)
loglikSigma(c(lam2, theta2))</code></pre>
<pre><code>&gt;# [1] -161.6866</code></pre>
<p>The first set gives higher log-likelihood (i.e., less negative) value, so should be preferred. We can try many sets of values until we find the best by hand, but it is very tedious. So instead we rely on some optimizers to help us achieve that. In R, some general-purpose optimizer can be found in the package <code>optim</code>.</p>
<p>One thing to note before we start. Usually optimizers help us find parameters that <em>minimizes</em> an objective function, but here we want to <em>maximizes</em> it. So one trick is to instead <em>minimize</em> the -2 <span class="math inline">\(\times\)</span> log-likelihood:</p>
<pre class="r"><code>minus2lSigma &lt;- function(pars, S = Sy, N = 100) {
  # pars: First four as lambdas, then thetas
  Sigma &lt;- tcrossprod(pars[1:4]) + diag(pars[5:8])
  N * (determinant(Sigma)$modulus[1] + 
         sum(diag(solve(Sigma, S))))
}</code></pre>
<pre class="r"><code>minus2lSigma(c(lam1, theta1))</code></pre>
<pre><code>&gt;# [1] 317.6109</code></pre>
<pre class="r"><code>minus2lSigma(c(lam2, theta2))</code></pre>
<pre><code>&gt;# [1] 323.3733</code></pre>
<p>So again the first set is preferred with lower -2 log-likelihood. Now we use <code>optim</code>, with the “L-BFGS-B” algorithm. It requires some starting values, so I put it the values of set 1 (you can verify that the results are the same with set 2 as starting values):</p>
<pre class="r"><code>opt_info &lt;- optim(c(lam1, theta1), minus2lSigma, 
                  lower = c(rep(-Inf, 4), rep(0, 4)), 
                  method = &quot;L-BFGS-B&quot;, hessian = TRUE)
# Maximum likelihood estimates:
opt_info$par</code></pre>
<pre><code>&gt;# [1] 0.6068613 0.7783132 0.5576544 0.6467392 0.6279234 0.4018284 0.6222836
&gt;# [8] 0.5205601</code></pre>
<p>So in terms of point estimates, that’s it! Let’s compare the results to <code>lavaan</code>:</p>
<pre class="r"><code>fit1 &lt;- lavaan::cfa(model = &#39;f =~ y1 + y2 + y3 + y4&#39;, 
                    std.lv = TRUE, 
                    # Note: lavaan transform S automatically by multiplying 
                    # (N - 1) / N
                    sample.cov = `colnames&lt;-`(cov(y), paste0(&quot;y&quot;, 1:4)), 
                    sample.nobs = 100)</code></pre>
<pre class="r"><code>cbind(optim = opt_info$par, 
      `lavaan::cfa` = lavaan::coef(fit1))</code></pre>
<pre><code>&gt;#            optim lavaan::cfa
&gt;# f=~y1  0.6068613   0.6068697
&gt;# f=~y2  0.7783132   0.7783153
&gt;# f=~y3  0.5576544   0.5576568
&gt;# f=~y4  0.6467392   0.6467406
&gt;# y1~~y1 0.6279234   0.6279174
&gt;# y2~~y2 0.4018284   0.4018292
&gt;# y3~~y3 0.6222836   0.6222836
&gt;# y4~~y4 0.5205601   0.5205593</code></pre>
<p>So they are essentially the same (the difference in rounding is mainly due to different optimizer and starting values).</p>
</div>
<div id="asymptotic-standard-errors" class="section level2">
<h2>Asymptotic Standard Errors</h2>
<p>Now onto standard errors, which is the main focus of <span class="citation">Maydeu-Olivares (2017)</span>. The paper also talk about sandwich estimators which are robust (to some degree) to nonnormality, but I will focus on the ones with observed and expected information first, and then show how to obtain MLM and MLR standard errors in plain R (to some degree).</p>
<p>Although in the statistics literature, generally observed information is preferred over the use of expected information (both of which related to the second partial derivatives of the log-likelihood function, see <a href="https://en.wikipedia.org/wiki/Observed_information">here</a>), in <code>lavaan</code> the default is to use the expected information, and so I will demonstrate that first.</p>
<div id="using-expected-information" class="section level3">
<h3>Using expected information</h3>
<p>As described in the paper, the asymptotic covariance matrix of the MLE, which estimates how the estimates vary and covary across repeated samples, has this formula:
<span class="math display">\[\mathop{\mathrm{acov}}(\hat{\boldsymbol{\mathbf{\lambda}}}, \hat{\boldsymbol{\mathbf{\theta}}}) = 
  N^{-1} (\boldsymbol{\mathbf{\Delta}}&#39; \boldsymbol{\mathbf{\Gamma}}^{-1}_E \boldsymbol{\mathbf{\Delta}})^{-1}\]</span>
where
<span class="math display">\[\boldsymbol{\mathbf{\Gamma}}^{-1}_E = \frac{1}{2} (\boldsymbol{\mathbf{\Sigma}}^{-1} \otimes \boldsymbol{\mathbf{\Sigma}}^{-1})\]</span>
is the expected information matrix (which will be evaluated as the MLE). Note that the paper also talked about the duplication matrix (which potentially increases computational efficiency), but for simplicity I left it out as we don’t have a huge matrix here.</p>
<!-- The duplication matrix $\bv D$ transforms the elements in $\bv \Sigma$ so that it only contains the unique (non-repeated) elements. For example,  -->
<p>TODO: Add the explanation on the terms</p>
<p>The asymptotic standard errors are the square root values of the diagonal elements in <span class="math inline">\(\mathop{\mathrm{acov}}(\hat{\boldsymbol{\mathbf{\lambda}}}, \hat{\boldsymbol{\mathbf{\theta}}})\)</span>. After we obtained the MLEs, we can easily construct the model-implied covariance <span class="math inline">\(\Sigma\)</span>, and let R do the inverse and the Kronecker product (there are computational shortcuts in this example, but not essential in this demo). The <span class="math inline">\(\boldsymbol{\mathbf{\Delta}}\)</span> matrix contains partial derivatives. As an example, with
<span class="math display">\[\boldsymbol{\mathbf{\Sigma }}= \begin{bmatrix}
                 \lambda_1^2 + \theta_1 &amp; &amp; &amp; \\
                 \lambda_1 \lambda_2 &amp; \lambda_2^2 + \theta_2 &amp; &amp; \\
                 \lambda_1 \lambda_3 &amp; \lambda_2 \lambda_3 &amp; 
                   \lambda_3^2 + \theta_3 &amp; \\
                 \lambda_1 \lambda_4 &amp; \lambda_2 \lambda_4 &amp; 
                   \lambda_3 \lambda_4 &amp; \lambda_4^2 + \theta_4 \\
               \end{bmatrix},\]</span>
if we align the elements in <span class="math inline">\(\Sigma\)</span> to be a vector, denoted as <span class="math inline">\(\mathop{\mathrm{vec}}(\Sigma)\)</span>, we have
<span class="math display">\[\mathop{\mathrm{vec}}(\Sigma) = [\lambda_1^2 + \theta_1, \lambda_1 \lambda_2, \lambda_1 \lambda_3, \cdots]&#39;, \]</span>
so
<span class="math display">\[\boldsymbol{\mathbf{\Delta }}= \begin{bmatrix}
                 \frac{\partial (\lambda_1^2 + \theta_1)}{\partial \lambda_1} &amp; 
                 \frac{\partial (\lambda_1^2 + \theta_1)}{\partial \lambda_2} &amp; \cdots \\ 
                 \frac{\partial (\lambda_1 \lambda_2)}{\partial \lambda_1} &amp; 
                 \frac{\partial (\lambda_1 \lambda_2)}{\partial \lambda_2} &amp; \cdots \\
                 \vdots &amp; \ddots &amp; \vdots \\
               \end{bmatrix}, \]</span>
which is a <span class="math inline">\(p^2 \times r\)</span> matrix where <span class="math inline">\(r\)</span> is the number of parameters (8 in this case). This can be obtained by hand, but we can also use the <code>numDeriv::jacobian()</code> function like:</p>
<pre class="r"><code># Using parameter estimates from optim to construct Sigma
(Sigmahat &lt;- tcrossprod(opt_info$par[1:4]) + diag(opt_info$par[5:8]))</code></pre>
<pre><code>&gt;#           [,1]      [,2]      [,3]      [,4]
&gt;# [1,] 0.9962040 0.4723281 0.3384188 0.3924810
&gt;# [2,] 0.4723281 1.0075997 0.4340297 0.5033656
&gt;# [3,] 0.3384188 0.4340297 0.9332620 0.3606569
&gt;# [4,] 0.3924810 0.5033656 0.3606569 0.9388317</code></pre>
<pre class="r"><code># Make it into a function, outputting vec(Sigma)
vecSigma &lt;- function(pars) {
  as.vector(tcrossprod(pars[1:4]) + diag(pars[5:8]))
}
# Numeric derivative:
(Delta &lt;- numDeriv::jacobian(vecSigma, opt_info$par))</code></pre>
<pre><code>&gt;#            [,1]      [,2]      [,3]      [,4] [,5] [,6] [,7] [,8]
&gt;#  [1,] 1.2137226 0.0000000 0.0000000 0.0000000    1    0    0    0
&gt;#  [2,] 0.7783132 0.6068613 0.0000000 0.0000000    0    0    0    0
&gt;#  [3,] 0.5576544 0.0000000 0.6068613 0.0000000    0    0    0    0
&gt;#  [4,] 0.6467392 0.0000000 0.0000000 0.6068613    0    0    0    0
&gt;#  [5,] 0.7783132 0.6068613 0.0000000 0.0000000    0    0    0    0
&gt;#  [6,] 0.0000000 1.5566263 0.0000000 0.0000000    0    1    0    0
&gt;#  [7,] 0.0000000 0.5576544 0.7783132 0.0000000    0    0    0    0
&gt;#  [8,] 0.0000000 0.6467392 0.0000000 0.7783132    0    0    0    0
&gt;#  [9,] 0.5576544 0.0000000 0.6068613 0.0000000    0    0    0    0
&gt;# [10,] 0.0000000 0.5576544 0.7783132 0.0000000    0    0    0    0
&gt;# [11,] 0.0000000 0.0000000 1.1153087 0.0000000    0    0    1    0
&gt;# [12,] 0.0000000 0.0000000 0.6467392 0.5576544    0    0    0    0
&gt;# [13,] 0.6467392 0.0000000 0.0000000 0.6068613    0    0    0    0
&gt;# [14,] 0.0000000 0.6467392 0.0000000 0.7783132    0    0    0    0
&gt;# [15,] 0.0000000 0.0000000 0.6467392 0.5576544    0    0    0    0
&gt;# [16,] 0.0000000 0.0000000 0.0000000 1.2934784    0    0    0    1</code></pre>
<p>Now, we’re ready to compute the asymptotic standard errors:</p>
<pre class="r"><code>invSigmahat &lt;- solve(Sigmahat)
# Expected Fisher information
exp_I &lt;- invSigmahat %x% invSigmahat / 2
# Asymptotic covariance
acov_exp &lt;- solve(crossprod(Delta, exp_I) %*% Delta) / N
# Asymptotic SE
ase_exp &lt;- sqrt(diag(acov_exp))</code></pre>
<p>Compared to <code>lavaan</code>:</p>
<pre class="r"><code>cbind(optim = ase_exp, 
      `lavaan::cfa (&#39;expected&#39;)` = sqrt(diag(lavaan::vcov(fit1))))</code></pre>
<pre><code>&gt;#             optim lavaan::cfa (&#39;expected&#39;)
&gt;# f=~y1  0.10400371               0.10400353
&gt;# f=~y2  0.10242249               0.10242227
&gt;# f=~y3  0.10139392               0.10139383
&gt;# f=~y4  0.09991077               0.09991052
&gt;# y1~~y1 0.10889211               0.10889173
&gt;# y2~~y2 0.10757581               0.10757529
&gt;# y3~~y3 0.10420294               0.10420288
&gt;# y4~~y4 0.09956021               0.09955978</code></pre>
<p>So that matches pretty well!</p>
</div>
<div id="observed-information-using-hessian" class="section level3">
<h3>Observed information (using Hessian)</h3>
<p>While <span class="citation">Maydeu-Olivares (2017)</span> also presented the formula (in matrix form) for the asymptotic covariance estimates using the expected information, in practice such a matrix is usually obtained based on the <a href="https://en.wikipedia.org/wiki/Hessian_matrix"><strong>Hessian</strong></a>, which usually is part of the output in the maximization algorithm in MLE. For example, going back to the output of the <code>optim()</code> call,</p>
<pre class="r"><code># As we&#39;ve multiplied by -2 before, we need to divide by 2 now.
# The Hessian is not affected by multiplication of -1
acov_obs &lt;- solve(opt_info$hessian / 2)
(ase_obs &lt;- sqrt(diag(solve(opt_info$hessian / 2))))</code></pre>
<pre><code>&gt;# [1] 0.10381614 0.10240102 0.10189012 0.09983351 0.10862886 0.10752628 0.10480306
&gt;# [8] 0.09943057</code></pre>
<p>And compare to <code>lavaan</code> using the <code>information = 'observed'</code> option:</p>
<pre class="r"><code>fit1_obs &lt;- lavaan::update(fit1, information = &#39;observed&#39;)
cbind(optim = ase_obs, 
      `lavaan::cfa (&#39;obs&#39;)` = sqrt(diag(lavaan::vcov(fit1_obs))))</code></pre>
<pre><code>&gt;#             optim lavaan::cfa (&#39;obs&#39;)
&gt;# f=~y1  0.10381614          0.10381636
&gt;# f=~y2  0.10240102          0.10240110
&gt;# f=~y3  0.10189012          0.10189033
&gt;# f=~y4  0.09983351          0.09983363
&gt;# y1~~y1 0.10862886          0.10862830
&gt;# y2~~y2 0.10752628          0.10752643
&gt;# y3~~y3 0.10480306          0.10480358
&gt;# y4~~y4 0.09943057          0.09943066</code></pre>
<p>So again it’s pretty close. You can try using the formula in <span class="citation">Maydeu-Olivares (2017)</span> to verify the results.</p>
</div>
</div>
<div id="mlmmlmv" class="section level2">
<h2>MLM/MLMV</h2>
<pre class="r"><code># Asymptotic covariance of S (4th moment)
Wi_adf &lt;- apply(t(y) - colMeans(y), 2, tcrossprod)
adf_I &lt;- tcrossprod(Wi_adf - as.vector(Sy)) / N
# Sandwich estimator:
Deltat_exp_I &lt;- crossprod(Delta, exp_I)
acov_mlm &lt;- acov_exp %*% Deltat_exp_I %*% adf_I %*% 
  crossprod(Deltat_exp_I, acov_exp) * N
ase_mlm &lt;- sqrt(diag(acov_mlm))</code></pre>
<p>Compare to <code>lavaan</code> with <code>estimator = 'MLR'</code>:</p>
<pre class="r"><code># lavaan
fit1_mlm &lt;- lavaan::cfa(model = &#39;f =~ y1 + y2 + y3 + y4&#39;, 
                        std.lv = TRUE, 
                        # Note: Full data needed for MLM
                        data = `colnames&lt;-`(y, paste0(&quot;y&quot;, 1:4)), 
                        estimator = &quot;MLM&quot;)</code></pre>
<pre class="r"><code>cbind(optim = ase_mlm, 
      `lavaan::cfa (&#39;MLM&#39;)` = sqrt(diag(lavaan::vcov(fit1_mlm))))</code></pre>
<pre><code>&gt;#             optim lavaan::cfa (&#39;MLM&#39;)
&gt;# f=~y1  0.12965429          0.12965371
&gt;# f=~y2  0.10522671          0.10522608
&gt;# f=~y3  0.08901649          0.08901603
&gt;# f=~y4  0.08981597          0.08981560
&gt;# y1~~y1 0.10278055          0.10278061
&gt;# y2~~y2 0.10252608          0.10252560
&gt;# y3~~y3 0.09792141          0.09792136
&gt;# y4~~y4 0.10608980          0.10608957</code></pre>
</div>
<div id="mlr" class="section level2">
<h2>MLR</h2>
<pre class="r"><code># Cross-product matrix (without mean structure)
# First, need the deviation/residual of covariance for each individual
# observation
Sdev &lt;- apply(t(y) - colMeans(y), 2, tcrossprod) - as.vector(Sigmahat)
g_score &lt;- crossprod(Sdev, invSigmahat %x% invSigmahat) / 2
xp_I &lt;- crossprod(g_score) / N  # Gamma^-1_XP
# Sandwich estimator:
acov_mlr &lt;- acov_obs %*% crossprod(Delta, xp_I %*% Delta) %*% acov_obs * N
ase_mlr &lt;- sqrt(diag(acov_mlr))</code></pre>
<p>Compare to <code>lavaan</code> with <code>estimator = 'MLR'</code>:</p>
<pre class="r"><code># lavaan
fit1_mlr &lt;- lavaan::cfa(model = &#39;f =~ y1 + y2 + y3 + y4&#39;, 
                        std.lv = TRUE, 
                        # Note: Full data needed for MLR
                        data = `colnames&lt;-`(y, paste0(&quot;y&quot;, 1:4)), 
                        estimator = &quot;MLR&quot;)</code></pre>
<pre class="r"><code>cbind(optim = ase_mlr, 
      `lavaan::cfa (&#39;MLR, Hessian&#39;)` = sqrt(diag(lavaan::vcov(fit1_mlr))))</code></pre>
<pre><code>&gt;#             optim lavaan::cfa (&#39;MLR, Hessian&#39;)
&gt;# f=~y1  0.12884960                   0.12885037
&gt;# f=~y2  0.10504885                   0.10504891
&gt;# f=~y3  0.08977431                   0.08977434
&gt;# f=~y4  0.08883634                   0.08883676
&gt;# y1~~y1 0.10203109                   0.10203105
&gt;# y2~~y2 0.10139778                   0.10139847
&gt;# y3~~y3 0.09785923                   0.09786031
&gt;# y4~~y4 0.10566978                   0.10567060</code></pre>
</div>
<div id="bibliography" class="section level2 unnumbered">
<h2>Bibliography</h2>
<div id="refs" class="references">
<div id="ref-Maydeu-Olivares2017">
<p>Maydeu-Olivares, Alberto. 2017. “Maximum Likelihood Estimation of Structural Equation Models for Continuous Data: Standard Errors and Goodness of Fit.” <em>Structural Equation Modeling: A Multidisciplinary Journal</em> 24 (3): 383–94. <a href="https://doi.org/10.1080/10705511.2016.1269606">https://doi.org/10.1080/10705511.2016.1269606</a>.</p>
</div>
</div>
</div>

---
title: Confidence Intervals for Multilevel R-Squared
author: marklai
date: '2022-05-12'
slug: confidence-intervals-for-multilevel-r-squared
categories:
  - Statistics
tags:
  - multilevel modeling
  - bootstrap
subtitle: ''
summary: ''
authors: []
lastmod: '2022-05-12T08:47:48-07:00'
featured: no
draft: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---



<div id="load-packages" class="section level2">
<h2>Load Packages</h2>
<pre class="r"><code>library(lme4)</code></pre>
<pre><code>## Loading required package: Matrix</code></pre>
<pre class="r"><code>library(MuMIn)  # for computing multilevel R-squared
library(r2mlm)  # another package for R-squared</code></pre>
<pre><code>## Loading required package: nlme</code></pre>
<pre><code>## 
## Attaching package: &#39;nlme&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:lme4&#39;:
## 
##     lmList</code></pre>
<pre><code>## Registered S3 method overwritten by &#39;parameters&#39;:
##   method                         from      
##   format.parameters_distribution datawizard</code></pre>
<pre class="r"><code>library(bootmlm)  # for multilevel bootstrapping
library(boot)  # for bootstrap CIs</code></pre>
</div>
<div id="an-example-multilevel-model" class="section level2">
<h2>An Example Multilevel Model</h2>
<pre class="r"><code>fm1 &lt;- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)</code></pre>
<div id="nakagawa-johnson-schielzeth-r2" class="section level3">
<h3>Nakagawa-Johnson-Schielzeth <span class="math inline">\(R^2\)</span></h3>
<pre class="r"><code>r.squaredGLMM(fm1)</code></pre>
<pre><code>## Warning: &#39;r.squaredGLMM&#39; now calculates a revised statistic. See the help page.</code></pre>
<pre><code>##            R2m       R2c
## [1,] 0.2786511 0.7992199</code></pre>
<p>The marginal <span class="math inline">\(R^2\)</span> considers the total variance accounted for due to the fixed effect associated with the predictors (<code>Days</code> in this example). See <a href="https://doi.org/10.1098/rsif.2017.0213">Nakagawa, Johnson, &amp; Schielzeth (2017)</a> for more information.</p>
</div>
<div id="right-sterba-r2" class="section level3">
<h3>Right-Sterba <span class="math inline">\(R^2\)</span></h3>
<p>More fine-grained partitioning, as described in <a href="https://doi.org/10.1037/met0000184">Rights &amp; Sterba (2019)</a></p>
<pre class="r"><code>r2mlm(fm1)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/r2mlm-fm1-1.png" width="672" /></p>
<pre><code>## $Decompositions
##                      total
## fixed           0.27851304
## slope variation 0.08915267
## mean variation  0.43165365
## sigma2          0.20068063
## 
## $R2s
##          total
## f   0.27851304
## v   0.08915267
## m   0.43165365
## fv  0.36766572
## fvm 0.79931937</code></pre>
<p>The <code>fixed</code> part is the same as the marginal <span class="math inline">\(R^2\)</span>.</p>
</div>
</div>
<div id="confidence-intervals-for-r2" class="section level2">
<h2>Confidence Intervals for <span class="math inline">\(R^2\)</span></h2>
<p>Neither <code>MuMIn::r.squaredGLMM()</code> nor <code>r2mlm::r2mlm()</code> provided confidence intervals (CIs) for the <span class="math inline">\(R^2\)</span>, but general guidelines for effect size reporting would suggest always reporting CIs for point estimates of effect size, just like for any point estimates in statistics. We can use multilevel bootstrapping to get CIs.</p>
<p>To do bootstrap, first defines an R function that gives the target <span class="math inline">\(R^2\)</span> statistics. We can do it for the marginal <span class="math inline">\(R^2\)</span>:</p>
<pre class="r"><code>marginal_r2 &lt;- function(object) {
  r.squaredGLMM(object)[[1]]
}
marginal_r2(fm1)</code></pre>
<pre><code>## [1] 0.2786511</code></pre>
</div>
<div id="parametric-bootstrap" class="section level2">
<h2>Parametric Bootstrap</h2>
<p>The <code>lme4::bootMer()</code> supports basic parametric multilevel bootstrapping</p>
<pre class="r"><code># This takes about 30 sec on my computer
boo01 &lt;- bootMer(fm1, FUN = marginal_r2, nsim = 999)
boo01</code></pre>
<pre><code>## 
## PARAMETRIC BOOTSTRAP
## 
## 
## Call:
## bootMer(x = fm1, FUN = marginal_r2, nsim = 999)
## 
## 
## Bootstrap Statistics :
##      original      bias    std. error
## t1* 0.2786511 0.005759548  0.07545455</code></pre>
<pre><code>## 
## 19 message(s): boundary (singular) fit: see help(&#39;isSingular&#39;)
## 9 warning(s): Model failed to converge with max|grad| = 0.00203576 (tol = 0.002, component 1) (and others)</code></pre>
<p>Here is the bootstrap distribution</p>
<pre class="r"><code>plot(boo01)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/hist-boo01-1.png" width="672" /></p>
<div id="bias-corrected-estimate" class="section level3">
<h3>Bias-corrected estimate</h3>
<p>The above shows that the sample estimate of <span class="math inline">\(R^2\)</span> was upwardly biased. To correct for the bias, we can use the bootstrap bias-corrected estimate</p>
<pre class="r"><code>2 * boo01$t0 - mean(boo01$t)</code></pre>
<pre><code>## [1] 0.2728915</code></pre>
</div>
<div id="confidence-intervals" class="section level3">
<h3>Confidence intervals</h3>
<p>You can get three types of bootstrap CIs (<code>"norm"</code>, <code>"basic"</code>, <code>"perc"</code>) with <code>bootMer</code>:</p>
<pre class="r"><code>boot::boot.ci(boo01, index = 1, type = c(&quot;norm&quot;, &quot;basic&quot;, &quot;perc&quot;))</code></pre>
<pre><code>## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
## Based on 999 bootstrap replicates
## 
## CALL : 
## boot::boot.ci(boot.out = boo01, type = c(&quot;norm&quot;, &quot;basic&quot;, &quot;perc&quot;), 
##     index = 1)
## 
## Intervals : 
## Level      Normal              Basic              Percentile     
## 95%   ( 0.1250,  0.4208 )   ( 0.1088,  0.4114 )   ( 0.1460,  0.4485 )  
## Calculations and Intervals on Original Scale</code></pre>
</div>
</div>
<div id="residual-bootstrap" class="section level2">
<h2>Residual Bootstrap</h2>
<p>The <code>bootmlm::bootstrap_mer()</code> implements the residual bootstrap, which is robust to non-normality.</p>
<pre class="r"><code># This takes about 30 sec on my computer
boo02 &lt;- bootstrap_mer(fm1, FUN = marginal_r2, nsim = 999, type = &quot;residual&quot;)</code></pre>
<pre><code>## boundary (singular) fit: see help(&#39;isSingular&#39;)
## boundary (singular) fit: see help(&#39;isSingular&#39;)
## boundary (singular) fit: see help(&#39;isSingular&#39;)
## boundary (singular) fit: see help(&#39;isSingular&#39;)
## boundary (singular) fit: see help(&#39;isSingular&#39;)
## boundary (singular) fit: see help(&#39;isSingular&#39;)
## boundary (singular) fit: see help(&#39;isSingular&#39;)
## boundary (singular) fit: see help(&#39;isSingular&#39;)
## boundary (singular) fit: see help(&#39;isSingular&#39;)
## boundary (singular) fit: see help(&#39;isSingular&#39;)
## boundary (singular) fit: see help(&#39;isSingular&#39;)
## boundary (singular) fit: see help(&#39;isSingular&#39;)
## boundary (singular) fit: see help(&#39;isSingular&#39;)
## boundary (singular) fit: see help(&#39;isSingular&#39;)
## boundary (singular) fit: see help(&#39;isSingular&#39;)
## boundary (singular) fit: see help(&#39;isSingular&#39;)
## boundary (singular) fit: see help(&#39;isSingular&#39;)
## boundary (singular) fit: see help(&#39;isSingular&#39;)</code></pre>
<pre class="r"><code>boo02</code></pre>
<pre><code>## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## bootstrap_mer(x = fm1, FUN = marginal_r2, nsim = 999, type = &quot;residual&quot;)
## 
## 
## Bootstrap Statistics :
##      original      bias    std. error
## t1* 0.2786511 0.005397786  0.08418536</code></pre>
<p>In this example, the results are similar. The boostrap bias-corrected estimate of <span class="math inline">\(R^2\)</span>, and the three basic CIs, can similarly be computed as in parametric bootstrap.</p>
<div id="confidence-intervals-1" class="section level3">
<h3>Confidence Intervals</h3>
<p>In addition to the three CIs previously discussed, which are first-order accurate, we can also obtain CIs that are second-order accurate: (a) bias-corrected and accelerated (BCa) CI and (b) studentized CI (also called the bootstrap-<span class="math inline">\(t\)</span> CI). For (a), it requires the influence value of the <span class="math inline">\(R^2\)</span> function, whereas for (b), it requires an estimate of the sampling variance of the <span class="math inline">\(R^2\)</span> estimate.</p>
<div id="influence-value" class="section level4">
<h4>Influence value</h4>
<pre class="r"><code># Based on the group jackknife
inf_val &lt;- bootmlm::empinf_mer(fm1, marginal_r2, index = 1)</code></pre>
</div>
<div id="sampling-variance-with-the-numerical-delta-method" class="section level4">
<h4>Sampling variance with the numerical <a href="https://en.wikipedia.org/wiki/Delta_method">delta method</a></h4>
<blockquote>
<p>This part is a bit more technical; skip this if youâ€™re not interested in the studentized CI.</p>
</blockquote>
<p>To obtain an approximate sampling variance of the <span class="math inline">\(R^2\)</span>, it would be easier to use the <code>r2mlm::r2mlm_manual()</code> function to compute <span class="math inline">\(R^2\)</span>. We first write a function that computes <span class="math inline">\(R^2\)</span> using input of the fixed and random effects:</p>
<pre class="r"><code>manual_r2 &lt;- function(theta, data,
                      # The following are model-specific
                      wc = 2, bc = NULL, rc = 2,
                      cmc = FALSE) {
  n_wc &lt;- length(wc)
  n_bc &lt;- length(bc) + 1
  dim_rc &lt;- length(rc) + 1
  n_rc &lt;- dim_rc * (dim_rc + 1) / 2
  gam_w &lt;- theta[seq_len(n_wc)]
  gam_b &lt;- theta[n_wc + seq_len(n_bc)]
  tau &lt;- matrix(NA, nrow = dim_rc, ncol = dim_rc)
  tau[lower.tri(tau, diag = TRUE)] &lt;- theta[n_wc + n_bc + seq_len(n_rc)]
  tau2 &lt;- t(tau)
  tau2[lower.tri(tau2)] &lt;- tau[lower.tri(tau)]
  s2 &lt;- tail(theta, n = 1)
  r2mlm_manual(data,
               within_covs = wc,
               between_covs = bc,
               random_covs = 2,
               gamma_w = gam_w,
               gamma_b = gam_b,
               Tau = tau2,
               sigma2 = s2,
               clustermeancentered = cmc,
               bargraph = FALSE)$R2s[1, 1]
}
theta_fm1 &lt;- c(fixef(fm1)[2], fixef(fm1)[1],
               VarCorr(fm1)[[1]][c(1, 2, 4)], sigma(fm1)^2)
manual_r2(theta_fm1, data = fm1@frame)</code></pre>
<pre><code>## [1] 0.278513</code></pre>
<p>Now computes the numerical gradient</p>
<pre class="r"><code>grad_fm1 &lt;- numDeriv::grad(manual_r2, x = theta_fm1, data = fm1@frame)</code></pre>
<p>We also need the asymptotic covariance matrix of the fixed and random effects</p>
<pre class="r"><code>vcov_fixed &lt;- vcov(fm1)
vcov_random &lt;- vcov_vc(fm1, sd_cor = FALSE, print_names = FALSE)
vcov_fm1 &lt;- bdiag(vcov_fixed, vcov_random)
# Need to re-arrange the first two columns
vcov_fm1 &lt;- vcov_fm1[c(2, 1, 3:6), c(2, 1, 3:6)]</code></pre>
<p>Now apply the multivariate delta method</p>
<pre class="r"><code>crossprod(grad_fm1, vcov_fm1) %*% grad_fm1</code></pre>
<pre><code>## 1 x 1 Matrix of class &quot;dgeMatrix&quot;
##             [,1]
## [1,] 0.005919918</code></pre>
<p>We now need a function that computes both <span class="math inline">\(R^2\)</span> and the asymptotic sampling variance of it.</p>
<pre class="r"><code>marginal_r2_with_var &lt;- function(object,
                                 wc = 2, bc = NULL, rc = 2) {
  dim_rc &lt;- length(rc) + 1
  vc_mat &lt;- matrix(seq_len(dim_rc^2), nrow = dim_rc, ncol = dim_rc)
  vc_index &lt;- vc_mat[lower.tri(vc_mat, diag = TRUE)]
  theta_obj &lt;- c(fixef(object)[wc], fixef(object)[c(1, bc)],
                 VarCorr(object)[[1]][vc_index], sigma(object)^2)
  r2 &lt;- manual_r2(theta_obj, data = object@frame)
  grad_obj &lt;- numDeriv::grad(manual_r2, x = theta_obj, data = object@frame)
  # Need to re-arrange the order of the fixed effects
  names_wc &lt;- names(object@frame)[wc]
  names_bc &lt;- c(&quot;(Intercept)&quot;, names(object@frame)[bc])
  vcov_fixed &lt;- vcov(object)[c(names_wc, names_bc), c(names_wc, names_bc)]
  vcov_random &lt;- vcov_vc(object, sd_cor = FALSE, print_names = FALSE)
  vcov_obj &lt;- bdiag(vcov_fixed, vcov_random)
  v_r2 &lt;- crossprod(grad_obj, vcov_obj) %*% grad_obj
  c(r2, as.numeric(v_r2))
}
marginal_r2_with_var(fm1)</code></pre>
<pre><code>## [1] 0.278513044 0.005919918</code></pre>
</div>
<div id="five-bootstrap-cis" class="section level4">
<h4>Five Bootstrap CIs</h4>
<p>Now, we can do bootstrap again, using the new function that computes both the estimate and the asymptotic sampling variance</p>
<pre class="r"><code># This takes quite a bit longer due to the need to compute variances
boo03 &lt;- bootstrap_mer(fm1, FUN = marginal_r2_with_var, nsim = 999,
                       type = &quot;residual&quot;)</code></pre>
<pre><code>## boundary (singular) fit: see help(&#39;isSingular&#39;)</code></pre>
<pre><code>## Warning in checkConv(attr(opt, &quot;derivs&quot;), opt$par, ctrl = control$checkConv, :
## Model failed to converge with max|grad| = 0.00828807 (tol = 0.002, component 1)</code></pre>
<pre><code>## boundary (singular) fit: see help(&#39;isSingular&#39;)
## boundary (singular) fit: see help(&#39;isSingular&#39;)</code></pre>
<pre><code>## Warning in checkConv(attr(opt, &quot;derivs&quot;), opt$par, ctrl = control$checkConv, :
## Model failed to converge with max|grad| = 0.00967386 (tol = 0.002, component 1)</code></pre>
<pre><code>## boundary (singular) fit: see help(&#39;isSingular&#39;)
## boundary (singular) fit: see help(&#39;isSingular&#39;)</code></pre>
<pre><code>## Warning in checkConv(attr(opt, &quot;derivs&quot;), opt$par, ctrl = control$checkConv, :
## Model failed to converge with max|grad| = 0.00453875 (tol = 0.002, component 1)</code></pre>
<pre><code>## boundary (singular) fit: see help(&#39;isSingular&#39;)</code></pre>
<pre><code>## Warning in checkConv(attr(opt, &quot;derivs&quot;), opt$par, ctrl = control$checkConv, :
## Model failed to converge with max|grad| = 0.00280723 (tol = 0.002, component 1)</code></pre>
<pre><code>## boundary (singular) fit: see help(&#39;isSingular&#39;)</code></pre>
<pre><code>## Warning in checkConv(attr(opt, &quot;derivs&quot;), opt$par, ctrl = control$checkConv, :
## Model failed to converge with max|grad| = 0.00764723 (tol = 0.002, component 1)</code></pre>
<pre><code>## Warning in checkConv(attr(opt, &quot;derivs&quot;), opt$par, ctrl = control$checkConv, :
## Model failed to converge with max|grad| = 0.0024541 (tol = 0.002, component 1)</code></pre>
<pre><code>## boundary (singular) fit: see help(&#39;isSingular&#39;)
## boundary (singular) fit: see help(&#39;isSingular&#39;)
## boundary (singular) fit: see help(&#39;isSingular&#39;)
## boundary (singular) fit: see help(&#39;isSingular&#39;)
## boundary (singular) fit: see help(&#39;isSingular&#39;)
## boundary (singular) fit: see help(&#39;isSingular&#39;)</code></pre>
<pre><code>## Warning in checkConv(attr(opt, &quot;derivs&quot;), opt$par, ctrl = control$checkConv, :
## Model failed to converge with max|grad| = 0.0049756 (tol = 0.002, component 1)</code></pre>
<pre><code>## Warning in checkConv(attr(opt, &quot;derivs&quot;), opt$par, ctrl = control$checkConv, :
## Model failed to converge with max|grad| = 0.00228953 (tol = 0.002, component 1)</code></pre>
<pre><code>## boundary (singular) fit: see help(&#39;isSingular&#39;)</code></pre>
<pre><code>## Warning in sqrt(diag(m)): NaNs produced</code></pre>
<pre><code>## Warning in sqrt(diag(m)): NaNs produced

## Warning in sqrt(diag(m)): NaNs produced

## Warning in sqrt(diag(m)): NaNs produced

## Warning in sqrt(diag(m)): NaNs produced

## Warning in sqrt(diag(m)): NaNs produced

## Warning in sqrt(diag(m)): NaNs produced

## Warning in sqrt(diag(m)): NaNs produced

## Warning in sqrt(diag(m)): NaNs produced

## Warning in sqrt(diag(m)): NaNs produced

## Warning in sqrt(diag(m)): NaNs produced

## Warning in sqrt(diag(m)): NaNs produced</code></pre>
<pre><code>## boundary (singular) fit: see help(&#39;isSingular&#39;)
## boundary (singular) fit: see help(&#39;isSingular&#39;)</code></pre>
<pre class="r"><code>boo03</code></pre>
<pre><code>## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## bootstrap_mer(x = fm1, FUN = marginal_r2_with_var, nsim = 999, 
##     type = &quot;residual&quot;)
## 
## 
## Bootstrap Statistics :
##        original        bias    std. error
## t1* 0.278513044  0.0100127766 0.083704256
## t2* 0.005919918 -0.0002646922 0.001150678</code></pre>
<p>And then obtain five types of CIs</p>
<pre class="r"><code>boot::boot.ci(boo03, L = inf_val)</code></pre>
<pre><code>## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
## Based on 998 bootstrap replicates
## 
## CALL : 
## boot::boot.ci(boot.out = boo03, L = inf_val)
## 
## Intervals : 
## Level      Normal              Basic             Studentized     
## 95%   ( 0.1044,  0.4326 )   ( 0.0890,  0.4067 )   ( 0.0822,  0.4385 )  
## 
## Level     Percentile            BCa          
## 95%   ( 0.1503,  0.4680 )   ( 0.1408,  0.4466 )  
## Calculations and Intervals on Original Scale</code></pre>
</div>
</div>
</div>
<div id="bootstrap-ci-with-transformation" class="section level2">
<h2>Bootstrap CI With Transformation</h2>
<p>Given that <span class="math inline">\(R^2\)</span> is bounded, it may be more accurate to first transform the <span class="math inline">\(R^2\)</span> estimates to an unbounded scale, obtain the CIs on the transformed scale, and then back transform it to between 0 and 1. This can be done in <code>boot::boot.ci()</code> as well with the logistic transformation:</p>
<pre class="r"><code>boot::boot.ci(boo03, L = inf_val, h = qlogis,
              # Need the derivative of the transformation
              hdot = function(x) 1 / (x - x^2),
              hinv = plogis)</code></pre>
<pre><code>## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
## Based on 998 bootstrap replicates
## 
## CALL : 
## boot::boot.ci(boot.out = boo03, L = inf_val, h = qlogis, hdot = function(x) 1/(x - 
##     x^2), hinv = plogis)
## 
## Intervals : 
## Level      Normal              Basic             Studentized     
## 95%   ( 0.1440,  0.4629 )   ( 0.1449,  0.4572 )   ( 0.1184,  0.4187 )  
## 
## Level     Percentile            BCa          
## 95%   ( 0.1503,  0.4680 )   ( 0.1408,  0.4466 )  
## Calculations on Transformed Scale;  Intervals on Original Scale</code></pre>
<p>Note that the transformation only affects the Normal, Basic, and Studentized CIs.</p>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>This post demonstrates how to use multilevel bootstrapping to obtain CIs for <span class="math inline">\(R^2\)</span>. The post only focuses on marginal <span class="math inline">\(R^2\)</span>, but CIs for other <span class="math inline">\(R^2\)</span> measures can be similarly obtained. The studentized CI is the most complex as it requires obtaining the sampling variance of <span class="math inline">\(R^2\)</span> for each bootstrap sample. So far, to my knowledge, there has not been studies on which CI(s) perform best, so simulation studies are needed.</p>
<p>Further readings on multilevel bootstrap:</p>
<ul>
<li><a href="https://link.springer.com/chapter/10.1007/978-0-387-73186-5_11">Chapter by van der Leeden et al.</a></li>
<li><a href="https://doi.org/10.1080/00273171.2020.1746902">Paper by Lai</a></li>
<li><a href="https://doi.org/10.1017/CBO9780511802843">Book by Davison &amp; Hinkley</a></li>
</ul>
</div>

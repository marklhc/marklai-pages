---
title: Bayesian MLM With Group Mean Centering
author: Mark Lai
date: '2020-02-04'
slug: bayesian-mlm-with-group-mean-centering
bibliography: references.bib
output:
  blogdown::html_page:
    toc: true
categories:
  - Statistics
tags:
  - multilevel
  - R
---


<div id="TOC">
<ul>
<li><a href="#the-problem">The Problem</a></li>
<li><a href="#demonstration">Demonstration</a><ul>
<li><a href="#group-mean-centering-with-lme4">Group mean centering with <code>lme4</code></a></li>
<li><a href="#same-analyses-with-bayesian-using-brms">Same analyses with Bayesian using <code>brms</code></a></li>
<li><a href="#group-mean-centering-treating-group-means-as-latent-variables">Group mean centering treating group means as latent variables</a></li>
<li><a href="#with-random-slopes">With random slopes</a></li>
</ul></li>
<li><a href="#using-the-full-data">Using the Full Data</a><ul>
<li><a href="#with-lme4">With <code>lme4</code></a></li>
<li><a href="#with-bayesian-taking-into-account-the-unreliability">With Bayesian taking into account the unreliability</a></li>
</ul></li>
<li><a href="#bibliography">Bibliography</a></li>
</ul>
</div>

<p>This post is updated on 2020-02-04 with cleaner and more efficient <code>STAN</code>
code.</p>
<p>In the past week I was teaching a one-and-a-half-day workshop on multilevel
modeling (MLM) at UC, where I discussed the use of group mean centering to
decompose level-1 and level-2 effects of a predictor. In that session I ended
by noting alternative approaches that reduce bias <span class="citation">(mainly from Lüdtke et al. 2008)</span>.
That lead me also consider the use of Bayesian methods for group mean centering,
which will be demonstrated in this post. <span class="citation">(Turns out Zitzmann, Lüdtke, and Robitzsch 2015 has already
discussed something similar, but still a good exercise.)</span></p>
<div id="the-problem" class="section level2">
<h2>The Problem</h2>
<p>In some social scienc research, a predictor can have different meanings at
different levels. For example, student’s competence, when aggregated to the
school level, becomes the competitiveness of a school. As explained in the
<a href="https://en.wikipedia.org/wiki/Big-fish%E2%80%93little-pond_effect">big-fish-little-pond
effect</a>,
whereas the former can help an individual’s self-concept, being in a more
competitive environment can hurt one’s self-concept. Traditionally, and still
popular nowadays, we investigate the differential effects of a predictor,
<span class="math inline">\(X_{ij}\)</span>, on an outcome at different levels by including the group means, <span class="math inline">\(\bar X_{.j}\)</span>, in the equation.</p>
<p>The problem, however, is that unless each cluster (e.g., school) has a very
large sample size, the group means will not be very reliable. This is true
even when the measurement of <span class="math inline">\(X_{ij}\)</span> is perfectly reliable. This is not
difficult to understand: just remember in intro stats we learn that the
standard error of the sample mean is <span class="math inline">\(\sigma / \sqrt{N}\)</span>, so with limited <span class="math inline">\(N\)</span>
our sample (group) mean can be quite different from the population (group) mean.</p>
<p>What’s the problem when the group means are not reliable? Regression literature
has shown that unreliability in the predictors lead to downwardly biased
coefficients, which is what happen here. The way to account for that, in
general, is to treat the unreliable as a latent variable, which is the
approach discussed in <span class="citation">Lüdtke et al. (2008)</span> and also demonstrated later in this post.</p>
</div>
<div id="demonstration" class="section level2">
<h2>Demonstration</h2>
<p>Let’s compare the results using the well-known High School and Beyond Survey
subsample discussed in the textbook by <span class="citation">Raudenbush and Bryk (2002)</span>. To illustrate the
issue with unreliability of group means, I’ll use a subset of 800 cases, with
a random sample of 5 cases from each school.</p>
<pre class="r"><code>library(tidyverse)
library(haven)
library(lme4)
library(brms)
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = 2L)</code></pre>
<pre class="r"><code>hsb &lt;- haven::read_sas(&#39;https://stats.idre.ucla.edu/wp-content/uploads/2016/02/hsb.sas7bdat&#39;)
# Select a subsample
set.seed(123)
hsb_sub &lt;- hsb %&gt;% group_by(ID) %&gt;% sample_n(5)
hsb_sub</code></pre>
<pre><code>## # A tibble: 800 x 12
## # Groups:   ID [160]
##    ID     SIZE SECTOR PRACAD DISCLIM HIMINTY MEANSES MINORITY FEMALE    SES
##    &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
##  1 1224    842      0   0.35   1.60        0  -0.428        0      1 -0.478
##  2 1224    842      0   0.35   1.60        0  -0.428        0      0  0.332
##  3 1224    842      0   0.35   1.60        0  -0.428        0      1 -0.988
##  4 1224    842      0   0.35   1.60        0  -0.428        0      0 -0.528
##  5 1224    842      0   0.35   1.60        0  -0.428        0      1 -1.07 
##  6 1288   1855      0   0.27   0.174       0   0.128        0      1  0.692
##  7 1288   1855      0   0.27   0.174       0   0.128        0      0 -0.118
##  8 1288   1855      0   0.27   0.174       0   0.128        0      0  1.26 
##  9 1288   1855      0   0.27   0.174       0   0.128        1      1 -1.47 
## 10 1288   1855      0   0.27   0.174       0   0.128        0      0  0.222
## # … with 790 more rows, and 2 more variables: MATHACH &lt;dbl&gt;, `_MERGE` &lt;dbl&gt;</code></pre>
<p>With the subsample, let’s study the association of socioeconomic status (<code>SES</code>)
with math achievement (<code>MATHACH</code>) at the student level and the school level.
The conventional way is to do group mean centering of <code>SES</code>, by computing the
group means and the deviation of each <code>SES</code> score from the corresponding group
mean.</p>
<pre class="r"><code>hsb_sub &lt;- hsb_sub %&gt;% ungroup() %&gt;% 
  mutate(SES_gpm = ave(SES, ID), SES_gpc = SES - SES_gpm)</code></pre>
<p>To make things simpler, the random slope of <code>SES</code> is not included. Also, one
can study differential effects with grand mean centering <span class="citation">(Enders and Tofighi 2007)</span>, but
still the group means should be added as a predictor, so the same issue with
unreliability still applies.</p>
<div id="group-mean-centering-with-lme4" class="section level3">
<h3>Group mean centering with <code>lme4</code></h3>
<pre class="r"><code>m1_mer &lt;- lmer(MATHACH ~ SES_gpm + SES_gpc + (1 | ID), data = hsb_sub)
summary(m1_mer)</code></pre>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: MATHACH ~ SES_gpm + SES_gpc + (1 | ID)
##    Data: hsb_sub
## 
## REML criterion at convergence: 5188.7
## 
## Scaled residuals: 
##      Min       1Q   Median       3Q      Max 
## -2.74545 -0.75449  0.04934  0.75875  2.64005 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  ID       (Intercept)  3.13    1.769   
##  Residual             35.81    5.984   
## Number of obs: 800, groups:  ID, 160
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  12.8714     0.2536   50.75
## SES_gpm       4.6564     0.4840    9.62
## SES_gpc       2.4229     0.3481    6.96
## 
## Correlation of Fixed Effects:
##         (Intr) SES_gpm
## SES_gpm -0.005        
## SES_gpc  0.000  0.000</code></pre>
<p>So the results suggested that one unit difference <code>SES</code> is associated with
4.7 (<em>SE</em> = 0.48) units
difference in mean <code>MATHACH</code> at the school level, but
2.4 (<em>SE</em> = 0.35) units
difference in mean <code>MATHACH</code> at the student level.</p>
<p>We can get the contextual effect by substracting the fixed effect coefficient
of <code>SES_gpm</code> from that of <code>SES_gpc</code>, or by using the original <code>SES</code> variable
<em>together with the group means</em>:</p>
<pre class="r"><code>m1_mer2 &lt;- lmer(MATHACH ~ SES_gpm + SES + (1 | ID), data = hsb_sub)
summary(m1_mer2)</code></pre>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: MATHACH ~ SES_gpm + SES + (1 | ID)
##    Data: hsb_sub
## 
## REML criterion at convergence: 5188.7
## 
## Scaled residuals: 
##      Min       1Q   Median       3Q      Max 
## -2.74545 -0.75449  0.04934  0.75875  2.64005 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  ID       (Intercept)  3.13    1.769   
##  Residual             35.81    5.984   
## Number of obs: 800, groups:  ID, 160
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  12.8714     0.2536  50.750
## SES_gpm       2.2336     0.5962   3.746
## SES           2.4229     0.3481   6.960
## 
## Correlation of Fixed Effects:
##         (Intr) SES_gp
## SES_gpm -0.004       
## SES      0.000 -0.584</code></pre>
<p>The coefficient for <code>SES_gpm</code> is now the contextual effect. We can get a
95% confidence interval:</p>
<pre class="r"><code>confint(m1_mer2, parm = &quot;beta_&quot;)</code></pre>
<pre><code>##                 2.5 %    97.5 %
## (Intercept) 12.374410 13.368315
## SES_gpm      1.066932  3.400232
## SES          1.740067  3.105664</code></pre>
</div>
<div id="same-analyses-with-bayesian-using-brms" class="section level3">
<h3>Same analyses with Bayesian using <code>brms</code></h3>
<p>I use the <code>brms</code> package with the default priors:</p>
<p><span class="math display">\[\begin{align*}
  Y_{ij} &amp; \sim N(\mu_j + \beta_1 (X_{ij} - \bar X_{.j}), \sigma^2) \\
  \mu_j &amp; \sim N(\beta_0 + \beta_2 \bar X_{.j}, \tau^2) \\
  \beta &amp; \sim N(0, \infty) \\
  \sigma &amp; \sim t_3(0, 10) \\
  \tau &amp; \sim t_3(0, 10)
\end{align*}\]</span></p>
<pre class="r"><code>m1_brm &lt;- brm(MATHACH ~ SES_gpm + SES_gpc + (1 | ID), data = hsb_sub, 
              control = list(adapt_delta = .90))
summary(m1_brm)</code></pre>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: MATHACH ~ SES_gpm + SES_gpc + (1 | ID) 
##    Data: hsb_sub (Number of observations: 800) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~ID (Number of levels: 160) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     1.71      0.41     0.73     2.42 1.00      720      656
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept    12.87      0.26    12.37    13.37 1.00     4424     2992
## SES_gpm       4.66      0.49     3.69     5.64 1.00     3771     3075
## SES_gpc       2.42      0.33     1.78     3.07 1.00     7839     3164
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     6.00      0.17     5.68     6.36 1.00     2265     2441
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>With Bayesian, the results are similar to those with <code>lme4</code>.</p>
<p>We can summarize the posterior of the contextual effect:</p>
<pre class="r"><code>post_fixef &lt;- posterior_samples(m1_brm, pars = c(&quot;b_SES_gpm&quot;, &quot;b_SES_gpc&quot;))
post_contextual &lt;- with(post_fixef, b_SES_gpm - b_SES_gpc)
c(mean = mean(post_contextual), median = median(post_contextual), 
  quantile(post_contextual, c(.025, .975)))</code></pre>
<pre><code>##     mean   median     2.5%    97.5% 
## 2.237795 2.220666 1.066570 3.424391</code></pre>
<pre class="r"><code>ggplot(aes(x = post_contextual), data = data_frame(post_contextual)) + 
    geom_density(bw = &quot;SJ&quot;)</code></pre>
<pre><code>## Warning: `data_frame()` is deprecated, use `tibble()`.
## This warning is displayed once per session.</code></pre>
<p><img src="/post/2017-08-01-bayesian-mlm-with-group-mean-centering_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
</div>
<div id="group-mean-centering-treating-group-means-as-latent-variables" class="section level3">
<h3>Group mean centering treating group means as latent variables</h3>
<p>Instead of treating the group means as known, we can instead treat them as
latent variables:
<span class="math display">\[X_{ij} \sim N(\mu_{Xj}, \sigma^2_X)\]</span></p>
<p>and we can set up the model with the priors:</p>
<p><span class="math display">\[\begin{align*}
  Y_{ij} &amp; \sim N(\mu_j + \beta_1 (X_{ij} - \mu_{Xj}), \sigma^2) \\
  X_{ij} &amp; \sim N(\mu_{Xj}, \sigma^2_X) \\
  \mu_j &amp; \sim N(\beta_0 + \beta_2 \mu_{Xj}, \tau^2) \\
  \mu_{Xj} &amp; \sim N(0, \tau^2_X) \\
  \sigma &amp; \sim t_3(0, 10) \\
  \tau &amp; \sim t_3(0, 10) \\
  \sigma_X &amp; \sim t_3(0, 10) \\
  \tau_X &amp; \sim t_3(0, 10) \\
  \beta &amp; \sim N(0, 10) \\
\end{align*}\]</span></p>
<p>Notice that here we treat <span class="math inline">\(X\)</span> as an outcome variable with a random intercept,
just like the way we model <span class="math inline">\(Y\)</span> when there is no predictor.</p>
<p>Below is the STAN code for this model:</p>
<pre><code>data { 
  int&lt;lower=1&gt; N;  // total number of observations 
  int&lt;lower=1&gt; J;  // number of clusters
  int&lt;lower=1, upper=J&gt; gid[N]; 
  vector[N] y;  // response variable 
  int&lt;lower=1&gt; K;  // number of population-level effects 
  matrix[N, K] X;  // population-level design matrix 
  int&lt;lower=1&gt; q;  // index of which column needs group mean centering
} 
transformed data { 
  int Kc = K - 1; 
  matrix[N, K - 1] Xc;  // centered version of X 
  vector[K - 1] means_X;  // column means of X before centering
  vector[N] xc;  // the column of X to be decomposed
  for (i in 2:K) { 
    means_X[i - 1] = mean(X[, i]); 
    Xc[, i - 1] = X[, i] - means_X[i - 1]; 
  } 
  xc = Xc[, q - 1];
} 
parameters { 
  vector[Kc] b;  // population-level effects at level-1
  real bm;  // population-level effects at level-2
  real b0;  // intercept (with centered variables)
  real&lt;lower=0&gt; sigma_y;  // residual SD 
  real&lt;lower=0&gt; tau_y;  // group-level standard deviations 
  vector[J] eta_y;  // normalized group-level effects of y
  real&lt;lower=0&gt; sigma_x;  // residual SD 
  real&lt;lower=0&gt; tau_x;  // group-level standard deviations 
  vector[J] eta_x;  // normalized latent group means of x
} 
transformed parameters { 
  // group means for x
  vector[J] theta_x = tau_x * eta_x;  // group means of x
  // group-level effects 
  vector[J] theta_y = b0 + tau_y * eta_y + bm * theta_x;  // group intercepts of y
  matrix[N, K - 1] Xw_c = Xc;  // copy the predictor matrix
  Xw_c[ , q - 1] = xc - theta_x[gid];  // group mean centering
} 
model {
  // prior specifications 
  b ~ normal(0, 10); 
  bm ~ normal(0, 10); 
  sigma_y ~ student_t(3, 0, 10); 
  tau_y ~ student_t(3, 0, 10); 
  eta_y ~ std_normal(); 
  sigma_x ~ student_t(3, 0, 10); 
  tau_x ~ student_t(3, 0, 10); 
  eta_x ~ std_normal(); 
  xc ~ normal(theta_x[gid], sigma_x);  // prior for lv-1 predictor
  // likelihood contribution 
  y ~ normal(theta_y[gid] + Xw_c * b, sigma_y); 
} 
generated quantities {
  // contextual effect
  real b_contextual = bm - b[q - 1];
}</code></pre>
<p>And to run it in <code>rstan</code>:</p>
<pre class="r"><code>hsb_sdata &lt;- with(hsb_sub, 
                  list(N = nrow(hsb_sub), 
                       y = MATHACH, 
                       K = 2, 
                       X = cbind(1, SES), 
                       q = 2, 
                       gid = match(ID, unique(ID)), 
                       J = length(unique(ID))))
m2_stan &lt;- stan(&quot;stan_gpc_pv.stan&quot;, data = hsb_sdata, 
                chains = 2L, iter = 1000L, 
                pars = c(&quot;b0&quot;, &quot;b&quot;, &quot;bm&quot;, &quot;b_contextual&quot;, &quot;sigma_y&quot;, &quot;tau_y&quot;, 
                         &quot;sigma_x&quot;, &quot;tau_x&quot;))</code></pre>
<pre><code>## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#bulk-ess</code></pre>
<pre><code>## Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#tail-ess</code></pre>
<p>The results are shown below:</p>
<pre class="r"><code>print(m2_stan, pars = &quot;lp__&quot;, include = FALSE)</code></pre>
<pre><code>## Inference for Stan model: stan_gpc_pv.
## 2 chains, each with iter=1000; warmup=500; thin=1; 
## post-warmup draws per chain=500, total post-warmup draws=1000.
## 
##               mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat
## b0           12.88    0.01 0.24 12.42 12.72 12.89 13.03 13.38  1183 1.00
## b[1]          2.41    0.01 0.35  1.67  2.17  2.41  2.65  3.10  1205 1.00
## bm            5.86    0.03 0.79  4.35  5.29  5.87  6.41  7.38   546 1.00
## b_contextual  3.45    0.04 0.93  1.67  2.79  3.45  4.09  5.22   518 1.00
## sigma_y       6.02    0.01 0.17  5.70  5.90  6.02  6.14  6.34   802 1.00
## tau_y         1.37    0.04 0.51  0.21  1.06  1.43  1.73  2.24   134 1.01
## sigma_x       0.68    0.00 0.02  0.65  0.67  0.68  0.69  0.72  1347 1.00
## tau_x         0.43    0.00 0.04  0.36  0.40  0.43  0.45  0.51   377 1.00
## 
## Samples were drawn using NUTS(diag_e) at Tue Feb  4 23:10:18 2020.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
<p>Note that the coefficient of <code>SES</code> at level-2 now has a posterior mean of
5.9 with a posterior <em>SD</em> of
0.79, and the contextual effect has a posterior mean
of 3.5 with a posterior <em>SD</em> of
0.93</p>
<div id="two-step-approach-using-brms" class="section level4">
<h4>Two-step approach using <code>brms</code></h4>
<div id="step-1-estimating-measurement-error-in-observed-means" class="section level5">
<h5>Step 1: estimating measurement error in observed means</h5>
<pre class="r"><code>m_ses &lt;- lmer(SES ~ (1 | ID), data = hsb_sub)
summary(m_ses)</code></pre>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: SES ~ (1 | ID)
##    Data: hsb_sub
## 
## REML criterion at convergence: 1830.9
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -4.8031 -0.6035  0.0062  0.6787  2.3937 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  ID       (Intercept) 0.1839   0.4289  
##  Residual             0.4617   0.6795  
## Number of obs: 800, groups:  ID, 160
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept) 0.002775   0.041555   0.067</code></pre>
<p>Extract measurement estimates:</p>
<pre class="r"><code># sigma_x
sigmax &lt;- sigma(m_ses)
# Add to data
hsb_sub &lt;- hsb_sub %&gt;% 
  group_by(ID) %&gt;% 
  mutate(SES_gpm_se = sigmax / sqrt(n())) %&gt;% 
  ungroup()</code></pre>
<p>Fit MLM, incorporating measurement error in latent group means:</p>
<pre class="r"><code>m1_brm_lm &lt;- brm(MATHACH ~ me(SES_gpm, sdx = SES_gpm_se, gr = ID) + 
                   SES + (1 | ID), 
                 data = hsb_sub, 
                 control = list(adapt_delta = .99, 
                                max_treedepth = 15))
summary(m1_brm_lm)</code></pre>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: MATHACH ~ me(SES_gpm, sdx = SES_gpm_se, gr = ID) + SES + (1 | ID) 
##    Data: hsb_sub (Number of observations: 800) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~ID (Number of levels: 160) 
##               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sd(Intercept)     1.37      0.51     0.20     2.21 1.01      644      773
## 
## Population-Level Effects: 
##                                Estimate Est.Error l-95% CI u-95% CI Rhat
## Intercept                         12.87      0.25    12.39    13.36 1.00
## SES                                2.40      0.36     1.70     3.09 1.00
## meSES_gpmsdxEQSES_gpm_segrEQID     3.45      0.94     1.60     5.28 1.00
##                                Bulk_ESS Tail_ESS
## Intercept                          7160     2855
## SES                                4088     3419
## meSES_gpmsdxEQSES_gpm_segrEQID     1882     2787
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     6.02      0.17     5.69     6.35 1.00     2810     2306
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
</div>
</div>
</div>
<div id="with-random-slopes" class="section level3">
<h3>With random slopes</h3>
<p>With <code>brms</code>, we have</p>
<pre class="r"><code>m2_brm &lt;- brm(MATHACH ~ SES_gpm + SES_gpc + (SES_gpc | ID), data = hsb_sub, 
              control = list(max_treedepth = 15, adapt_delta = .90))
summary(m2_brm)</code></pre>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: MATHACH ~ SES_gpm + SES_gpc + (SES_gpc | ID) 
##    Data: hsb_sub (Number of observations: 800) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~ID (Number of levels: 160) 
##                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
## sd(Intercept)              1.72      0.41     0.78     2.42 1.01      832
## sd(SES_gpc)                1.02      0.65     0.04     2.39 1.00      867
## cor(Intercept,SES_gpc)     0.27      0.48    -0.82     0.96 1.00     2433
##                        Tail_ESS
## sd(Intercept)               847
## sd(SES_gpc)                1304
## cor(Intercept,SES_gpc)     2385
## 
## Population-Level Effects: 
##           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## Intercept    12.86      0.26    12.38    13.37 1.00     4287     2749
## SES_gpm       4.66      0.48     3.69     5.60 1.00     3508     2608
## SES_gpc       2.42      0.38     1.67     3.20 1.00     5437     2252
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     5.97      0.18     5.64     6.33 1.00     1977     2753
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
<p>Below is the STAN code for this model incorporating the unreliability of
group means:</p>
<pre><code>data { 
  int&lt;lower=1&gt; N;  // total number of observations 
  int&lt;lower=1&gt; J;  // number of clusters
  int&lt;lower=1, upper=J&gt; gid[N]; 
  vector[N] y;  // response variable 
  int&lt;lower=1&gt; K;  // number of population-level effects 
  matrix[N, K] X;  // population-level design matrix 
  int&lt;lower=1&gt; q;  // index of which column needs group mean centering
} 
transformed data { 
  int Kc = K - 1; 
  matrix[N, K - 1] Xc;  // centered version of X 
  vector[K - 1] means_X;  // column means of X before centering
  vector[N] xc;  // the column of X to be decomposed
  for (i in 2:K) { 
    means_X[i - 1] = mean(X[, i]); 
    Xc[ , i - 1] = X[ , i] - means_X[i - 1]; 
  } 
  xc = Xc[ , q - 1];
} 
parameters { 
  vector[Kc] b;  // population-level effects at level-1
  real bm;  // population-level effects at level-2
  real b0;  // intercept (with centered variables)
  real&lt;lower=0&gt; sigma_y;  // residual SD 
  vector&lt;lower=0&gt;[2] tau_y;  // group-level standard deviations 
  matrix[2, J] z_u;  // normalized group-level effects 
  real&lt;lower=0&gt; sigma_x;  // residual SD 
  real&lt;lower=0&gt; tau_x;  // group-level standard deviations
  cholesky_factor_corr[2] L_1; // Cholesky correlation factor of lv-2 residuals
  vector[J] eta_x;  // unscaled group-level effects 
} 
transformed parameters { 
  // group means for x
  vector[J] theta_x = tau_x * eta_x;  // group means of x
  // group-level effects of y
  matrix[J, 2] u = (diag_pre_multiply(tau_y, L_1) * z_u)&#39;;
} 
model { 
  // prior specifications 
  b ~ normal(0, 10); 
  bm ~ normal(0, 10); 
  sigma_y ~ student_t(3, 0, 10); 
  tau_y ~ student_t(3, 0, 10); 
  L_1 ~ lkj_corr_cholesky(2);
  // z_y ~ normal(0, 1);
  to_vector(z_u) ~ normal(0, 1);
  sigma_x ~ student_t(3, 0, 10); 
  tau_x ~ student_t(3, 0, 10); 
  eta_x ~ std_normal(); 
  xc ~ normal(theta_x[gid], sigma_x);  // prior for lv-1 predictor
  // likelihood contribution 
  {
    matrix[N, K - 1] Xw_c = Xc;  // copy the predictor matrix
    vector[N] x_gpc = xc - theta_x[gid]; 
    vector[J] beta0 = b0 + theta_x * bm + u[ , 1];
    Xw_c[ , q - 1] = x_gpc;  // group mean centering
    y ~ normal(beta0[gid] + Xw_c * b + u[gid, 2] .* x_gpc, 
               sigma_y); 
  }
} 
generated quantities {
  // contextual effect
  real b_contextual = bm - b[q - 1];
}</code></pre>
<p>And to run it in <code>rstan</code>:</p>
<pre class="r"><code>hsb_sdata &lt;- with(hsb_sub, 
                  list(N = nrow(hsb_sub), 
                       y = MATHACH, 
                       K = 2, 
                       X = cbind(1, SES), 
                       q = 2, 
                       gid = match(ID, unique(ID)), 
                       J = length(unique(ID))))
m3_stan &lt;- stan(&quot;stan_gpc_pv_ran_slp.stan&quot;, data = hsb_sdata, 
                chains = 2L, iter = 1000L, 
                control = list(adapt_delta = .99, max_treedepth = 15), 
                pars = c(&quot;b0&quot;, &quot;b&quot;, &quot;bm&quot;, &quot;b_contextual&quot;, &quot;sigma_y&quot;, &quot;tau_y&quot;, 
                         &quot;sigma_x&quot;, &quot;tau_x&quot;))</code></pre>
<pre><code>## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#bulk-ess</code></pre>
<pre><code>## Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#tail-ess</code></pre>
<p>The results are shown below:</p>
<pre class="r"><code>print(m3_stan, pars = &quot;lp__&quot;, include = FALSE)</code></pre>
<pre><code>## Inference for Stan model: stan_gpc_pv_ran_slp.
## 2 chains, each with iter=1000; warmup=500; thin=1; 
## post-warmup draws per chain=500, total post-warmup draws=1000.
## 
##               mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat
## b0           12.90    0.01 0.26 12.42 12.72 12.89 13.07 13.42  1766 1.00
## b[1]          2.35    0.01 0.37  1.58  2.11  2.36  2.61  3.07   759 1.00
## bm            5.89    0.04 0.77  4.45  5.35  5.87  6.44  7.35   352 1.01
## b_contextual  3.54    0.05 0.93  1.68  2.88  3.58  4.17  5.35   305 1.01
## sigma_y       5.97    0.01 0.17  5.64  5.85  5.97  6.10  6.32   971 1.00
## tau_y[1]      1.39    0.04 0.49  0.30  1.09  1.45  1.72  2.23   125 1.02
## tau_y[2]      0.90    0.05 0.59  0.04  0.41  0.85  1.32  2.15   165 1.01
## sigma_x       0.68    0.00 0.02  0.64  0.67  0.68  0.69  0.72  1489 1.00
## tau_x         0.43    0.00 0.04  0.36  0.40  0.43  0.46  0.51   334 1.00
## 
## Samples were drawn using NUTS(diag_e) at Tue Feb  4 23:14:19 2020.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
<div id="using-two-step-with-brms" class="section level4">
<h4>Using two-step with <code>brms</code></h4>
<pre class="r"><code>m3_brm_lm &lt;- brm(MATHACH ~ me(SES_gpm, sdx = SES_gpm_se, gr = ID) + 
                   SES + 
                   (SES_gpc | ID), 
                 data = hsb_sub, 
                 control = list(adapt_delta = .99, 
                                max_treedepth = 15))
summary(m3_brm_lm)</code></pre>
<pre><code>##  Family: gaussian 
##   Links: mu = identity; sigma = identity 
## Formula: MATHACH ~ me(SES_gpm, sdx = SES_gpm_se, gr = ID) + SES + (SES_gpc | ID) 
##    Data: hsb_sub (Number of observations: 800) 
## Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;
##          total post-warmup samples = 4000
## 
## Group-Level Effects: 
## ~ID (Number of levels: 160) 
##                        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS
## sd(Intercept)              1.39      0.49     0.26     2.20 1.00      477
## sd(SES_gpc)                1.00      0.65     0.04     2.35 1.01      715
## cor(Intercept,SES_gpc)     0.22      0.51    -0.85     0.96 1.00     1383
##                        Tail_ESS
## sd(Intercept)               516
## sd(SES_gpc)                1136
## cor(Intercept,SES_gpc)     1805
## 
## Population-Level Effects: 
##                                Estimate Est.Error l-95% CI u-95% CI Rhat
## Intercept                         12.87      0.26    12.38    13.37 1.00
## SES                                2.38      0.36     1.66     3.09 1.00
## meSES_gpmsdxEQSES_gpm_segrEQID     3.54      0.94     1.70     5.40 1.00
##                                Bulk_ESS Tail_ESS
## Intercept                          4398     2982
## SES                                2489     3025
## meSES_gpmsdxEQSES_gpm_segrEQID     1388     2199
## 
## Family Specific Parameters: 
##       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS
## sigma     5.98      0.17     5.65     6.33 1.00     2014     2502
## 
## Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS
## and Tail_ESS are effective sample size measures, and Rhat is the potential
## scale reduction factor on split chains (at convergence, Rhat = 1).</code></pre>
</div>
</div>
</div>
<div id="using-the-full-data" class="section level2">
<h2>Using the Full Data</h2>
<div id="with-lme4" class="section level3">
<h3>With <code>lme4</code></h3>
<pre class="r"><code>hsb &lt;- hsb %&gt;% mutate(SES_gpm = ave(SES, ID), SES_gpc = SES - SES_gpm)
mfull_mer &lt;- lmer(MATHACH ~ SES_gpm + SES_gpc + (1 | ID), data = hsb)
summary(mfull_mer)</code></pre>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: MATHACH ~ SES_gpm + SES_gpc + (1 | ID)
##    Data: hsb
## 
## REML criterion at convergence: 46568.6
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -3.1666 -0.7254  0.0174  0.7558  2.9454 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  ID       (Intercept)  2.693   1.641   
##  Residual             37.019   6.084   
## Number of obs: 7185, groups:  ID, 160
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  12.6833     0.1494   84.91
## SES_gpm       5.8662     0.3617   16.22
## SES_gpc       2.1912     0.1087   20.16
## 
## Correlation of Fixed Effects:
##         (Intr) SES_gpm
## SES_gpm 0.010         
## SES_gpc 0.000  0.000</code></pre>
</div>
<div id="with-bayesian-taking-into-account-the-unreliability" class="section level3">
<h3>With Bayesian taking into account the unreliability</h3>
<pre class="r"><code>hsb_sdata &lt;- with(hsb, 
                  list(N = nrow(hsb), 
                       y = MATHACH, 
                       K = 2, 
                       X = cbind(1, SES), 
                       q = 2, 
                       gid = match(ID, unique(ID)), 
                       J = length(unique(ID))))
# This takes about 4 minutes on my computer
m2full_stan &lt;- stan(&quot;stan_gpc_pv.stan&quot;, data = hsb_sdata, 
                    chains = 2L, iter = 1000L, 
                    pars = c(&quot;b0&quot;, &quot;b&quot;, &quot;bm&quot;, &quot;b_contextual&quot;, 
                            &quot;sigma_y&quot;, &quot;tau_y&quot;, &quot;sigma_x&quot;, &quot;tau_x&quot;))</code></pre>
<pre><code>## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.
## Running the chains for more iterations may help. See
## http://mc-stan.org/misc/warnings.html#bulk-ess</code></pre>
<pre class="r"><code>print(m2full_stan, pars = &quot;lp__&quot;, include = FALSE)</code></pre>
<pre><code>## Inference for Stan model: stan_gpc_pv.
## 2 chains, each with iter=1000; warmup=500; thin=1; 
## post-warmup draws per chain=500, total post-warmup draws=1000.
## 
##               mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat
## b0           12.68    0.01 0.15 12.37 12.58 12.69 12.78 12.96   347 1.00
## b[1]          2.19    0.00 0.11  1.98  2.12  2.19  2.27  2.40  1536 1.00
## bm            6.14    0.02 0.40  5.36  5.88  6.14  6.40  6.91   296 1.01
## b_contextual  3.95    0.02 0.42  3.09  3.69  3.95  4.22  4.74   312 1.01
## sigma_y       6.09    0.00 0.05  5.98  6.05  6.09  6.12  6.18  2003 1.00
## tau_y         1.60    0.01 0.13  1.36  1.51  1.60  1.70  1.86   405 1.00
## sigma_x       0.67    0.00 0.01  0.66  0.66  0.67  0.67  0.68  1741 1.00
## tau_x         0.40    0.00 0.03  0.35  0.38  0.40  0.42  0.46   101 1.02
## 
## Samples were drawn using NUTS(diag_e) at Tue Feb  4 23:17:16 2020.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
<p>Note that with higher reliabilities, the results are more similar. Also,
the results accounting for unreliability with the subset is much closer to the
results with the full sample.</p>
</div>
</div>
<div id="bibliography" class="section level2 unnumbered">
<h2>Bibliography</h2>
<div id="refs" class="references">
<div id="ref-Enders2007">
<p>Enders, Craig K., and Davood Tofighi. 2007. “Centering predictor variables in cross-sectional multilevel models: A new look at an old issue.” <em>Psychological Methods</em> 12: 121–38. <a href="https://doi.org/10.1037/1082-989X.12.2.121">https://doi.org/10.1037/1082-989X.12.2.121</a>.</p>
</div>
<div id="ref-Ludtke2008">
<p>Lüdtke, Oliver, Herbert W. Marsh, Alexander Robitzsch, Ulrich Trautwein, Tihomir Asparouhov, and Bengt Muthén. 2008. “The multilevel latent covariate model: A new, more reliable approach to group-level effects in contextual studies.” <em>Psychological Methods</em> 13: 203–29. <a href="https://doi.org/10.1037/a0012869">https://doi.org/10.1037/a0012869</a>.</p>
</div>
<div id="ref-Raudenbush2002">
<p>Raudenbush, Stephen W., and Anthony S. Bryk. 2002. <em>Hierarchical linear models: Applications and data analysis methods</em>. 2nd ed. Thousand Oaks, CA: Sage.</p>
</div>
<div id="ref-Zitzmann2015">
<p>Zitzmann, Steffen, Oliver Lüdtke, and Alexander Robitzsch. 2015. “A Bayesian approach to more stable estimates of group-level effects in contextual studies.” <em>Multivariate Behavioral Research</em> 50: 688–705. <a href="https://doi.org/10.1080/00273171.2015.1090899">https://doi.org/10.1080/00273171.2015.1090899</a>.</p>
</div>
</div>
</div>

[
  {
    "objectID": "post.html",
    "href": "post.html",
    "title": "Posts",
    "section": "",
    "text": "Multilevel Composite Reliability\n\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2022\n\n\nMark Lai\n\n\n\n\n\n\n  \n\n\n\n\nNominal Regression in STAN\n\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2022\n\n\nMark Lai\n\n\n\n\n\n\n  \n\n\n\n\nScaling and Standard Errors in SEM\n\n\n\n\n\n\n\n\n\n\n\n\nJun 17, 2022\n\n\nMark Lai\n\n\n\n\n\n\n  \n\n\n\n\nEstimating a 2-PL Model in Julia (Part 2)\n\n\n\n\n\n\n\n\n\n\n\n\nMay 28, 2022\n\n\nMark Lai\n\n\n\n\n\n\n  \n\n\n\n\nEstimating a 2-PL Model in Julia (Part 1)\n\n\n\n\n\n\n\n\n\n\n\n\nMay 19, 2022\n\n\nMark Lai\n\n\n\n\n\n\n  \n\n\n\n\nConfidence Intervals for Multilevel R-Squared\n\n\n\n\n\n\n\n\n\n\n\n\nMay 12, 2022\n\n\nMark Lai\n\n\n\n\n\n\n  \n\n\n\n\nActor Partner Interdependence Model With Multilevel Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nOct 23, 2021\n\n\nMark Lai\n\n\n\n\n\n\n  \n\n\n\n\nIRT Scoring With Covariates\n\n\n\n\n\n\n\n\n\n\n\n\nSep 13, 2021\n\n\nMark Lai\n\n\n\n\n\n\n  \n\n\n\n\nUsing cmdstanr in SimDesign\n\n\n\n\n\n\n\n\n\n\n\n\nJul 14, 2021\n\n\nMark Lai\n\n\n\n\n\n\n  \n\n\n\n\nMixed Factorial ANOVA in R\n\n\n\n\n\n\n\n\n\n\n\n\nMar 29, 2021\n\n\nMark Lai\n\n\n\n\n\n\n  \n\n\n\n\nUnit-Specific vs. Population-Average Models\n\n\n\n\n\n\n\n\n\n\n\n\nDec 28, 2020\n\n\nMark Lai\n\n\n\n\n\n\n  \n\n\n\n\nPiping with magrittr\n\n\n\n\n\n\n\n\n\n\n\n\nDec 16, 2020\n\n\nmarklai\n\n\n\n\n\n\n  \n\n\n\n\nWeighted Least Squares\n\n\n\n\n\n\n\n\n\n\n\n\nJun 12, 2020\n\n\nMark Lai\n\n\n\n\n\n\n  \n\n\n\n\nAsymptotic Standard Errors in CFA\n\n\n\n\n\n\n\n\n\n\n\n\nApr 11, 2020\n\n\nMark Lai\n\n\n\n\n\n\n  \n\n\n\n\nUsing Julia to Find MLE for a Factor Model\n\n\n\n\n\n\n\n\n\n\n\n\nMar 7, 2020\n\n\nMark Lai\n\n\n\n\n\n\n  \n\n\n\n\nModel Selection for Multilevel Modeling\n\n\n\n\n\n\n\n\n\n\n\n\nDec 26, 2017\n\n\nMark Lai\n\n\n\n\n\n\n  \n\n\n\n\nBayesian MLM With Group Mean Centering\n\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2017\n\n\nMark Lai\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publication.html",
    "href": "publication.html",
    "title": "Selected Publications",
    "section": "",
    "text": "See my CV for the full list of publications.\n\nLai, M. H. C., Tse, W. W.-Y., Zhang, G., Li, Y., & Hsiao, Y.-Y. (2023). Correcting for Unreliability and Partial Invariance: A Two-Stage Path Analysis Approach. Structural Equation Modeling: A Multidisciplinary Journal, 30(2), 258–271. https://doi.org/10.1080/10705511.2022.2125397\n[Postprint] [Supp] [Slides]\nLai, M. H. C. (2023). Adjusting for Measurement Noninvariance with Alignment in Growth Modeling. Multivariate Behavioral Research, 58(1), 30–47. https://doi.org/10.1080/00273171.2021.1941730\n[Postprint] [Supp]\nZhang, Y., Lai, M. H. C., & Palardy, G. J. (2022). A Bayesian region of measurement equivalence (ROME) approach for establishing measurement invariance. Psychological Methods. https://doi.org/10.1037/met0000455\n[Supp]\nLai, M. H. C., & Zhang, Y. (2022). Classification Accuracy of Multidimensional Tests: Quantifying the Impact of Noninvariance. Structural Equation Modeling: A Multidisciplinary Journal, 29(4), 620–629. https://doi.org/10.1080/10705511.2021.1977936\n[Postprint] [Supp] [Poster]\nLai, M. H. C., & Hsiao, Y.-Y. (2022). Two-stage path analysis with definition variables: An alternative framework to account for measurement error. Psychological Methods, 27(4), 568–588. https://doi.org/10.1037/met0000410\n[Postprint] [Supp] [Poster]\nLuo, W., & Lai, H. C. (2021). A Weighted Residual Bootstrap Method for Multilevel Modeling with Sampling Weights. Journal of Behavioral Data Science. https://doi.org/10.35566/jbds/v1n2/p6\nLai, M. H. C., Liu, Y., & Tse, W. W.-Y. (2021). Adjusting for partial invariance in latent parameter estimation: Comparing forward specification search and approximate invariance methods. Behavior Research Methods, 54(1), 414–434. https://doi.org/10.3758/s13428-021-01560-2\n[Postprint] [Supp]\nLai, M. H. C. (2021). Composite reliability of multilevel data: It’s about observed scores and construct meanings. Psychological Methods, 26(1), 90–102. https://doi.org/10.1037/met0000287\n[Postprint] [Supp] [Slides]\nLai, M. H. C. (2021). Bootstrap Confidence Intervals for Multilevel Standardized Effect Size. Multivariate Behavioral Research, 56(4), 558–578. https://doi.org/10.1080/00273171.2020.1746902\n[Postprint] [Supp] [Package] [Slides]\nLai, M. H. C., Richardson, G. B., & Mak, H. W. (2019). Quantifying the impact of partial measurement invariance in diagnostic research: An application to addiction research. Addictive Behaviors, 94, 50–56. https://doi.org/10.1016/j.addbeh.2018.11.029\n[Postprint]\nLai, M. H. C. (2019). Correcting Fixed Effect Standard Errors When a Crossed Random Effect Was Ignored for Balanced and Unbalanced Designs. Journal of Educational and Behavioral Statistics, 44(4), 448–472. https://doi.org/10.3102/1076998619843168\n[Postprint]\nYoon, M., & Lai, M. H. C. (2018). Testing Factorial Invariance With Unbalanced Samples. Structural Equation Modeling: A Multidisciplinary Journal, 25(2), 201–213. https://doi.org/10.1080/10705511.2017.1387859\nLai, M. H. C., Kwok, O., Hsiao, Y.-Y., & Cao, Q. (2018). Finite population correction for two-level hierarchical linear models. Psychological Methods, 23(1), 94–112. https://doi.org/10.1037/met0000137\n[Postprint] [Code]\nKwok, O.-M., Lai, M. H.-C., Tong, F., Lara-Alecio, R., Irby, B., Yoon, M., & Yeh, Y.-C. (2018). Analyzing Complex Longitudinal Data in Educational Research: A Demonstration With Project English Language and Literacy Acquisition (ELLA) Data Using xxM. Frontiers in Psychology, 9, 790. https://doi.org/10.3389/fpsyg.2018.00790\nHsiao, Y.-Y., & Lai, M. H. C. (2018). The Impact of Partial Measurement Invariance on Testing Moderation for Single and Multi-Level Data. Frontiers in Psychology, 9, 740. https://doi.org/10.3389/fpsyg.2018.00740\nLai, M. H. C., & Zhang, J. (2017). Evaluating fit indices for multivariate t-based structural equation modeling with data contamination. Frontiers in Psychology, 8, 1286. https://doi.org/10.3389/fpsyg.2017.01286\nLai, M. H. C., Kwok, O., Yoon, M., & Hsiao, Y.-Y. (2017). Understanding the impact of partial factorial invariance on selection accuracy: An R script. Structural Equation Modeling: A Multidisciplinary Journal, 24(5), 783–799. https://doi.org/10.1080/10705511.2017.1318703\n[Postprint] [Code] [Package] [Webapp]\nLai, M. H. C., & Kwok, O. (2016). Estimating standardized effect sizes for two- and three-level partially nested data. Multivariate Behavioral Research, 1–17. https://doi.org/10.1080/00273171.2016.1231606\n[Postprint]\nLai, M. H. C., & Yoon, M. (2015). A modified comparative fit index for factorial invariance studies. Structural Equation Modeling: A Multidisciplinary Journal, 22(2), 236–248. https://doi.org/10.1080/10705511.2014.935928\nLai, M. H. C., & Kwok, O. (2015). Examining the rule of thumb of not using multilevel modeling: The “design effect smaller than two” rule. The Journal of Experimental Education, 83(3), 423–438. https://doi.org/10.1080/00220973.2014.907229\n[Postprint]\nLai, M. H. C., & Kwok, O.-M. (2014). Standardized mean differences in two-level cross-classified random effects models. Journal of Educational and Behavioral Statistics, 39(4), 282–302. https://doi.org/10.3102/1076998614532950"
  },
  {
    "objectID": "posts/2021-10-23-actor-partner-interdependence-model-with-multivariate-multilevel-analysis/index.html",
    "href": "posts/2021-10-23-actor-partner-interdependence-model-with-multivariate-multilevel-analysis/index.html",
    "title": "Actor Partner Interdependence Model With Multilevel Analysis",
    "section": "",
    "text": "Every time I teach multilevel modeling (MLM) at USC, I have students interested in running the actor partner independence model (APIM) using dyadic model. While such models are easier to fit with structural equation modeling, it can also be fit using MLM. In this post I provide a software tutorial for fitting such a model using MLM, and contrast to results to SEM.\nNote that this is not a conceptual introduction to APIM. Please check out this chapter and this paper.\nThere is also this great tutorial on using the nlme package, which uses the dummy variable trick to allow a univariate MLM to handle multivariate analyses. The current blog post is similar but with a different package.\nSee also https://hydra.smith.edu/~rgarcia/workshop/Day_1-Actor-Partner_Interdependence_Model.html with the same example using nlme::gls()."
  },
  {
    "objectID": "posts/2021-10-23-actor-partner-interdependence-model-with-multivariate-multilevel-analysis/index.html#load-packages",
    "href": "posts/2021-10-23-actor-partner-interdependence-model-with-multivariate-multilevel-analysis/index.html#load-packages",
    "title": "Actor Partner Interdependence Model With Multilevel Analysis",
    "section": "Load Packages",
    "text": "Load Packages\n\nlibrary(tidyverse)\nlibrary(psych)\nlibrary(glmmTMB)\n\nWarning in checkMatrixPackageVersion(): Package version inconsistency detected.\nTMB was built with Matrix version 1.5.4\nCurrent Matrix version is 1.5.4.1\nPlease re-install 'TMB' from source using install.packages('TMB', type = 'source') or ask CRAN for a binary version of 'TMB' matching CRAN's 'Matrix' package\n\n\nWarning in checkDepPackageVersion(dep_pkg = \"TMB\"): Package version inconsistency detected.\nglmmTMB was built with TMB version 1.9.2\nCurrent TMB version is 1.9.4\nPlease re-install glmmTMB from source or restore original 'TMB' package (see '?reinstalling' for more information)"
  },
  {
    "objectID": "posts/2021-10-23-actor-partner-interdependence-model-with-multivariate-multilevel-analysis/index.html#data",
    "href": "posts/2021-10-23-actor-partner-interdependence-model-with-multivariate-multilevel-analysis/index.html#data",
    "title": "Actor Partner Interdependence Model With Multilevel Analysis",
    "section": "Data",
    "text": "Data\nThe data set comes from https://github.com/RandiLGarcia/dyadr/tree/master/data, which has 148 married couples. You can see the codebook at https://randilgarcia.github.io/week-dyad-workshop/Acitelli%20Codebook.pdf\n\npair_data &lt;- here::here(\"data_files\", \"acipair.RData\")\n# Download data if not already exists in local folder\nif (!file.exists(pair_data)) {\n  download.file(\"https://github.com/RandiLGarcia/dyadr/raw/master/data/acipair.RData\",\n                pair_data)\n}\nload(pair_data)\n# Show data\nhead(acipair)\n\n  CoupleID  Yearsmar Partnum Gender_A SelfPos_A OtherPos_A Satisfaction_A\n1        3  8.202667       1     Wife       4.8        4.6       4.000000\n2        3  8.202667       2  Husband       3.8        4.0       3.666667\n3       10 10.452667       1     Wife       4.6        3.8       3.166667\n4       10 10.452667       2  Husband       4.2        4.0       3.666667\n5       11 -8.297333       1     Wife       5.0        4.4       3.833333\n6       11 -8.297333       2  Husband       4.2        4.8       3.833333\n  Tension_A SimHob_A Gender_P SelfPos_P OtherPos_P Satisfaction_P Tension_P\n1       1.5        0     MALE       3.8        4.0       3.666667       2.5\n2       2.5        1   FEMALE       4.8        4.6       4.000000       1.5\n3       4.0        0     MALE       4.2        4.0       3.666667       2.0\n4       2.0        0   FEMALE       4.6        3.8       3.166667       4.0\n5       2.5        0     MALE       4.2        4.8       3.833333       2.5\n6       2.5        0   FEMALE       5.0        4.4       3.833333       2.5\n  SimHob_P                   Gender COtherPos_A COtherPos_P CTension_A\n1        1 Wife                          0.3365     -0.2635    -0.9307\n2        0 Husband                      -0.2635      0.3365     0.0693\n3        0 Wife                         -0.4635     -0.2635     1.5693\n4        0 Husband                      -0.2635     -0.4635    -0.4307\n5        0 Wife                          0.1365      0.5365     0.0693\n6        0 Husband                       0.5365      0.1365     0.0693\n  CTension_P\n1     0.0693\n2    -0.9307\n3    -0.4307\n4     1.5693\n5     0.0693\n6     0.0693"
  },
  {
    "objectID": "posts/2021-10-23-actor-partner-interdependence-model-with-multivariate-multilevel-analysis/index.html#hypothetical-research-question",
    "href": "posts/2021-10-23-actor-partner-interdependence-model-with-multivariate-multilevel-analysis/index.html#hypothetical-research-question",
    "title": "Actor Partner Interdependence Model With Multilevel Analysis",
    "section": "Hypothetical Research Question",
    "text": "Hypothetical Research Question\nHere we have a hypothetical research question of how a person views their partners (OtherPos_A) and how their partner views them (OtherPos_P) predict their marriage satisfaction (Satisfaction_A).\n\n# Overall\npairs.panels(\n  subset(acipair, select = c(\"Satisfaction_A\", \"OtherPos_A\", \"OtherPos_P\"))\n)\n\n\n\n# Wife\npairs.panels(\n  subset(acipair, subset = Gender_A == \"Wife\",\n         select = c(\"Satisfaction_A\", \"OtherPos_A\", \"OtherPos_P\"))\n)\n\n\n\n# Husband\npairs.panels(\n  subset(acipair, subset = Gender_A == \"Husband\",\n         select = c(\"Satisfaction_A\", \"OtherPos_A\", \"OtherPos_P\"))\n)\n\n\n\n# Note the ceiling effect of satisfaction\n\nIn this example, there are two major paths of interest:\n\nActor effect: OtherPos_A to Satisfaction_A\nPartner effect: OtherPos_P to Satisfaction_A\n\nIn addition, for distinguishable dyads, like husband and wives, we want the gender interactions as well, resulting in four paths of interest (actor/partner effects for husbands/wives)."
  },
  {
    "objectID": "posts/2021-10-23-actor-partner-interdependence-model-with-multivariate-multilevel-analysis/index.html#model-comparison",
    "href": "posts/2021-10-23-actor-partner-interdependence-model-with-multivariate-multilevel-analysis/index.html#model-comparison",
    "title": "Actor Partner Interdependence Model With Multilevel Analysis",
    "section": "Model Comparison",
    "text": "Model Comparison\n\nanova(m_apim, m2, m3)\n\nData: acipair\nModels:\nm3: Satisfaction_A ~ OtherPos_A + OtherPos_P + (1 | CoupleID), zi=~0, disp=~1\nm2: Satisfaction_A ~ Gender_A + OtherPos_A + OtherPos_P + (1 | CoupleID), zi=~0, disp=~1\nm_apim: Satisfaction_A ~ (OtherPos_A + OtherPos_P) * Gender_A + us(0 + , zi=~0, disp=~Gender_A\nm_apim:     Gender_A | CoupleID), zi=~0, disp=~1\n       Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)\nm3      5 292.88 311.34 -141.44   282.88                         \nm2      6 294.45 316.60 -141.23   282.45 0.4312      1     0.5114\nm_apim 11 297.61 338.20 -137.80   275.61 6.8458      5     0.2324\n\n\nThere model with all paths equal is the best in terms of AIC and BIC, and the likelihood ratio tests were not significant."
  },
  {
    "objectID": "posts/2021-10-23-actor-partner-interdependence-model-with-multivariate-multilevel-analysis/index.html#using-sem",
    "href": "posts/2021-10-23-actor-partner-interdependence-model-with-multivariate-multilevel-analysis/index.html#using-sem",
    "title": "Actor Partner Interdependence Model With Multilevel Analysis",
    "section": "Using SEM",
    "text": "Using SEM"
  },
  {
    "objectID": "posts/2021-10-23-actor-partner-interdependence-model-with-multivariate-multilevel-analysis/index.html#distinguishable-dyads-1",
    "href": "posts/2021-10-23-actor-partner-interdependence-model-with-multivariate-multilevel-analysis/index.html#distinguishable-dyads-1",
    "title": "Actor Partner Interdependence Model With Multilevel Analysis",
    "section": "Distinguishable Dyads",
    "text": "Distinguishable Dyads\n\n# Wide format\naciwide &lt;- filter(acipair, Partnum == 1)  # Actor = Wife; Partner = Husband\nlibrary(lavaan)\n\nThis is lavaan 0.6-13\nlavaan is FREE software! Please report any bugs.\n\n\n\nAttaching package: 'lavaan'\n\n\nThe following object is masked from 'package:psych':\n\n    cor2cov\n\napim_mod &lt;- ' Satisfaction_A + Satisfaction_P ~ OtherPos_A + OtherPos_P '\napim_fit &lt;- sem(apim_mod, data = aciwide)\nsummary(apim_fit)\n\nlavaan 0.6.13 ended normally after 12 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         7\n\n  Number of observations                           148\n\nModel Test User Model:\n                                                      \n  Test statistic                                 0.000\n  Degrees of freedom                                 0\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  Satisfaction_A ~                                    \n    OtherPos_A        0.378    0.073    5.162    0.000\n    OtherPos_P        0.321    0.081    3.984    0.000\n  Satisfaction_P ~                                    \n    OtherPos_A        0.262    0.061    4.305    0.000\n    OtherPos_P        0.424    0.067    6.332    0.000\n\nCovariances:\n                    Estimate  Std.Err  z-value  P(&gt;|z|)\n .Satisfaction_A ~~                                    \n   .Satisfaction_P     0.080    0.015    5.221    0.000\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .Satisfaction_A    0.203    0.024    8.602    0.000\n   .Satisfaction_P    0.140    0.016    8.602    0.000\n\n\n\nlibrary(semPlot)\nlibrary(semptools)\np1 &lt;- semPaths(apim_fit, what = \"est\", \n               rotation = 2, \n               sizeMan = 15,\n               nCharNodes = 0,\n               edge.label.cex = 1.15,\n               label.cex = 1.25)\n\n\nmy_label_list &lt;- list(list(node = \"Satisfaction_A\", to = \"Satisfaction (W)\"),\n                      list(node = \"Satisfaction_P\", to = \"Satisfaction (H)\"),\n                      list(node = \"OtherPos_A\", to = \"View\\nPartner (W)\"),\n                      list(node = \"OtherPos_P\", to = \"View\\nPartner (H)\"))\np2 &lt;- change_node_label(p1, my_label_list)\np3 &lt;- mark_se(p2, apim_fit, sep = \"\\n\")\nmy_position_list &lt;- c(\"Satisfaction_A ~ OtherPos_P\" = .25,\n                      \"Satisfaction_P ~ OtherPos_A\" = .25)\np4 &lt;- set_edge_label_position(p3, my_position_list)\nplot(p4)"
  },
  {
    "objectID": "posts/2021-10-23-actor-partner-interdependence-model-with-multivariate-multilevel-analysis/index.html#indistinguishable-dyads-1",
    "href": "posts/2021-10-23-actor-partner-interdependence-model-with-multivariate-multilevel-analysis/index.html#indistinguishable-dyads-1",
    "title": "Actor Partner Interdependence Model With Multilevel Analysis",
    "section": "Indistinguishable Dyads",
    "text": "Indistinguishable Dyads\n\napim2_mod &lt;- ' Satisfaction_A ~ a * OtherPos_A + p * OtherPos_P\n               Satisfaction_P ~ p * OtherPos_A + a * OtherPos_P \n               # Constrain the variances and means to be equal\n               Satisfaction_A ~~ v * Satisfaction_A\n               Satisfaction_P ~~ v * Satisfaction_P\n               Satisfaction_A ~ m * 1\n               Satisfaction_P ~ m * 1 '\napim2_fit &lt;- sem(apim2_mod, data = aciwide)\nsummary(apim2_fit)\n\nlavaan 0.6.13 ended normally after 22 iterations\n\n  Estimator                                         ML\n  Optimization method                           NLMINB\n  Number of model parameters                         9\n  Number of equality constraints                     4\n\n  Number of observations                           148\n\nModel Test User Model:\n                                                      \n  Test statistic                                 7.277\n  Degrees of freedom                                 4\n  P-value (Chi-square)                           0.122\n\nParameter Estimates:\n\n  Standard errors                             Standard\n  Information                                 Expected\n  Information saturated (h1) model          Structured\n\nRegressions:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n  Satisfaction_A ~                                    \n    OtherPos_A (a)    0.400    0.047    8.510    0.000\n    OtherPos_P (p)    0.288    0.047    6.120    0.000\n  Satisfaction_P ~                                    \n    OtherPos_A (p)    0.288    0.047    6.120    0.000\n    OtherPos_P (a)    0.400    0.047    8.510    0.000\n\nCovariances:\n                    Estimate  Std.Err  z-value  P(&gt;|z|)\n .Satisfaction_A ~~                                    \n   .Satisfaction_P     0.080    0.016    5.145    0.000\n\nIntercepts:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .Satsfctn_A (m)    0.670    0.320    2.091    0.036\n   .Satsfctn_P (m)    0.670    0.320    2.091    0.036\n\nVariances:\n                   Estimate  Std.Err  z-value  P(&gt;|z|)\n   .Satsfctn_A (v)    0.172    0.016   11.024    0.000\n   .Satsfctn_P (v)    0.172    0.016   11.024    0.000\n\n\n\np1 &lt;- semPaths(apim2_fit, what = \"est\", \n               rotation = 2, \n               sizeMan = 15,\n               nCharNodes = 0,\n               edge.label.cex = 1.15,\n               label.cex = 1.25,\n               intercepts = FALSE)\n\n\nmy_label_list &lt;- list(list(node = \"Satisfaction_A\", to = \"Satisfaction (W)\"),\n                      list(node = \"Satisfaction_P\", to = \"Satisfaction (H)\"),\n                      list(node = \"OtherPos_A\", to = \"View\\nPartner (W)\"),\n                      list(node = \"OtherPos_P\", to = \"View\\nPartner (H)\"))\np2 &lt;- change_node_label(p1, my_label_list)\np3 &lt;- mark_se(p2, apim2_fit, sep = \"\\n\")\nmy_position_list &lt;- c(\"Satisfaction_A ~ OtherPos_P\" = .25,\n                      \"Satisfaction_P ~ OtherPos_A\" = .25)\np4 &lt;- set_edge_label_position(p3, my_position_list)\nplot(p4)\n\n\n\n\nThe results are basically identical."
  },
  {
    "objectID": "posts/2020-03-27-julia-cfa/index.html",
    "href": "posts/2020-03-27-julia-cfa/index.html",
    "title": "Using Julia to Find MLE for a Factor Model",
    "section": "",
    "text": "I’ve been staying home for a bit more than a week now. While keep working on my research, I also think it may help fill my time by picking up some skills. I’ve been following the development of Julia but haven’t had the time to start learning it seriously, so this is the perfect time! As my first learning note, I’ve decided to try obtaining maximum likelihood estimates on some simulated data based on a one-factor model. I’m sure a lot of improvement can be made as this is really my first Julia notebook. But since to my knowledge there is currently no SEM packages in Julia, may be it could be a starting point."
  },
  {
    "objectID": "posts/2020-03-27-julia-cfa/index.html#set-seed",
    "href": "posts/2020-03-27-julia-cfa/index.html#set-seed",
    "title": "Using Julia to Find MLE for a Factor Model",
    "section": "Set Seed",
    "text": "Set Seed\nusing Random, Distributions\nRandom.seed!(123) # Setting the seed"
  },
  {
    "objectID": "posts/2020-03-27-julia-cfa/index.html#generate-univarate-normal-data",
    "href": "posts/2020-03-27-julia-cfa/index.html#generate-univarate-normal-data",
    "title": "Using Julia to Find MLE for a Factor Model",
    "section": "Generate Univarate Normal Data",
    "text": "Generate Univarate Normal Data\nLet’s generate some normal data. In Julia the Distribution.Normal function can be used to set a Normal distribution. The Base.rand function will then generate data from that class. I was amazed that the Distributions package supported a wide range of distributions.\nd = Normal()\nx = rand(d, 100)\n100-element Array{Float64,1}:\n  1.1902678809862768\n  2.04817970778924\n  1.142650902867199\n  0.45941562040708034\n -0.396679079295223\n -0.6647125451916877\n  0.9809678267585334\n -0.07548306639775595\n  0.27381537121215616\n -0.19422906710572047\n -0.33936602980781916\n -0.84387792144707\n -0.8889357468973064\n  ⋮\n -0.7339610029444202\n  0.45939803668120377\n  1.7061863874321828\n  0.6784427697934589\n  0.28717953880710856\n  1.0681555054109295\n -0.3068768981211787\n -1.9202140874350073\n  1.6696020873668111\n -0.2135576214062456\n -0.16371133936712523\n -0.9029858060964956\nusing StatsPlots\ndensity(x)\n\n\n\nsvg"
  },
  {
    "objectID": "posts/2020-03-27-julia-cfa/index.html#generate-multivariate-normal-data",
    "href": "posts/2020-03-27-julia-cfa/index.html#generate-multivariate-normal-data",
    "title": "Using Julia to Find MLE for a Factor Model",
    "section": "Generate Multivariate Normal Data",
    "text": "Generate Multivariate Normal Data\nd = MvNormal([1 0.5\n              0.5 1])\nZeroMeanFullNormal(\ndim: 2\nμ: [0.0, 0.0]\nΣ: [1.0 0.5; 0.5 1.0]\n)\nx = rand(d, 1000)\n2×1000 Array{Float64,2}:\n  0.173994   1.25713  -1.33584   …  1.22644   0.207192   0.254031\n -0.371218  -0.22681  -0.730016     0.944955  0.606183  -0.000434324\n# Covariance matrix\nusing LinearAlgebra\ncov(x')\n2×2 Array{Float64,2}:\n 0.987069  0.473074\n 0.473074  0.949026\nThe above shows how Julia can simulate multivariate normal data. Now, generate some cfa data\nΛ = [.7\n     .7\n     .7\n     .7]\nΣ = Λ*Λ' + Diagonal(ones(4) * .51)\n4×4 Array{Float64,2}:\n 1.0   0.49  0.49  0.49\n 0.49  1.0   0.49  0.49\n 0.49  0.49  1.0   0.49\n 0.49  0.49  0.49  1.0\n# Simulate data\ny = rand(MvNormal(Σ), 100)\nSy = cov(y')\n4×4 Array{Float64,2}:\n 0.846131  0.460656  0.418965  0.368367\n 0.460656  0.942118  0.396868  0.405196\n 0.418965  0.396868  0.98197   0.342197\n 0.368367  0.405196  0.342197  0.902086\nfunction loglik(θ, S = Sy)\n    Λ = θ[1:4]\n    Θ = Diagonal(θ[5:8])\n    Σ = Λ*Λ' + Θ\n    logdet(Σ) + tr(Σ\\Sy)\nend\nloglik (generic function with 2 methods)\n# Try the function\nloglik([.8, .7, .7, .7, .51, .51, .51, .51])\n2.8167873279580555\n# Find maximum likelihood estimates\nusing Optim\n@time optimize(loglik, [ones(4) * 0.7; ones(4) * 0.51], LBFGS())\n  0.030934 seconds (24.63 k allocations: 1.637 MiB)\n\n\n\n\n\n * Status: success\n\n * Candidate solution\n    Minimizer: [6.72e-01, 6.85e-01, 6.03e-01,  ...]\n    Minimum:   2.748551e+00\n\n * Found with\n    Algorithm:     L-BFGS\n    Initial Point: [7.00e-01, 7.00e-01, 7.00e-01,  ...]\n\n * Convergence measures\n    |x - x'|               = 7.06e-09 ≰ 0.0e+00\n    |x - x'|/|x'|          = 1.03e-08 ≰ 0.0e+00\n    |f(x) - f(x')|         = 1.33e-15 ≰ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = 4.85e-16 ≰ 0.0e+00\n    |g(x)|                 = 7.33e-11 ≤ 1.0e-08\n\n * Work counters\n    Seconds run:   0  (vs limit Inf)\n    Iterations:    12\n    f(x) calls:    33\n    ∇f(x) calls:   33\n# Use Hessians\nusing NLSolversBase, ForwardDiff\ntd = TwiceDifferentiable(loglik, [ones(4) * 0.7; ones(4) * 0.51]; autodiff = :forward)\n@time opt = optimize(td, [ones(4) * 0.7; ones(4) * 0.51])\n  0.540849 seconds (330.71 k allocations: 17.147 MiB, 2.58% gc time)\n\n\n\n\n\n * Status: success\n\n * Candidate solution\n    Minimizer: [6.72e-01, 6.85e-01, 6.03e-01,  ...]\n    Minimum:   2.748551e+00\n\n * Found with\n    Algorithm:     Newton's Method\n    Initial Point: [7.00e-01, 7.00e-01, 7.00e-01,  ...]\n\n * Convergence measures\n    |x - x'|               = 2.33e-06 ≰ 0.0e+00\n    |x - x'|/|x'|          = 3.40e-06 ≰ 0.0e+00\n    |f(x) - f(x')|         = 2.45e-11 ≰ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = 8.92e-12 ≰ 0.0e+00\n    |g(x)|                 = 1.20e-11 ≤ 1.0e-08\n\n * Work counters\n    Seconds run:   0  (vs limit Inf)\n    Iterations:    4\n    f(x) calls:    12\n    ∇f(x) calls:   12\n    ∇²f(x) calls:  4\n# Numerical Hessian\nOptim.minimizer(opt)\n@time ForwardDiff.hessian(loglik, Optim.minimizer(opt))\n  0.026639 seconds (8.52 k allocations: 540.367 KiB)\n\n\n\n\n\n8×8 Array{Float64,2}:\n  3.16649   -0.6156    -0.440022  …  -0.390832   -0.150853   -0.205811\n -0.6156     2.72063   -0.325774      1.09962    -0.174804   -0.124924\n -0.440022  -0.325774   2.24179      -0.260119    0.634179   -0.102112\n -0.38865   -0.380951  -0.237476     -0.184736   -0.101475    0.686983\n  1.48048   -0.46019   -0.264316      0.333658    0.106909    0.198808\n -0.390832   1.09962   -0.260119  …   2.64123     0.144781    0.0711164\n -0.150853  -0.174804   0.634179      0.144781    1.94531     0.0498062\n -0.205811  -0.124924  -0.102112      0.0711164   0.0498062   2.25437\n# Using NLSolverBase\n@time hess = NLSolversBase.hessian!(td, Optim.minimizer(opt))\n  0.000150 seconds (46 allocations: 80.734 KiB)\n\n\n\n\n\n8×8 Array{Float64,2}:\n  3.16649   -0.6156    -0.440022  …  -0.390832   -0.150853   -0.205811\n -0.6156     2.72063   -0.325774      1.09962    -0.174804   -0.124924\n -0.440022  -0.325774   2.24179      -0.260119    0.634179   -0.102112\n -0.38865   -0.380951  -0.237476     -0.184736   -0.101475    0.686983\n  1.48048   -0.46019   -0.264316      0.333658    0.106909    0.198808\n -0.390832   1.09962   -0.260119  …   2.64123     0.144781    0.0711164\n -0.150853  -0.174804   0.634179      0.144781    1.94531     0.0498062\n -0.205811  -0.124924  -0.102112      0.0711164   0.0498062   2.25437\n# Asymptotic covariance matrix\ndiag(inv(hess) * 2 / 100)\n8-element Array{Float64,1}:\n 0.008868570489264425\n 0.009937870865887575\n 0.010707718924937756\n 0.009921369374706349\n 0.00792975201139125\n 0.009846163164104109\n 0.011585717625197327\n 0.00995946113518446"
  },
  {
    "objectID": "posts/2022-07-30-multinomial-regression-in-stan/index.html",
    "href": "posts/2022-07-30-multinomial-regression-in-stan/index.html",
    "title": "Nominal Regression in STAN",
    "section": "",
    "text": "I was talking to a colleague about modeling nominal outcomes in STAN, and wrote up this example. Just put it here in case it’s helpful for anyone (probably myself in the future). This is based on an example I made for a course, where you can find the brms code for nominal regression. Please also check out the Multi-logit regression session on the Stan User’s guide."
  },
  {
    "objectID": "posts/2022-07-30-multinomial-regression-in-stan/index.html#load-packages",
    "href": "posts/2022-07-30-multinomial-regression-in-stan/index.html#load-packages",
    "title": "Nominal Regression in STAN",
    "section": "Load Packages",
    "text": "Load Packages\n\nlibrary(cmdstanr)\nlibrary(dplyr)\nlibrary(ggplot2)\n\nCheck out this paper: https://journals.sagepub.com/doi/full/10.1177/2515245918823199\n\nstemcell &lt;- read.csv(\"https://osf.io/vxw73/download\")\n\n\nstemcell |&gt;\n    ggplot(aes(x = rating)) +\n    geom_bar() +\n    facet_wrap(~ gender)\n\n\n\n\nhttps://www.thearda.com/archive/files/Codebooks/GSS2006_CB.asp\nThe outcome is attitude towards stem cells research, and the predictor is gender.\n\nRecently, there has been controversy over whether the government should provide any funds at all for scientific research that uses stem cells taken from human embryos. Would you say the government . . .\n\n\n1 = Definitely, should fund such research\n2 = Probably should fund such research\n3 = Probably should not fund such research\n4 = Definitely should not fund such research"
  },
  {
    "objectID": "posts/2022-07-30-multinomial-regression-in-stan/index.html#nominal-logistic-regression",
    "href": "posts/2022-07-30-multinomial-regression-in-stan/index.html#nominal-logistic-regression",
    "title": "Nominal Regression in STAN",
    "section": "Nominal Logistic Regression",
    "text": "Nominal Logistic Regression\nOrdinal regression is a special case of nominal regression with the proportional odds assumption."
  },
  {
    "objectID": "posts/2022-07-30-multinomial-regression-in-stan/index.html#model",
    "href": "posts/2022-07-30-multinomial-regression-in-stan/index.html#model",
    "title": "Nominal Regression in STAN",
    "section": "Model",
    "text": "Model\n\\[\\begin{align}\n  \\text{rating}_i & \\sim \\mathrm{Categorical}(\\pi^1_{i}, \\pi^2_{i}, \\pi^3_{i}, \\pi^4_{i})  \\\\\n  \\pi^1_{i} & = \\frac{1}{\\exp(\\eta^2_{i}) + \\exp(\\eta^3_{i}) + \\exp(\\eta^4_{i}) + 1}  \\\\\n  \\pi^2_{i} & = \\frac{\\exp(\\eta^2_{i})}{\\exp(\\eta^2_{i}) + \\exp(\\eta^3_{i}) + \\exp(\\eta^4_{i}) + 1}  \\\\\n  \\pi^3_{i} & = \\frac{\\exp(\\eta^3_{i})}{\\exp(\\eta^2_{i}) + \\exp(\\eta^3_{i}) + \\exp(\\eta^4_{i}) + 1}  \\\\\n  \\pi^4_{i} & = \\frac{\\exp(\\eta^4_{i})}{\\exp(\\eta^2_{i}) + \\exp(\\eta^3_{i}) + \\exp(\\eta^4_{i}) + 1}  \\\\\n  \\eta^2_{i} & = \\beta^2_{0} + \\beta^2_{1} \\text{male}_{i}  \\\\\n  \\eta^3_{i} & = \\beta^3_{0} + \\beta^3_{1} \\text{male}_{i} \\\\\n  \\eta^4_{i} & = \\beta^4_{0} + \\beta^4_{1} \\text{male}_{i} \\\\\n\\end{align}\\]\n\nmod &lt;- cmdstan_model(\"nominal_reg.stan\")\nmod\n\n//\n// This Stan program defines a nominal regression model.\n//\n// It is based on\n//   https://mc-stan.org/docs/stan-users-guide/multi-logit.html\n//\n\n// The input data is a vector 'y' of length 'N'.\ndata {\n  int&lt;lower=0&gt; K;  // number of response categories\n  int&lt;lower=0&gt; N;  // number of observations (data rows)\n  int&lt;lower=0&gt; D;  // number of predictors\n  array[N] int&lt;lower=1, upper=K&gt; y;  // response vector\n  matrix[N, D] x;  // predictor matrix\n}\n\ntransformed data {\n  vector[D] zeros = rep_vector(0, D);\n}\n\n// The parameters accepted by the model.\nparameters {\n  vector[K - 1] b0_raw;  // intercept for second to last categories\n  matrix[D, K - 1] beta_raw;\n}\n\n// The model to be estimated.\nmodel {\n  // Add zeros for reference category\n  vector[K] b0 = append_row(0, b0_raw);\n  matrix[D, K] beta = append_col(zeros, beta_raw);\n  to_vector(beta_raw) ~ normal(0, 5);\n  y ~ categorical_logit_glm(x, b0, beta);\n}\n\n\n\nstan_dat &lt;- with(stemcell,\n     list(K = n_distinct(rating),\n          N = length(rating),\n          D = 1,\n          y = rating,\n          x = matrix(as.integer(gender == \"male\")))\n)\n\n\n# Draw samples\nfit &lt;- mod$sample(data = stan_dat, seed = 123, chains = 4, \n                  parallel_chains = 2, refresh = 500,\n                  iter_sampling = 2000, iter_warmup = 2000)\n\n\nfit$summary() |&gt;\n    knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\nsd\nmad\nq5\nq95\nrhat\ness_bulk\ness_tail\n\n\n\n\nlp__\n-1035.3043988\n-1034.9800000\n1.7372773\n1.5863820\n-1038.6200000\n-1033.1495000\n1.000844\n3578.616\n5343.796\n\n\nb0_raw[1]\n0.4697455\n0.4689280\n0.1105168\n0.1110905\n0.2891216\n0.6530174\n1.001056\n4017.342\n5352.187\n\n\nb0_raw[2]\n-0.6797744\n-0.6782500\n0.1495865\n0.1485951\n-0.9299347\n-0.4383052\n1.000235\n4383.623\n5447.054\n\n\nb0_raw[3]\n-0.9695071\n-0.9694105\n0.1636312\n0.1642639\n-1.2425745\n-0.7015362\n1.000692\n4591.053\n5376.113\n\n\nbeta_raw[1,1]\n-0.1747948\n-0.1736585\n0.1666419\n0.1655399\n-0.4543450\n0.1009604\n1.001103\n4012.471\n5258.483\n\n\nbeta_raw[1,2]\n-0.0074333\n-0.0060478\n0.2177309\n0.2196757\n-0.3693494\n0.3494878\n1.000258\n4256.543\n5283.208\n\n\nbeta_raw[1,3]\n-0.1722835\n-0.1734355\n0.2474491\n0.2485277\n-0.5740370\n0.2324758\n1.000817\n4722.193\n5370.922\n\n\n\n\n\nCompare to brms:\n\nlibrary(brms)\nbrm(rating ~ gender, data = stemcell, family = categorical(link = \"logit\"),\n    file = \"mlogit\")\n\n Family: categorical \n  Links: mu2 = logit; mu3 = logit; mu4 = logit \nFormula: rating ~ gender \n   Data: stemcell (Number of observations: 829) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nmu2_Intercept      0.47      0.11     0.26     0.69 1.00     3990     3108\nmu3_Intercept     -0.67      0.15    -0.97    -0.39 1.00     3809     3130\nmu4_Intercept     -0.96      0.17    -1.29    -0.63 1.00     4027     3031\nmu2_gendermale    -0.18      0.17    -0.51     0.15 1.00     4272     2829\nmu3_gendermale    -0.01      0.22    -0.43     0.41 1.00     4141     2902\nmu4_gendermale    -0.17      0.25    -0.67     0.34 1.00     3922     2937\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe estimates are pretty much the same."
  },
  {
    "objectID": "posts/2020-06-12-weighted-least-squares.en/index.html",
    "href": "posts/2020-06-12-weighted-least-squares.en/index.html",
    "title": "Weighted Least Squares",
    "section": "",
    "text": "Recently I was working on a revision for a paper that involves structural equation modeling with categorical observed variables, and it uses a robust variant of weighted least square (also called asymptotic distribution free) estimators. Even though I had some basic understanding of WLS, the experience made me aware that I hadn’t fully understand how it was implemented in software. Therefore, I decided to write a (not so) short note to show how the polychoric correlation matrix can be estimated, and then how the weighted least squares estimation can be applied."
  },
  {
    "objectID": "posts/2020-06-12-weighted-least-squares.en/index.html#load-packages",
    "href": "posts/2020-06-12-weighted-least-squares.en/index.html#load-packages",
    "title": "Weighted Least Squares",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(lavaan)\nlibrary(ggplot2)  # for plotting\nlibrary(polycor)  # for estimating polychoric correlations\nlibrary(mvtnorm)\nlibrary(numDeriv)  # getting numerical derivatives\ntheme_set(theme_classic() +\n            theme(panel.grid.major.y = element_line(color = \"grey85\")))"
  },
  {
    "objectID": "posts/2020-06-12-weighted-least-squares.en/index.html#data",
    "href": "posts/2020-06-12-weighted-least-squares.en/index.html#data",
    "title": "Weighted Least Squares",
    "section": "Data",
    "text": "Data\nThe data will be three variables from the classic Holzinger & Swineford (1939) data set. The variables are\n\nx1: Visual perception\nx2: Cubes\nx3: Lozenges\n\nTo illustrate categorical variables, I’ll categorize each of the variables into three categories using the cut function in R.\n\n# Holzinger and Swineford (1939) example\nHS3 &lt;- HolzingerSwineford1939[ , c(\"x1\",\"x2\",\"x3\")]\n# ordinal version, with three categories\nHS3ord &lt;- as.data.frame(lapply(HS3, function(v) {\n  as.ordered(cut(v, breaks = 3, labels = FALSE))\n}))\n\nHere is the contingency table of the first two items\n\ntable(HS3ord[1:2])\n\n#&gt;    x2\n#&gt; x1    1   2   3\n#&gt;   1   4  19   3\n#&gt;   2  12 162  41\n#&gt;   3   2  33  25"
  },
  {
    "objectID": "posts/2020-06-12-weighted-least-squares.en/index.html#polychoric-correlations",
    "href": "posts/2020-06-12-weighted-least-squares.en/index.html#polychoric-correlations",
    "title": "Weighted Least Squares",
    "section": "Polychoric Correlations",
    "text": "Polychoric Correlations\nTo use WLS, we first assume that each categorical variable has an underlying normal variate that has been categorized, and usually it’s assumed to be a standard normal variable so that the scale can be fixed. Based on the contingency table for each pair of observed variables, we infer the correlation of the corresponding pair of underlying response variates. That correlation is called the polychoric correlation.\n\nlavaan\nThere are different ways to estimate the polychoric correlations, but generally it involves numerical optimization to find maximum likelihood or psuedo maximum likelihood values. In lavaan it is easy to estimate that:\n\n# polychoric correlations, two-stage estimation\npcor_lavaan &lt;- lavCor(HS3ord, ordered = names(HS3ord), \n                      se = \"robust.sem\", output = \"fit\")\nsubset(\n  parameterestimates(pcor_lavaan), \n  op %in% c(\"~~\", \"|\")  # polychoric correlations and thresholds\n)\n\n#&gt;    lhs op rhs    est    se       z pvalue ci.lower ci.upper\n#&gt; 1   x1 ~~  x1  1.000 0.000      NA     NA    1.000    1.000\n#&gt; 2   x2 ~~  x2  1.000 0.000      NA     NA    1.000    1.000\n#&gt; 3   x3 ~~  x3  1.000 0.000      NA     NA    1.000    1.000\n#&gt; 4   x1 ~~  x2  0.317 0.070   4.534      0    0.180    0.455\n#&gt; 5   x1 ~~  x3  0.508 0.060   8.484      0    0.391    0.625\n#&gt; 6   x2 ~~  x3  0.304 0.066   4.616      0    0.175    0.433\n#&gt; 7   x1  |  t1 -1.363 0.103 -13.239      0   -1.565   -1.162\n#&gt; 8   x1  |  t2  0.844 0.083  10.224      0    0.682    1.006\n#&gt; 9   x2  |  t1 -1.556 0.115 -13.508      0   -1.782   -1.331\n#&gt; 10  x2  |  t2  0.741 0.080   9.259      0    0.584    0.898\n#&gt; 11  x3  |  t1 -0.353 0.074  -4.766      0   -0.498   -0.208\n#&gt; 12  x3  |  t2  0.626 0.078   8.047      0    0.473    0.778\n\n\nThe default in lavaan uses a two-stage estimator that first obtains the maximum likelihood estimate of the thresholds, and then obtain the polychoric correlation using the DWLS estimator with robust standard errors, which will be further discussed.\n\nThresholds\nThe thresholds are the cut points in the underlying standard normal distribution. For example, the proportions for x1 are\n\n(prop_x1 &lt;- prop.table(table(HS3ord$x1)))\n\n#&gt; \n#&gt;          1          2          3 \n#&gt; 0.08637874 0.71428571 0.19933555\n\n\nThis suggests that a sensible way to estimate these cut points is\n\n(thresholds_x1 &lt;- qnorm(cumsum(prop_x1)))\n\n#&gt;         1         2         3 \n#&gt; -1.363397  0.843997       Inf\n\n\nwhich basically matches the estimates in lavaan. Do the same for x2:\n\n(thresholds_x2 &lt;- qnorm(cumsum(prop.table(table(HS3ord$x2)))))\n\n#&gt;          1          2          3 \n#&gt; -1.5564491  0.7413657        Inf\n\n\nNote that there are only two thresholds with three categories. This may be more readily seen in a graph:\n\nggplot(data.frame(xstar = c(-3, 3)), \n       aes(x = xstar)) + \n  stat_function(fun = dnorm) + \n  geom_segment(data = data.frame(tau = thresholds_x2[1:2], \n                                 density = dnorm(thresholds_x2[1:2])), \n               aes(x = tau, xend = tau, y = density, yend = 0))\n\n\n\n\nThe conversion using the cumulative normal density to obtain the thresholds is equivalent to obtaining \\(\\tau_j\\) for the \\(j\\)th threshold (\\(j = 1, 2\\)) as solving for \\[\\Phi(\\tau_j) - \\Phi(\\tau_{j - 1}) = \\frac{\\sum_i [x_i = j]}{N}, \\] where \\(\\Phi(\\cdot)\\) is the standard normal cdf (i.e., pnorm() in R), \\(\\sum_i [x_i = j] = n_j\\) is the count of \\(x_i\\) that equals \\(j\\), \\(\\Phi(\\tau_0) = 0\\), and \\(\\Phi(\\tau_3) = 1\\). Writing it this way would make it clearer how the standard errors (SEs) of the \\(\\tau\\)s can be obtained. In practice, most software uses maximum likelihood estimation and obtain the asymptotic SEs by inverting the Hessian. Here’s an example to get the same results as in lavaan for the thresholds of x1, which minimizes \\[Q(\\tau_1, \\tau_2) = \\sum_{j = 1}^3 n_j \\log [\\Phi(\\tau_j) - \\Phi(\\tau_{j - 1})]\\] (see Jin & Yang-Wallentin, 2017, for example.)\n\nlastx &lt;- function(x) x[length(x)]  # helper for last element\n# Minimization criterion\nQ &lt;- function(taus, ns = table(HS3ord[ , 1])) {\n  hs &lt;- pnorm(taus)\n  hs &lt;- c(hs[1], diff(hs), 1 - lastx(hs))\n  - sum(ns * log(hs))\n}\ntaus1_optim &lt;- optim(c(-1, 1), Q, hessian = TRUE)\n# Compare to lavaan\nlist(`lavaan` = parameterEstimates(pcor_lavaan)[7:8, c(\"est\", \"se\")], \n     `optim` = data.frame(\n       est = taus1_optim$par, \n       se = sqrt(diag(solve(taus1_optim$hessian)))\n     )\n)\n\n#&gt; $lavaan\n#&gt;      est    se\n#&gt; 7 -1.363 0.103\n#&gt; 8  0.844 0.083\n#&gt; \n#&gt; $optim\n#&gt;          est        se\n#&gt; 1 -1.3639182 0.1028333\n#&gt; 2  0.8440422 0.0824186\n\n\nThey are not exactly the same but are pretty close.\n\n\nPolychoric correlations\nWhereas the thresholds can be computed based on the proportions of each individual variable, the polychoric correlation needs the contingency table between two variables. The underlying variates are assumed to follow a bivariate normal distribution, which an example (with \\(r = .3\\)) shown below:\n\n# Helper function\nexpand_grid_matrix &lt;- function(x, y) {\n  cbind(rep(x, length(y)), \n        rep(y, each = length(x)))\n}\nx_pts &lt;- seq(-3, 3, length.out = 29)\ny_pts &lt;- seq(-3, 3, length.out = 29)\nxy_grid &lt;- expand_grid_matrix(x = x_pts, y = y_pts)\nexample_sigma &lt;- matrix(c(1, .3, .3, 1), nrow = 2)\nz_pts &lt;- dmvnorm(xy_grid, sigma = example_sigma)\nz_grid &lt;- matrix(z_pts, nrow = 29)\npersp(x_pts, y_pts, z_grid, theta = 15, phi = 30, expand = 0.5, \n      col = \"lightblue\", box = FALSE)\n\n\n\n\nWith the thresholds set, a bivariate normal distribution will be cut into 9 quadrants when each item has 3 categories:\n\nggplot(data = data.frame(x = xy_grid[ , 1], \n                         y = xy_grid[ , 2], \n                         z = z_pts), \n       aes(x, y, z = z)) + \n  geom_contour(breaks = c(0.02, 0.1)) + \n  geom_vline(xintercept = thresholds_x1) + \n  geom_hline(yintercept = thresholds_x2) + \n  labs(x = \"x1\", y = \"x2\")\n\n\n\n\nThe main goal is to find a correlation, \\(\\rho\\), such that the implied proportions match the observed contingency table as closely as possible:\n\ntable_x1x2 &lt;- as.data.frame(\n  with(HS3ord, \n       round(prop.table(table(x1, x2)) * 100, 2)\n  )\n)\nggplot(data = table_x1x2, aes(x = x1, y = x2)) + \n  geom_tile(aes(fill = Freq)) + \n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \n   space = \"Lab\", \n   name = \"\") +\n  geom_text(aes(label = Freq), color = \"black\", size = 4) +\n  theme_minimal() + \n  guides(fill = FALSE)\n\n#&gt; Warning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\n#&gt; of ggplot2 3.3.4.\n\n\n\n\n\nFor example, if \\(\\rho = .3\\), the implied proportions are\n\n# Bivariate normal probability\npbivariatenormal &lt;- function(lower, upper, rho) {\n  mvtnorm::pmvnorm(\n    lower = lower, \n    upper = upper, \n    corr = matrix(c(1, rho, rho, 1), nrow = 2)\n  )\n}\nlower_lims &lt;- expand_grid_matrix(c(-Inf, thresholds_x1[1:2]), \n                                 c(-Inf, thresholds_x2[1:2]))\nupper_lims &lt;- expand_grid_matrix(c(thresholds_x1[1:2], Inf), \n                                 c(thresholds_x2[1:2], Inf))\nprobs &lt;- \n  vapply(seq_len(nrow(lower_lims)), \n         function(i, r = .3) pbivariatenormal(lower_lims[i, ], \n                                              upper_lims[i, ], \n                                              r), \n         FUN.VALUE = numeric(1))\nmatrix(round(probs * 100, 2), nrow = 3, ncol = 3)\n\n#&gt;      [,1]  [,2]  [,3]\n#&gt; [1,] 1.27  6.59  0.78\n#&gt; [2,] 4.31 52.34 14.78\n#&gt; [3,] 0.40 12.17  7.36\n\n\nwhich is not too far away. An optimization algorithm for the (pseudo-) maximum likelihood estimates can be obtained by minimizing \\[\\sum_{j = 1}^3 \\sum_{k = 1}^3 p_{jk} \\log \\pi_{jk},\\]\n(Jin & Yang-Wallentin, 2017, p. 71)\nwhere \\(p_{jk}\\) is the observed proportions and \\(\\pi_{jk}\\) is the implied proportions with a given correlation \\(\\rho\\).\n\nlikelihood_pcor &lt;- function(rho, ns = table(HS3ord[ , 1:2]), \n                            taus = cbind(thresholds_x1[1:2], \n                                         thresholds_x2[1:2])) {\n  taus1 &lt;- taus[ , 1]\n  taus2 &lt;- taus[ , 2]\n  lower_lims &lt;- expand_grid_matrix(c(-Inf, taus1), \n                                   c(-Inf, taus2))\n  upper_lims &lt;- expand_grid_matrix(c(taus1, Inf), \n                                   c(taus2, Inf))\n  probs &lt;- \n    vapply(seq_len(nrow(lower_lims)), \n           function(i, r = rho) pbivariatenormal(lower_lims[i, ], \n                                                 upper_lims[i, ], \n                                                 r), \n           FUN.VALUE = numeric(1))\n  - sum(ns * log(probs))\n}\npcor_optim &lt;- \n  optim(0, likelihood_pcor, lower = -.995, upper = .995, method = \"L-BFGS-B\", \n      hessian = TRUE)\n# Compare to lavaan\nrbind(`lavaan` = parameterEstimates(pcor_lavaan)[4, c(\"est\", \"se\")], \n     `optim` = data.frame(\n       est = pcor_optim$par, \n       se = sqrt(1 / pcor_optim$hessian)\n     )\n)\n\n#&gt;          est    se\n#&gt; lavaan 0.317 0.070\n#&gt; optim  0.317 0.075\n\n\nThe SE estimates are different because optim uses maximum likelihood, whereas lavaan uses WLS-type estimates. You will see the values with ML in OpenMx is closer below.\n\n\n\nOpenMx\nWith OpenMx, the polychoric correlations can be estimated directly with maximum likelihood or weighted least squares. First, with DWLS that should give similar results as lavaan:\n\n# OpenMx\nlibrary(OpenMx)\npolychoric_mxmodel &lt;- \n  mxModel(model = \"polychoric\", \n          type = \"RAM\", \n          mxData(HS3ord, type = \"raw\"), \n          manifestVars = names(HS3ord), \n          mxPath(from = names(HS3ord), connect = \"unique.bivariate\", \n                 arrows = 2, free = TRUE, values = .3), \n          mxPath(from = names(HS3ord), \n                 arrows = 2, free = FALSE, values = 1), \n          mxPath(from = \"one\", to = names(HS3ord), \n                 arrows = 1, free = FALSE, values = 0), \n          mxThreshold(vars = names(HS3ord), nThresh = 2, \n                      free = TRUE, values = c(-1, 1)) \n          )\nsummary(\n  mxRun(\n    mxModel(polychoric_mxmodel, \n            mxFitFunctionWLS(\"DWLS\"))\n  )\n)\n\n#&gt; Summary of polychoric \n#&gt;  \n#&gt; free parameters:\n#&gt;                         name     matrix row col   Estimate  Std.Error\n#&gt; 1          polychoric.S[1,2]          S  x1  x2  0.3173173 0.06988377\n#&gt; 2          polychoric.S[1,3]          S  x1  x3  0.5079673 0.05978313\n#&gt; 3          polychoric.S[2,3]          S  x2  x3  0.3039566 0.06572188\n#&gt; 4 polychoric.Thresholds[1,1] Thresholds   1  x1 -1.3633968 0.10281052\n#&gt; 5 polychoric.Thresholds[2,1] Thresholds   2  x1  0.8439970 0.08241477\n#&gt; 6 polychoric.Thresholds[1,2] Thresholds   1  x2 -1.5564491 0.11503135\n#&gt; 7 polychoric.Thresholds[2,2] Thresholds   2  x2  0.7413657 0.07993884\n#&gt; 8 polychoric.Thresholds[1,3] Thresholds   1  x3 -0.3527812 0.07389738\n#&gt; 9 polychoric.Thresholds[2,3] Thresholds   2  x3  0.6256117 0.07762006\n#&gt; \n#&gt; Model Statistics: \n#&gt;                |  Parameters  |  Degrees of Freedom  |  Fit (r'wr units)\n#&gt;        Model:              9                      0                   NA\n#&gt;    Saturated:              9                      0                    0\n#&gt; Independence:              6                      3                   NA\n#&gt; Number of observations/statistics: 301/9\n#&gt; \n#&gt; chi-square:  χ² ( df=0 ) = 0,  p = 1\n#&gt; CFI: NA \n#&gt; TLI: 1   (also known as NNFI) \n#&gt; RMSEA:  0  [95% CI (NA, NA)]\n#&gt; Prob(RMSEA &lt;= 0.05): NA\n#&gt; To get additional fit indices, see help(mxRefModels)\n#&gt; timestamp: 2023-06-14 09:58:38 \n#&gt; Wall clock time: 0.07439637 secs \n#&gt; optimizer:  SLSQP \n#&gt; OpenMx version number: 2.21.8 \n#&gt; Need help?  See help(mxSummary)\n\n\nWith ML\n\nsummary(\n  mxRun(\n    mxModel(polychoric_mxmodel, \n            mxFitFunctionML())\n  )\n)\n\n#&gt; Summary of polychoric \n#&gt;  \n#&gt; free parameters:\n#&gt;                         name     matrix row col   Estimate  Std.Error A\n#&gt; 1          polychoric.S[1,2]          S  x1  x2  0.3166411 0.07631802  \n#&gt; 2          polychoric.S[1,3]          S  x1  x3  0.5080022 0.06299348  \n#&gt; 3          polychoric.S[2,3]          S  x2  x3  0.3090142 0.07183812  \n#&gt; 4 polychoric.Thresholds[1,1] Thresholds   1  x1 -1.3737991 0.10400215  \n#&gt; 5 polychoric.Thresholds[2,1] Thresholds   2  x1  0.8411834 0.08187341  \n#&gt; 6 polychoric.Thresholds[1,2] Thresholds   1  x2 -1.5475371 0.11402095  \n#&gt; 7 polychoric.Thresholds[2,2] Thresholds   2  x2  0.7410892 0.08015387  \n#&gt; 8 polychoric.Thresholds[1,3] Thresholds   1  x3 -0.3430065 0.07407156  \n#&gt; 9 polychoric.Thresholds[2,3] Thresholds   2  x3  0.6277570 0.07713488  \n#&gt; \n#&gt; Model Statistics: \n#&gt;                |  Parameters  |  Degrees of Freedom  |  Fit (-2lnL units)\n#&gt;        Model:              9                    894              1502.269\n#&gt;    Saturated:              9                    894                    NA\n#&gt; Independence:              6                    897                    NA\n#&gt; Number of observations/statistics: 301/903\n#&gt; \n#&gt; Information Criteria: \n#&gt;       |  df Penalty  |  Parameters Penalty  |  Sample-Size Adjusted\n#&gt; AIC:       -285.731               1520.269                 1520.888\n#&gt; BIC:      -3599.888               1553.633                 1525.090\n#&gt; CFI: NA \n#&gt; TLI: 1   (also known as NNFI) \n#&gt; RMSEA:  0  [95% CI (NA, NA)]\n#&gt; Prob(RMSEA &lt;= 0.05): NA\n#&gt; To get additional fit indices, see help(mxRefModels)\n#&gt; timestamp: 2023-06-14 09:58:39 \n#&gt; Wall clock time: 0.1355882 secs \n#&gt; optimizer:  SLSQP \n#&gt; OpenMx version number: 2.21.8 \n#&gt; Need help?  See help(mxSummary)\n\n\nThe \\(p(p - 1) / 2 \\times p(p - 1) / 2\\) asymptotic covariance matrix of the polychoric correlations will be used to obtain robust standard errors with the WLS estimators. I’ll see the one from lavaan for consistency.\n\n(acov_pcor &lt;- vcov(pcor_lavaan)[1:3, 1:3])\n\n#&gt;             x1~~x2       x1~~x3       x2~~x3\n#&gt; x1~~x2 0.004899261 0.0011380143 0.0018417210\n#&gt; x1~~x3 0.001138014 0.0035854771 0.0005619927\n#&gt; x2~~x3 0.001841721 0.0005619927 0.0043343069\n\n\nI’ll now move to WLS."
  },
  {
    "objectID": "posts/2020-06-12-weighted-least-squares.en/index.html#weighted-least-squares-estimation",
    "href": "posts/2020-06-12-weighted-least-squares.en/index.html#weighted-least-squares-estimation",
    "title": "Weighted Least Squares",
    "section": "Weighted Least Squares Estimation",
    "text": "Weighted Least Squares Estimation\nThe WLS estimator in SEM has a discrepancy function \\[F(\\boldsymbol{\\mathbf{\\theta}}) = (\\hat{\\boldsymbol{\\mathbf{\\rho}}} - \\boldsymbol{\\mathbf{\\rho}}(\\boldsymbol{\\mathbf{\\theta}}))^\\top \\hat{\\boldsymbol{\\mathbf{W}}} (\\hat{\\boldsymbol{\\mathbf{\\rho}}} - \\boldsymbol{\\mathbf{\\rho}}(\\boldsymbol{\\mathbf{\\theta}})), \\] where \\(\\hat{\\boldsymbol{\\mathbf{\\rho}}}\\) is a column vector of the estimated unique polychoric correlations, \\(bv \\rho(\\boldsymbol{\\mathbf{\\theta}})\\) is the vector of model-implied polychoric correlations given the model parameters \\(\\boldsymbol{\\mathbf{\\theta}}\\), and \\(\\hat{\\boldsymbol{\\mathbf{W}}}\\) is some weight matrix. The WLS estimator uses the inverse of the asymptotic covariance matrix of the polychoric correlations, i.e., \\(\\hat{\\boldsymbol{\\mathbf{W}}} = \\hat{\\boldsymbol{\\mathbf{\\Sigma}}}^{-1}_{\\rho \\rho}\\). However, when the number of variables is large, inverting this large matrix is computationally demanding, and previous studies have shown that WLS did not work well until the sample size is large (e.g., \\(&gt; 2,000\\)).\nA more popular variant is to instead use only the diagonals in \\(\\hat{\\boldsymbol{\\mathbf{\\Sigma}}}^{-1}_{\\rho \\rho}\\) to form the weight matrix, which requires only taking inverse of the individual elements. In other words, \\(\\hat{\\boldsymbol{\\mathbf{W}}} = \\mathrm{diag} \\hat{\\boldsymbol{\\mathbf{\\Sigma}}}^{-1}_{\\rho \\rho}\\). This is called the diagonally-weighted least squares (DWLS) estimation. In Mplus and lavaan, there are variants such as WLSM, WLSMV, etc, but they differ mainly in the test statistics computed, while the parameter estimates are all based on the DWLS estimator.\n\nOne-factor model\nAs an example, consider the one-factor model:\n\n# One-factor model\nonefactor_fit &lt;- \n  cfa(' f =~ x1 + x2 + x3 ', ordered = c(\"x1\", \"x2\", \"x3\"), \n    data = HS3ord, std.lv = TRUE, estimator = \"WLSMV\")\n\nAside from the threshold parameters, which was estimated in the first stage, the model only has three loading parameter \\(\\boldsymbol{\\mathbf{\\lambda}}\\). To obtain the estimates from scratch, we can use the estimated polychoric correlation and the diagonal of the asymptotic covariance matrix:\n\nrhos_hat &lt;- coef(pcor_lavaan)[1:3]\nacov_rhos &lt;- vcov(pcor_lavaan)[1:3, 1:3]\nase_rhos &lt;- sqrt(diag(acov_rhos))\n\nDefine the \\(\\boldsymbol{\\mathbf{\\rho}}(\\boldsymbol{\\mathbf{\\theta}})\\) function:\n\n# Function for model-implied correlation (delta parameterization)\nimplied_cor &lt;- function(lambdas) {\n  lambdalambdat &lt;- tcrossprod(lambdas)\n  lambdalambdat[lower.tri(lambdalambdat)]\n}\n# implied_cor(rep(.7, 3))  # example\n\nand define the discrepancy function. Note that with DWLS, \\[\\hat{\\boldsymbol{\\mathbf{W}}} = \\mathrm{diag} \\hat{\\boldsymbol{\\mathbf{\\Sigma}}}^{-1}_{\\rho \\rho} = \\hat{\\boldsymbol{\\mathbf{D}}}^{-1}_{\\rho \\rho} \\hat{\\boldsymbol{\\mathbf{D}}}^{-1}_{\\rho \\rho}, \\] where \\(\\hat{\\boldsymbol{\\mathbf{D}}}^{-1}_{\\rho \\rho}\\) is a diagonal matrix containing the asymptotic standard errors (i.e., square root of the variances).\n\n# Discrepancy function\ndwls_fitfunction &lt;- function(lambdas, \n                             sample_cors = rhos_hat, \n                             ase_cors = ase_rhos) {\n  crossprod(\n    (implied_cor(lambdas) - sample_cors) / ase_cors\n  )\n}\n# Optimize\noptim_lambdas &lt;- optim(rep(.7, 3), dwls_fitfunction)\n# Compare to lavaan\ncbind(`lavaan` = coef(onefactor_fit)[1:3], \n     `optim` = optim_lambdas$par\n)\n\n#&gt;          lavaan     optim\n#&gt; f=~x1 0.7283664 0.7283460\n#&gt; f=~x2 0.4357404 0.4357493\n#&gt; f=~x3 0.6974518 0.6974572\n\n\n\n\nStandard Errors\nThe discussion of this section draws on the materials in Bollen & Maydeu-Olivares (2007). Using a first-order approximation, the asymptotic covariance matrix of the WLS estimator is \\((\\boldsymbol{\\mathbf{\\Delta}}^\\top (\\boldsymbol{\\mathbf{\\theta}}) \\boldsymbol{\\mathbf{\\Sigma}}^{-1}_{\\rho \\rho}\\boldsymbol{\\mathbf{\\Delta}})^{-1}\\), where \\(\\boldsymbol{\\mathbf{\\Delta }}= \\partial \\boldsymbol{\\mathbf{\\rho}}(\\boldsymbol{\\mathbf{\\theta}}) / \\partial \\boldsymbol{\\mathbf{\\theta}}^\\top\\) is the matrix of derivatives with respect to the model parameters. However, in DWLS the full matrix is not used, so the asymptotic covariance should be corrected using a sandwich-type estimator as \\[\\boldsymbol{\\mathbf{H}} \\boldsymbol{\\mathbf{\\Sigma}}_{\\rho \\rho} \\boldsymbol{\\mathbf{H}}^\\top,\\] where \\(\\boldsymbol{\\mathbf{H}} = (\\boldsymbol{\\mathbf{\\Delta}}^\\top \\boldsymbol{\\mathbf{W}} \\boldsymbol{\\mathbf{\\Delta}})^{-1} \\boldsymbol{\\mathbf{\\Delta}}^\\top \\boldsymbol{\\mathbf{W}}\\). This does not involve inversion of the full \\(\\boldsymbol{\\mathbf{\\Sigma}}_{\\rho \\rho}\\) matrix, so it’s computational less demanding. This is how the standard errors are obtained with the WLSM and the WLSMV estimators. In lavaan, this also corresponds to the se = \"robust.sem\" option (which is the default with WLSMV).\n\n# Derivatives\nDelta &lt;- numDeriv::jacobian(implied_cor, optim_lambdas$par)\n# H Matrix\nH &lt;- solve(crossprod(Delta / ase_rhos), t(Delta / ase_rhos^2))\n# Asymptotic covariance matrix based on the formula\nH %*% acov_pcor %*% t(H)\n\n#&gt;              [,1]          [,2]          [,3]\n#&gt; [1,]  0.010358799 -0.0003888680 -0.0055221428\n#&gt; [2,] -0.000388868  0.0059929608 -0.0001132114\n#&gt; [3,] -0.005522143 -0.0001132114  0.0078359250\n\n# Compare to lavaan\nvcov(onefactor_fit)[1:3, 1:3]\n\n#&gt;               f=~x1         f=~x2         f=~x3\n#&gt; f=~x1  0.0103593905 -0.0003890896 -0.0055224662\n#&gt; f=~x2 -0.0003890896  0.0059928338 -0.0001129392\n#&gt; f=~x3 -0.0055224662 -0.0001129392  0.0078359251\n\n\nSo the results are essentially the same as in lavaan. The asymptotic standard errors can then be obtained as the square roots of the diagonal elements:\n\nsqrt(diag(H %*% acov_pcor %*% t(H)))\n\n#&gt; [1] 0.10177818 0.07741422 0.08852076"
  },
  {
    "objectID": "posts/2020-06-12-weighted-least-squares.en/index.html#final-thoughts",
    "href": "posts/2020-06-12-weighted-least-squares.en/index.html#final-thoughts",
    "title": "Weighted Least Squares",
    "section": "Final thoughts",
    "text": "Final thoughts\nSo that’s what I have learned with the WLS estimators, and I felt like I finally got a better understanding of it. It reminds me things I have learned about the GLS estimator in the regression context (and I do wonder why it’s been called WLS in SEM given that in the context of regression, WLS generally refers to the use of a diagonal weight matrix; perhaps that’s the reason we now use a diagonal weight matrix). There are things I may further explore, like doing it on the Theta parameterization instead of the Delta parameterization in this post, and dealing with the test statistics. But I will need to deal with the revision first."
  },
  {
    "objectID": "posts/2021-03-29-mixed-factorial-anova-in-r/index.html",
    "href": "posts/2021-03-29-mixed-factorial-anova-in-r/index.html",
    "title": "Mixed Factorial ANOVA in R",
    "section": "",
    "text": "library(afex)\n\nLoading required package: lme4\n\n\nLoading required package: Matrix\n\n\n************\nWelcome to afex. For support visit: http://afex.singmann.science/\n\n\n- Functions for ANOVAs: aov_car(), aov_ez(), and aov_4()\n- Methods for calculating p-values with mixed(): 'S', 'KR', 'LRT', and 'PB'\n- 'afex_aov' and 'mixed' objects can be passed to emmeans() for follow-up tests\n- Get and set global package options with: afex_options()\n- Set sum-to-zero contrasts globally: set_sum_contrasts()\n- For example analyses see: browseVignettes(\"afex\")\n************\n\n\n\nAttaching package: 'afex'\n\n\nThe following object is masked from 'package:lme4':\n\n    lmer\n\nlibrary(tidyr)\n\n\nAttaching package: 'tidyr'\n\n\nThe following objects are masked from 'package:Matrix':\n\n    expand, pack, unpack"
  },
  {
    "objectID": "posts/2021-03-29-mixed-factorial-anova-in-r/index.html#data-import",
    "href": "posts/2021-03-29-mixed-factorial-anova-in-r/index.html#data-import",
    "title": "Mixed Factorial ANOVA in R",
    "section": "Data Import",
    "text": "Data Import\n\ncharisma_data &lt;- read.csv(\"charisma_ugly.csv\")\n# Show the first six rows\nhead(charisma_data)\n\n  participant gender ug_high ug_some ug_none\n1         P01   Male      67      50      47\n2         P02   Male      53      48      46\n3         P03   Male      48      48      48\n4         P04   Male      58      40      53\n5         P05   Male      57      50      45\n6         P06   Male      51      42      43"
  },
  {
    "objectID": "posts/2021-03-29-mixed-factorial-anova-in-r/index.html#mixed-anova",
    "href": "posts/2021-03-29-mixed-factorial-anova-in-r/index.html#mixed-anova",
    "title": "Mixed Factorial ANOVA in R",
    "section": "Mixed ANOVA",
    "text": "Mixed ANOVA\nIn R, one needs to first convert the data to a long format, so that each row represents an observation (instead of a person). In this data, there were 60 observations.\n\n# First, convert to long format data\nlong_data &lt;- tidyr::pivot_longer(\n  charisma_data,\n  cols = ug_high:ug_none,\n  # name of IV\n  names_to = \"charisma\",\n  # remove the \"ug_\" prefix\n  names_prefix = \"ug_\", \n  # name of DV\n  values_to = \"rating\"\n)\nlong_data\n\n# A tibble: 60 × 4\n   participant gender charisma rating\n   &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt;     &lt;int&gt;\n 1 P01         Male   high         67\n 2 P01         Male   some         50\n 3 P01         Male   none         47\n 4 P02         Male   high         53\n 5 P02         Male   some         48\n 6 P02         Male   none         46\n 7 P03         Male   high         48\n 8 P03         Male   some         48\n 9 P03         Male   none         48\n10 P04         Male   high         58\n# ℹ 50 more rows\n\n\n\nmixed_anova &lt;- aov_ez(\n  id = \"participant\",\n  dv = \"rating\",\n  data = long_data,\n  between = \"gender\",\n  within = \"charisma\"\n)\n\nConverting to factor: gender\n\n\nContrasts set to contr.sum for the following variables: gender\n\n\n\ntest_sphericity(mixed_anova)\n\nWarning: Functionality has moved to the 'performance' package.\nCalling 'performance::check_sphericity()'.\n\n\nWarning in summary.Anova.mlm(object$Anova, multivariate = FALSE): HF eps &gt; 1\ntreated as 1\n\n\nOK: Data seems to be spherical (p &gt; 0.612).\n\n\nWhen one or more independent variable is a within-subject variable, mixed or repeated-measure ANOVA requires testing the assumption of sphericity. Here, the Mauchly’s test of sphericity provided insufficient evidence that the assumption of sphericity was violated, \\(p = .612\\).\n\nANOVA Table\n\nmixed_anova\n\nAnova Table (Type 3 tests)\n\nResponse: rating\n           Effect          df   MSE          F  ges p.value\n1          gender       1, 18 25.42  71.82 *** .586   &lt;.001\n2        charisma 1.89, 34.09 24.42 167.84 *** .857   &lt;.001\n3 gender:charisma 1.89, 34.09 24.42  58.10 *** .676   &lt;.001\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\nSphericity correction method: GG \n\n\nNote that the result by default applies the Greenhousse-Geisser correction for violations of the sphericity assumption, but since the Mauchly’s test was not statistically significant, the results would be similar with or without the correction. Also note that the result here reports the generalized \\(\\eta^2\\) (the ges column), which is different from some other software. See the documentation of the ?afex::nice page.\n\n\nPlot\n\nafex_plot(mixed_anova, x = \"charisma\", trace = \"gender\")\n\nWarning: Panel(s) show a mixed within-between-design.\nError bars do not allow comparisons across all means.\nSuppress error bars with: error = \"none\""
  },
  {
    "objectID": "posts/2021-03-29-mixed-factorial-anova-in-r/index.html#sample-result-reporting",
    "href": "posts/2021-03-29-mixed-factorial-anova-in-r/index.html#sample-result-reporting",
    "title": "Mixed Factorial ANOVA in R",
    "section": "Sample Result Reporting",
    "text": "Sample Result Reporting\n\n\n\n(#tab:unnamed-chunk-3)\n\n\nTwo-Way ANOVA Statistics for Study Variables\n\n\n\n\nEffect\n\\(\\hat{\\eta}^2_G\\)\n\\(F\\)\n\\(\\mathit{df}^{\\mathrm{GG}}\\)\n\\(\\mathit{df}_{\\mathrm{res}}^{\\mathrm{GG}}\\)\n\\(\\mathit{MSE}\\)\n\\(p\\)\n\n\n\n\nGender\n.586\n71.82\n1\n18\n25.42\n&lt; .001\n\n\nCharisma\n.857\n167.84\n1.89\n34.09\n24.42\n&lt; .001\n\n\nGender \\(\\times\\) Charisma\n.676\n58.10\n1.89\n34.09\n24.42\n&lt; .001\n\n\n\n\n\nA two-way mixed ANOVA was conducted, with gender (female vs. male) as the between-subject independent variable and charisma (high, some, none) as the within-subject independent variable. There was a significant main effect of charisma, \\(F(1.89, 34.09) = 167.84\\), \\(\\mathit{MSE} = 24.42\\), \\(p &lt; .001\\), \\(\\hat{\\eta}^2_G = .857\\). As shown in the figure, overall female participants provided a higer rating than male participants. There was also a significant main effect of gender, \\(F(1, 18) = 71.82\\), \\(\\mathit{MSE} = 25.42\\), \\(p &lt; .001\\), \\(\\hat{\\eta}^2_G = .586\\). As shown in the figure, ratings were higher for partners who had high charisma. Finally, there was a significant charisma \\(\\times\\) gender interaction, \\(F(1.89, 34.09) = 58.10\\), \\(\\mathit{MSE} = 24.42\\), \\(p &lt; .001\\), \\(\\hat{\\eta}^2_G = .676\\). As shown in the figure, while the ratings were similar between female and male participants for partners who had some or no charisma, female participants gave higher ratings for the partner with high charisma, as compared to male participants."
  },
  {
    "objectID": "posts/2021-03-29-mixed-factorial-anova-in-r/index.html#footnotes",
    "href": "posts/2021-03-29-mixed-factorial-anova-in-r/index.html#footnotes",
    "title": "Mixed Factorial ANOVA in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nField, A. P. (2017). Discovering Statistics Using IBM SPSS Statistics (5th ed.). Sage.↩︎"
  },
  {
    "objectID": "posts/2022-05-12-confidence-intervals-for-multilevel-r-squared/index.html",
    "href": "posts/2022-05-12-confidence-intervals-for-multilevel-r-squared/index.html",
    "title": "Confidence Intervals for Multilevel R-Squared",
    "section": "",
    "text": "library(lme4)\n\nLoading required package: Matrix\n\nlibrary(MuMIn)  # for computing multilevel R-squared\nlibrary(r2mlm)  # another package for R-squared\n\nLoading required package: nlme\n\n\n\nAttaching package: 'nlme'\n\n\nThe following object is masked from 'package:lme4':\n\n    lmList\n\nlibrary(bootmlm)  # for multilevel bootstrapping\nlibrary(boot)  # for bootstrap CIs"
  },
  {
    "objectID": "posts/2022-05-12-confidence-intervals-for-multilevel-r-squared/index.html#load-packages",
    "href": "posts/2022-05-12-confidence-intervals-for-multilevel-r-squared/index.html#load-packages",
    "title": "Confidence Intervals for Multilevel R-Squared",
    "section": "",
    "text": "library(lme4)\n\nLoading required package: Matrix\n\nlibrary(MuMIn)  # for computing multilevel R-squared\nlibrary(r2mlm)  # another package for R-squared\n\nLoading required package: nlme\n\n\n\nAttaching package: 'nlme'\n\n\nThe following object is masked from 'package:lme4':\n\n    lmList\n\nlibrary(bootmlm)  # for multilevel bootstrapping\nlibrary(boot)  # for bootstrap CIs"
  },
  {
    "objectID": "posts/2022-05-12-confidence-intervals-for-multilevel-r-squared/index.html#an-example-multilevel-model",
    "href": "posts/2022-05-12-confidence-intervals-for-multilevel-r-squared/index.html#an-example-multilevel-model",
    "title": "Confidence Intervals for Multilevel R-Squared",
    "section": "An Example Multilevel Model",
    "text": "An Example Multilevel Model\n\nfm1 &lt;- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)\n\n\nNakagawa-Johnson-Schielzeth \\(R^2\\)\n\nr.squaredGLMM(fm1)\n\nWarning: 'r.squaredGLMM' now calculates a revised statistic. See the help page.\n\n\n           R2m       R2c\n[1,] 0.2786511 0.7992199\n\n\nThe marginal \\(R^2\\) considers the total variance accounted for due to the fixed effect associated with the predictors (Days in this example). See Nakagawa, Johnson, & Schielzeth (2017) for more information.\n\n\nRight-Sterba \\(R^2\\)\nMore fine-grained partitioning, as described in Rights & Sterba (2019)\n\nr2mlm(fm1)\n\n\n\n\n$Decompositions\n                     total\nfixed           0.27851304\nslope variation 0.08915267\nmean variation  0.43165365\nsigma2          0.20068063\n\n$R2s\n         total\nf   0.27851304\nv   0.08915267\nm   0.43165365\nfv  0.36766572\nfvm 0.79931937\n\n\nThe fixed part is the same as the marginal \\(R^2\\)."
  },
  {
    "objectID": "posts/2022-05-12-confidence-intervals-for-multilevel-r-squared/index.html#confidence-intervals-for-r2",
    "href": "posts/2022-05-12-confidence-intervals-for-multilevel-r-squared/index.html#confidence-intervals-for-r2",
    "title": "Confidence Intervals for Multilevel R-Squared",
    "section": "Confidence Intervals for \\(R^2\\)",
    "text": "Confidence Intervals for \\(R^2\\)\nNeither MuMIn::r.squaredGLMM() nor r2mlm::r2mlm() provided confidence intervals (CIs) for the \\(R^2\\), but general guidelines for effect size reporting would suggest always reporting CIs for point estimates of effect size, just like for any point estimates in statistics. We can use multilevel bootstrapping to get CIs.\nTo do bootstrap, first defines an R function that gives the target \\(R^2\\) statistics. We can do it for the marginal \\(R^2\\):\n\nmarginal_r2 &lt;- function(object) {\n  r.squaredGLMM(object)[[1]]\n}\nmarginal_r2(fm1)\n\n[1] 0.2786511"
  },
  {
    "objectID": "posts/2022-05-12-confidence-intervals-for-multilevel-r-squared/index.html#parametric-bootstrap",
    "href": "posts/2022-05-12-confidence-intervals-for-multilevel-r-squared/index.html#parametric-bootstrap",
    "title": "Confidence Intervals for Multilevel R-Squared",
    "section": "Parametric Bootstrap",
    "text": "Parametric Bootstrap\nThe lme4::bootMer() supports basic parametric multilevel bootstrapping\n\n# This takes about 30 sec on my computer\nboo01 &lt;- bootMer(fm1, FUN = marginal_r2, nsim = 999)\nboo01\n\n\nPARAMETRIC BOOTSTRAP\n\n\nCall:\nbootMer(x = fm1, FUN = marginal_r2, nsim = 999)\n\n\nBootstrap Statistics :\n     original      bias    std. error\nt1* 0.2786511 0.006541379  0.07553305\n\n\n\n19 message(s): boundary (singular) fit: see help('isSingular')\n7 warning(s): Model failed to converge with max|grad| = 0.00239607 (tol = 0.002, component 1) (and others)\n\n\nHere is the bootstrap distribution\n\nplot(boo01)\n\n\n\n\n\nBias-corrected estimate\nThe above shows that the sample estimate of \\(R^2\\) was upwardly biased. To correct for the bias, we can use the bootstrap bias-corrected estimate\n\n2 * boo01$t0 - mean(boo01$t)\n\n[1] 0.2721097\n\n\n\n\nConfidence intervals\nYou can get three types of bootstrap CIs (\"norm\", \"basic\", \"perc\") with bootMer:\n\nboot::boot.ci(boo01, index = 1, type = c(\"norm\", \"basic\", \"perc\"))\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 999 bootstrap replicates\n\nCALL : \nboot::boot.ci(boot.out = boo01, type = c(\"norm\", \"basic\", \"perc\"), \n    index = 1)\n\nIntervals : \nLevel      Normal              Basic              Percentile     \n95%   ( 0.1241,  0.4202 )   ( 0.1079,  0.4108 )   ( 0.1465,  0.4494 )  \nCalculations and Intervals on Original Scale"
  },
  {
    "objectID": "posts/2022-05-12-confidence-intervals-for-multilevel-r-squared/index.html#residual-bootstrap",
    "href": "posts/2022-05-12-confidence-intervals-for-multilevel-r-squared/index.html#residual-bootstrap",
    "title": "Confidence Intervals for Multilevel R-Squared",
    "section": "Residual Bootstrap",
    "text": "Residual Bootstrap\nThe bootmlm::bootstrap_mer() implements the residual bootstrap, which is robust to non-normality.\n\n# This takes about 30 sec on my computer\nboo02 &lt;- bootstrap_mer(fm1, FUN = marginal_r2, nsim = 999, type = \"residual\")\n\n18 message(s) : boundary (singular) fit: see help('isSingular')\n\n1 warning(s) in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv,     lbound = lower) : Model failed to converge with max|grad| = 0.00211361 (tol = 0.002, component 1)\n1 warning(s) in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv,     lbound = lower) : Model failed to converge with max|grad| = 0.00217517 (tol = 0.002, component 1)\n1 warning(s) in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv,     lbound = lower) : Model failed to converge with max|grad| = 0.002215 (tol = 0.002, component 1)\n1 warning(s) in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv,     lbound = lower) : Model failed to converge with max|grad| = 0.00255425 (tol = 0.002, component 1)\n1 warning(s) in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv,     lbound = lower) : Model failed to converge with max|grad| = 0.00258348 (tol = 0.002, component 1)\n1 warning(s) in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv,     lbound = lower) : Model failed to converge with max|grad| = 0.00275089 (tol = 0.002, component 1)\n1 warning(s) in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv,     lbound = lower) : Model failed to converge with max|grad| = 0.00410731 (tol = 0.002, component 1)\n1 warning(s) in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv,     lbound = lower) : Model failed to converge with max|grad| = 0.00438296 (tol = 0.002, component 1)\n1 warning(s) in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv,     lbound = lower) : Model failed to converge with max|grad| = 0.00940885 (tol = 0.002, component 1)\n1 warning(s) in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv,     lbound = lower) : Model failed to converge with max|grad| = 0.0109771 (tol = 0.002, component 1)\n\nboo02\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nbootstrap_mer(x = fm1, FUN = marginal_r2, nsim = 999, type = \"residual\")\n\n\nBootstrap Statistics :\n     original    bias    std. error\nt1* 0.2786511 0.0061661  0.08425842\n\n\nIn this example, the results are similar. The boostrap bias-corrected estimate of \\(R^2\\), and the three basic CIs, can similarly be computed as in parametric bootstrap.\n\nConfidence Intervals\nIn addition to the three CIs previously discussed, which are first-order accurate, we can also obtain CIs that are second-order accurate: (a) bias-corrected and accelerated (BCa) CI and (b) studentized CI (also called the bootstrap-\\(t\\) CI). For (a), it requires the influence value of the \\(R^2\\) function, whereas for (b), it requires an estimate of the sampling variance of the \\(R^2\\) estimate.\n\nInfluence value\n\n# Based on the group jackknife\ninf_val &lt;- bootmlm::empinf_mer(fm1, marginal_r2, index = 1)\n\n\n\nSampling variance with the numerical delta method\n\nThis part is a bit more technical; skip this if you’re not interested in the studentized CI.\n\nTo obtain an approximate sampling variance of the \\(R^2\\), it would be easier to use the r2mlm::r2mlm_manual() function to compute \\(R^2\\). We first write a function that computes \\(R^2\\) using input of the fixed and random effects:\n\nmanual_r2 &lt;- function(theta, data,\n                      # The following are model-specific\n                      wc = 2, bc = NULL, rc = 2,\n                      cmc = FALSE) {\n  n_wc &lt;- length(wc)\n  n_bc &lt;- length(bc) + 1\n  dim_rc &lt;- length(rc) + 1\n  n_rc &lt;- dim_rc * (dim_rc + 1) / 2\n  gam_w &lt;- theta[seq_len(n_wc)]\n  gam_b &lt;- theta[n_wc + seq_len(n_bc)]\n  tau &lt;- matrix(NA, nrow = dim_rc, ncol = dim_rc)\n  tau[lower.tri(tau, diag = TRUE)] &lt;- theta[n_wc + n_bc + seq_len(n_rc)]\n  tau2 &lt;- t(tau)\n  tau2[lower.tri(tau2)] &lt;- tau[lower.tri(tau)]\n  s2 &lt;- tail(theta, n = 1)\n  r2mlm_manual(data,\n               within_covs = wc,\n               between_covs = bc,\n               random_covs = 2,\n               gamma_w = gam_w,\n               gamma_b = gam_b,\n               Tau = tau2,\n               sigma2 = s2,\n               clustermeancentered = cmc,\n               bargraph = FALSE)$R2s[1, 1]\n}\ntheta_fm1 &lt;- c(fixef(fm1)[2], fixef(fm1)[1],\n               VarCorr(fm1)[[1]][c(1, 2, 4)], sigma(fm1)^2)\nmanual_r2(theta_fm1, data = fm1@frame)\n\n[1] 0.278513\n\n\nNow computes the numerical gradient\n\ngrad_fm1 &lt;- numDeriv::grad(manual_r2, x = theta_fm1, data = fm1@frame)\n\nWe also need the asymptotic covariance matrix of the fixed and random effects\n\nvcov_fixed &lt;- vcov(fm1)\nvcov_random &lt;- vcov_vc(fm1, sd_cor = FALSE, print_names = FALSE)\nvcov_fm1 &lt;- bdiag(vcov_fixed, vcov_random)\n# Need to re-arrange the first two columns\nvcov_fm1 &lt;- vcov_fm1[c(2, 1, 3:6), c(2, 1, 3:6)]\n\nNow apply the multivariate delta method\n\ncrossprod(grad_fm1, vcov_fm1) %*% grad_fm1\n\n1 x 1 Matrix of class \"dgeMatrix\"\n            [,1]\n[1,] 0.005919918\n\n\nWe now need a function that computes both \\(R^2\\) and the asymptotic sampling variance of it.\n\nmarginal_r2_with_var &lt;- function(object,\n                                 wc = 2, bc = NULL, rc = 2) {\n  dim_rc &lt;- length(rc) + 1\n  vc_mat &lt;- matrix(seq_len(dim_rc^2), nrow = dim_rc, ncol = dim_rc)\n  vc_index &lt;- vc_mat[lower.tri(vc_mat, diag = TRUE)]\n  theta_obj &lt;- c(fixef(object)[wc], fixef(object)[c(1, bc)],\n                 VarCorr(object)[[1]][vc_index], sigma(object)^2)\n  r2 &lt;- manual_r2(theta_obj, data = object@frame)\n  grad_obj &lt;- numDeriv::grad(manual_r2, x = theta_obj, data = object@frame)\n  # Need to re-arrange the order of the fixed effects\n  names_wc &lt;- names(object@frame)[wc]\n  names_bc &lt;- c(\"(Intercept)\", names(object@frame)[bc])\n  vcov_fixed &lt;- vcov(object)[c(names_wc, names_bc), c(names_wc, names_bc)]\n  vcov_random &lt;- vcov_vc(object, sd_cor = FALSE, print_names = FALSE)\n  vcov_obj &lt;- bdiag(vcov_fixed, vcov_random)\n  v_r2 &lt;- crossprod(grad_obj, vcov_obj) %*% grad_obj\n  c(r2, as.numeric(v_r2))\n}\nmarginal_r2_with_var(fm1)\n\n[1] 0.278513044 0.005919918\n\n\n\n\nFive Bootstrap CIs\nNow, we can do bootstrap again, using the new function that computes both the estimate and the asymptotic sampling variance\n\n# This takes quite a bit longer due to the need to compute variances\nboo03 &lt;- bootstrap_mer(fm1, FUN = marginal_r2_with_var, nsim = 999,\n                       type = \"residual\")\n\n16 message(s) : boundary (singular) fit: see help('isSingular')\n\n1 warning(s) in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv,     lbound = lower) : Model failed to converge with max|grad| = 0.00219088 (tol = 0.002, component 1)\n1 warning(s) in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv,     lbound = lower) : Model failed to converge with max|grad| = 0.00237722 (tol = 0.002, component 1)\n1 warning(s) in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv,     lbound = lower) : Model failed to converge with max|grad| = 0.00249764 (tol = 0.002, component 1)\n1 warning(s) in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv,     lbound = lower) : Model failed to converge with max|grad| = 0.00264528 (tol = 0.002, component 1)\n1 warning(s) in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv,     lbound = lower) : Model failed to converge with max|grad| = 0.00271037 (tol = 0.002, component 1)\n1 warning(s) in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv,     lbound = lower) : Model failed to converge with max|grad| = 0.00309034 (tol = 0.002, component 1)\n1 warning(s) in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv,     lbound = lower) : Model failed to converge with max|grad| = 0.00349693 (tol = 0.002, component 1)\n1 warning(s) in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv,     lbound = lower) : Model failed to converge with max|grad| = 0.00351591 (tol = 0.002, component 1)\n1 warning(s) in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv,     lbound = lower) : Model failed to converge with max|grad| = 0.00378687 (tol = 0.002, component 1)\n1 warning(s) in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv,     lbound = lower) : Model failed to converge with max|grad| = 0.00655311 (tol = 0.002, component 1)\n1 warning(s) in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv,     lbound = lower) : Model failed to converge with max|grad| = 0.0124035 (tol = 0.002, component 1)\n2 warning(s) in sqrt(diag(m)) : NaNs produced\n\nboo03\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nbootstrap_mer(x = fm1, FUN = marginal_r2_with_var, nsim = 999, \n    type = \"residual\")\n\n\nBootstrap Statistics :\n       original        bias    std. error\nt1* 0.278513044  0.0104280454 0.083466874\nt2* 0.005919918 -0.0002962602 0.001142685\n\n\nAnd then obtain five types of CIs\n\nboot::boot.ci(boo03, L = inf_val)\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 997 bootstrap replicates\n\nCALL : \nboot::boot.ci(boot.out = boo03, L = inf_val)\n\nIntervals : \nLevel      Normal              Basic             Studentized     \n95%   ( 0.1045,  0.4317 )   ( 0.0898,  0.4062 )   ( 0.0819,  0.4380 )  \n\nLevel     Percentile            BCa          \n95%   ( 0.1509,  0.4673 )   ( 0.1383,  0.4448 )  \nCalculations and Intervals on Original Scale"
  },
  {
    "objectID": "posts/2022-05-12-confidence-intervals-for-multilevel-r-squared/index.html#bootstrap-ci-with-transformation",
    "href": "posts/2022-05-12-confidence-intervals-for-multilevel-r-squared/index.html#bootstrap-ci-with-transformation",
    "title": "Confidence Intervals for Multilevel R-Squared",
    "section": "Bootstrap CI With Transformation",
    "text": "Bootstrap CI With Transformation\nGiven that \\(R^2\\) is bounded, it may be more accurate to first transform the \\(R^2\\) estimates to an unbounded scale, obtain the CIs on the transformed scale, and then back transform it to between 0 and 1. This can be done in boot::boot.ci() as well with the logistic transformation:\n\nboot::boot.ci(boo03, L = inf_val, h = qlogis,\n              # Need the derivative of the transformation\n              hdot = function(x) 1 / (x - x^2),\n              hinv = plogis)\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 997 bootstrap replicates\n\nCALL : \nboot::boot.ci(boot.out = boo03, L = inf_val, h = qlogis, hdot = function(x) 1/(x - \n    x^2), hinv = plogis)\n\nIntervals : \nLevel      Normal              Basic             Studentized     \n95%   ( 0.1440,  0.4616 )   ( 0.1452,  0.4561 )   ( 0.1181,  0.4184 )  \n\nLevel     Percentile            BCa          \n95%   ( 0.1509,  0.4673 )   ( 0.1383,  0.4448 )  \nCalculations on Transformed Scale;  Intervals on Original Scale\n\n\nNote that the transformation only affects the Normal, Basic, and Studentized CIs."
  },
  {
    "objectID": "posts/2022-05-12-confidence-intervals-for-multilevel-r-squared/index.html#conclusion",
    "href": "posts/2022-05-12-confidence-intervals-for-multilevel-r-squared/index.html#conclusion",
    "title": "Confidence Intervals for Multilevel R-Squared",
    "section": "Conclusion",
    "text": "Conclusion\nThis post demonstrates how to use multilevel bootstrapping to obtain CIs for \\(R^2\\). The post only focuses on marginal \\(R^2\\), but CIs for other \\(R^2\\) measures can be similarly obtained. The studentized CI is the most complex as it requires obtaining the sampling variance of \\(R^2\\) for each bootstrap sample. So far, to my knowledge, there has not been studies on which CI(s) perform best, so simulation studies are needed.\nFurther readings on multilevel bootstrap:\n\nChapter by van der Leeden et al.\nPaper by Lai\nBook by Davison & Hinkley"
  },
  {
    "objectID": "posts/2021-07-14-using-cmdstanr-in-simdesign/index.html",
    "href": "posts/2021-07-14-using-cmdstanr-in-simdesign/index.html",
    "title": "Using cmdstanr in SimDesign",
    "section": "",
    "text": "library(SimDesign)\nlibrary(cmdstanr)\n\n[Update: Use parallel computing with two cores.]\nAdapted from https://cran.r-project.org/web/packages/SimDesign/vignettes/SimDesign-intro.html\nSee https://mc-stan.org/cmdstanr/articles/cmdstanr.html for using cmdstanr\n\nDesign &lt;- createDesign(sample_size = c(30, 60, 120, 240), \n                       distribution = c('norm', 'chi'))\nDesign\n\n# A tibble: 8 × 2\n  sample_size distribution\n        &lt;dbl&gt; &lt;chr&gt;       \n1          30 norm        \n2          60 norm        \n3         120 norm        \n4         240 norm        \n5          30 chi         \n6          60 chi         \n7         120 chi         \n8         240 chi         \n\n\n\nGenerate &lt;- function(condition, fixed_objects = NULL) {\n    N &lt;- condition$sample_size\n    dist &lt;- condition$distribution\n    if(dist == 'norm'){\n        dat &lt;- rnorm(N, mean = 3)\n    } else if(dist == 'chi'){\n        dat &lt;- rchisq(N, df = 3)\n    }\n    dat\n}\n\nDefine Bayes estimator of the mean with STAN\n\n# STAN model\nbmean_stan &lt;- \"\n    data {\n        int&lt;lower=0&gt; N;\n        real x[N];\n    }\n    parameters {\n        real mu;\n        real&lt;lower=0&gt; sigma;\n    }\n    model {\n        target += normal_lpdf(mu | 0, 10);  // weakly informative prior\n        target += normal_lpdf(x | mu, sigma);\n    }\n\"\n# Save file\nstan_path &lt;- write_stan_file(bmean_stan)\nmod &lt;- cmdstan_model(stan_path)\n\nWarning in '/tmp/Rtmpma4Flb/model-39a57203b445.stan', line 4, column 8: Declaration\n    of arrays by placing brackets after a variable name is deprecated and\n    will be removed in Stan 2.33.0. Instead use the array keyword before the\n    type. This can be changed automatically using the auto-format flag to\n    stanc\n\n\n\nAnalyse &lt;- function(condition, dat, fixed_objects = NULL) {\n    mod &lt;- fixed_objects$mod\n    M0 &lt;- mean(dat)\n    M1 &lt;- mean(dat, trim = .1)\n    M2 &lt;- mean(dat, trim = .2)\n    med &lt;- median(dat)\n    stan_fit &lt;- quiet(mod$sample(list(x = dat, N = length(dat)),\n                                 refresh = 0, chains = 1, \n                                 show_messages = FALSE))\n    MB &lt;- stan_fit$summary(\"mu\", mean)$mean[1]\n    ret &lt;- c(mean_no_trim = M0, mean_trim.1 = M1, \n             mean_trim.2 = M2, median = med, \n             bayes_mean = MB)\n    ret\n}\n\n\nSummarise &lt;- function(condition, results, fixed_objects = NULL) {\n    obs_bias &lt;- bias(results, parameter = 3)\n    obs_RMSE &lt;- RMSE(results, parameter = 3)\n    ret &lt;- c(bias = obs_bias, RMSE = obs_RMSE, RE = RE(obs_RMSE))\n    ret\n}\n\n\nres &lt;- runSimulation(Design, replications = 50, generate = Generate, \n                     analyse = Analyse, summarise = Summarise, \n                     parallel = TRUE,\n                     ncores = min(2, parallel::detectCores()), \n                     fixed_objects = list(mod = mod),\n                     packages = \"cmdstanr\")\n\n\nNumber of parallel clusters in use: 2\n\n\n\n\nDesign row: 1/8;   Started: Wed Jun 14 09:50:08 2023;   Total elapsed time: 0.00s \n Conditions: sample_size=30, distribution=norm\n\n\nDesign row: 2/8;   Started: Wed Jun 14 09:50:16 2023;   Total elapsed time: 7.27s \n Conditions: sample_size=60, distribution=norm\n\n\nDesign row: 3/8;   Started: Wed Jun 14 09:50:22 2023;   Total elapsed time: 13.84s \n Conditions: sample_size=120, distribution=norm\n\n\nDesign row: 4/8;   Started: Wed Jun 14 09:50:29 2023;   Total elapsed time: 20.55s \n Conditions: sample_size=240, distribution=norm\n\n\nDesign row: 5/8;   Started: Wed Jun 14 09:50:36 2023;   Total elapsed time: 27.51s \n Conditions: sample_size=30, distribution=chi\n\n\nDesign row: 6/8;   Started: Wed Jun 14 09:50:43 2023;   Total elapsed time: 34.42s \n Conditions: sample_size=60, distribution=chi\n\n\nDesign row: 7/8;   Started: Wed Jun 14 09:50:49 2023;   Total elapsed time: 41.13s \n Conditions: sample_size=120, distribution=chi\n\n\nDesign row: 8/8;   Started: Wed Jun 14 09:50:56 2023;   Total elapsed time: 48.00s \n Conditions: sample_size=240, distribution=chi\n\n\n\n\nSimulation complete. Total execution time: 54.96s\n\n\n\nknitr::kable(res)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsample_size\ndistribution\nbias.mean_no_trim\nbias.mean_trim.1\nbias.mean_trim.2\nbias.median\nbias.bayes_mean\nRMSE.mean_no_trim\nRMSE.mean_trim.1\nRMSE.mean_trim.2\nRMSE.median\nRMSE.bayes_mean\nRE.mean_no_trim\nRE.mean_trim.1\nRE.mean_trim.2\nRE.median\nRE.bayes_mean\nREPLICATIONS\nSIM_TIME\nCOMPLETED\nSEED\n\n\n\n\n30\nnorm\n0.0297498\n0.0311210\n0.0416024\n0.0512205\n0.0284791\n0.1843068\n0.1789694\n0.1800554\n0.2130513\n0.1826303\n1\n0.9429198\n0.9543979\n1.336243\n0.9818899\n50\n7.270\nWed Jun 14 09:50:16 2023\n903416221\n\n\n60\nnorm\n-0.0097494\n-0.0121612\n-0.0147662\n-0.0258952\n-0.0103229\n0.1409947\n0.1515269\n0.1594346\n0.1791889\n0.1409252\n1\n1.1549796\n1.2786739\n1.615166\n0.9990154\n50\n6.571\nWed Jun 14 09:50:22 2023\n1077467357\n\n\n120\nnorm\n0.0154507\n0.0190990\n0.0184514\n0.0201537\n0.0156482\n0.0854772\n0.0880060\n0.0926618\n0.1033099\n0.0853852\n1\n1.0600443\n1.1751710\n1.460775\n0.9978502\n50\n6.714\nWed Jun 14 09:50:29 2023\n1461526622\n\n\n240\nnorm\n-0.0062304\n-0.0076866\n-0.0082527\n0.0007573\n-0.0063264\n0.0661626\n0.0672409\n0.0708765\n0.0739489\n0.0664047\n1\n1.0328611\n1.1475699\n1.249218\n1.0073314\n50\n6.957\nWed Jun 14 09:50:36 2023\n1727633798\n\n\n30\nchi\n0.0532533\n-0.3137971\n-0.4661847\n-0.5730742\n0.0415595\n0.4726788\n0.5550194\n0.6340303\n0.7317198\n0.4682795\n1\n1.3787454\n1.7992347\n2.396389\n0.9814723\n50\n6.905\nWed Jun 14 09:50:43 2023\n1641328088\n\n\n60\nchi\n0.0329161\n-0.3278993\n-0.4757649\n-0.6092868\n0.0296970\n0.3139272\n0.4392542\n0.5562916\n0.7027590\n0.3117431\n1\n1.9578254\n3.1401265\n5.011353\n0.9861338\n50\n6.709\nWed Jun 14 09:50:49 2023\n1415833419\n\n\n120\nchi\n-0.0533443\n-0.3921347\n-0.5362277\n-0.6736837\n-0.0545067\n0.2094071\n0.4419034\n0.5771418\n0.7123931\n0.2124386\n1\n4.4531980\n7.5959550\n11.573290\n1.0291632\n50\n6.878\nWed Jun 14 09:50:56 2023\n1729985301\n\n\n240\nchi\n0.0089197\n-0.3296886\n-0.4742613\n-0.6334701\n0.0086411\n0.1523843\n0.3597207\n0.4949397\n0.6520986\n0.1529830\n1\n5.5725027\n10.5493059\n18.312427\n1.0078723\n50\n6.957\nWed Jun 14 09:51:03 2023\n928324547"
  },
  {
    "objectID": "posts/2021-09-13-irt-scoring-with-covariates/index.html",
    "href": "posts/2021-09-13-irt-scoring-with-covariates/index.html",
    "title": "IRT Scoring With Covariates",
    "section": "",
    "text": "I was working on an extension to the two-stage path analysis Lai & Hsiao (2021) related to integrative data analysis, and ran into an issue described in Davoudzadeh et al. (2021) (which is a very inspiring paper). The basic idea is that when doing a multiple-group analysis to obtain factor scores, a multiple-group and a single-group approach generally give different results due to the different priors. This happens for both confirmatory factor analysis (CFA) and item response theory (IRT). The math can be found in the cited paper; here I just make some notes and show the differences."
  },
  {
    "objectID": "posts/2021-09-13-irt-scoring-with-covariates/index.html#load-packages",
    "href": "posts/2021-09-13-irt-scoring-with-covariates/index.html#load-packages",
    "title": "IRT Scoring With Covariates",
    "section": "Load Packages",
    "text": "Load Packages\n\nlibrary(mirt)\n\nLoading required package: stats4\n\n\nLoading required package: lattice"
  },
  {
    "objectID": "posts/2021-09-13-irt-scoring-with-covariates/index.html#simulate-data",
    "href": "posts/2021-09-13-irt-scoring-with-covariates/index.html#simulate-data",
    "title": "IRT Scoring With Covariates",
    "section": "Simulate Data",
    "text": "Simulate Data\nI’ll use the simulated data from the mirt::multipleGroup() function (see ?multipleGroup). The two groups have different means (0 and 1) but the same SDs (1). Having different SDs make things a bit more complicated, so I avoided it here.\n\n# 15 items, 2 groups, each with n = 1000\nset.seed(12345)\na &lt;- matrix(abs(rnorm(15, 1, .3)), ncol = 1)\nd &lt;- matrix(rnorm(15, 0, .7), ncol = 1)\nitemtype &lt;- rep('2PL', nrow(a))\nN &lt;- 1000\nsim_dat &lt;- rbind(\n  simdata(a, d, N, itemtype),\n  simdata(a, d, N, itemtype, mu = 1)\n) |&gt; as.data.frame()\nsim_dat$group &lt;- c(rep('D1', N), rep('D2', N))"
  },
  {
    "objectID": "posts/2021-09-13-irt-scoring-with-covariates/index.html#irt-analayses",
    "href": "posts/2021-09-13-irt-scoring-with-covariates/index.html#irt-analayses",
    "title": "IRT Scoring With Covariates",
    "section": "IRT Analayses",
    "text": "IRT Analayses\nThere are two ways to incorporate the group information: Multiple-group analyses and single-group analyses with the grouping variable as a covariate.\n\nSingle Group With No Covariates\n\nsg_irtfit &lt;- mirt(sim_dat[, 1:15], model = 1)\n\n\nIteration: 1, Log-Lik: -17563.859, Max-Change: 0.37567\nIteration: 2, Log-Lik: -17439.794, Max-Change: 0.19031\nIteration: 3, Log-Lik: -17415.336, Max-Change: 0.08938\nIteration: 4, Log-Lik: -17409.343, Max-Change: 0.05436\nIteration: 5, Log-Lik: -17407.219, Max-Change: 0.02927\nIteration: 6, Log-Lik: -17406.462, Max-Change: 0.01671\nIteration: 7, Log-Lik: -17406.087, Max-Change: 0.00705\nIteration: 8, Log-Lik: -17406.009, Max-Change: 0.00448\nIteration: 9, Log-Lik: -17405.972, Max-Change: 0.00319\nIteration: 10, Log-Lik: -17405.942, Max-Change: 0.00108\nIteration: 11, Log-Lik: -17405.938, Max-Change: 0.00084\nIteration: 12, Log-Lik: -17405.935, Max-Change: 0.00065\nIteration: 13, Log-Lik: -17405.931, Max-Change: 0.00010\n\n# Factor score\nfs1 &lt;- fscores(sg_irtfit)\n\n\nmimic_irtfit &lt;- mirt(sim_dat[, 1:15], model = 1,\n                     covdata = sim_dat[, \"group\", drop = FALSE],\n                     formula = ~ group)\n\n\nIteration: 1, Log-Lik: -17563.859, Max-Change: 0.70774\nIteration: 2, Log-Lik: -17371.328, Max-Change: 0.20764\nIteration: 3, Log-Lik: -17345.735, Max-Change: 0.13851\nIteration: 4, Log-Lik: -17308.700, Max-Change: 0.11318\nIteration: 5, Log-Lik: -17283.429, Max-Change: 0.08602\nIteration: 6, Log-Lik: -17267.239, Max-Change: 0.06126\nIteration: 7, Log-Lik: -17258.182, Max-Change: 0.04622\nIteration: 8, Log-Lik: -17253.056, Max-Change: 0.03645\nIteration: 9, Log-Lik: -17250.527, Max-Change: 0.02887\nIteration: 10, Log-Lik: -17248.078, Max-Change: 0.01472\nIteration: 11, Log-Lik: -17247.805, Max-Change: 0.00328\nIteration: 12, Log-Lik: -17247.784, Max-Change: 0.00164\nIteration: 13, Log-Lik: -17247.773, Max-Change: 0.00126\nIteration: 14, Log-Lik: -17247.769, Max-Change: 0.00105\nIteration: 15, Log-Lik: -17247.766, Max-Change: 0.00079\nIteration: 16, Log-Lik: -17247.762, Max-Change: 0.00045\nIteration: 17, Log-Lik: -17247.762, Max-Change: 0.00013\nIteration: 18, Log-Lik: -17247.762, Max-Change: 0.00011\nIteration: 19, Log-Lik: -17247.762, Max-Change: 0.00007\n\n# Factor score\nfs2 &lt;- fscores(mimic_irtfit)\n\n\n\nMultiple Group\n\nmg_irtfit &lt;- multipleGroup(\n  sim_dat[, 1:15],\n  model = 1,\n  group = sim_dat$group,\n  invariance =\n    c(\"free_means\", \"slopes\", \"intercepts\")\n)\n\n\nIteration: 1, Log-Lik: -17563.859, Max-Change: 0.37617\nIteration: 2, Log-Lik: -17363.419, Max-Change: 0.17124\nIteration: 3, Log-Lik: -17323.202, Max-Change: 0.07469\nIteration: 4, Log-Lik: -17305.002, Max-Change: 0.05986\nIteration: 5, Log-Lik: -17292.975, Max-Change: 0.05443\nIteration: 6, Log-Lik: -17284.074, Max-Change: 0.04842\nIteration: 7, Log-Lik: -17277.118, Max-Change: 0.04322\nIteration: 8, Log-Lik: -17271.567, Max-Change: 0.03816\nIteration: 9, Log-Lik: -17267.107, Max-Change: 0.03421\nIteration: 10, Log-Lik: -17259.260, Max-Change: 0.11071\nIteration: 11, Log-Lik: -17252.001, Max-Change: 0.02217\nIteration: 12, Log-Lik: -17251.059, Max-Change: 0.01457\nIteration: 13, Log-Lik: -17250.118, Max-Change: 0.01677\nIteration: 14, Log-Lik: -17249.644, Max-Change: 0.01040\nIteration: 15, Log-Lik: -17249.285, Max-Change: 0.00864\nIteration: 16, Log-Lik: -17248.750, Max-Change: 0.03253\nIteration: 17, Log-Lik: -17248.087, Max-Change: 0.00731\nIteration: 18, Log-Lik: -17248.018, Max-Change: 0.00380\nIteration: 19, Log-Lik: -17247.913, Max-Change: 0.00828\nIteration: 20, Log-Lik: -17247.867, Max-Change: 0.00216\nIteration: 21, Log-Lik: -17247.848, Max-Change: 0.00198\nIteration: 22, Log-Lik: -17247.816, Max-Change: 0.00714\nIteration: 23, Log-Lik: -17247.782, Max-Change: 0.00110\nIteration: 24, Log-Lik: -17247.778, Max-Change: 0.00095\nIteration: 25, Log-Lik: -17247.774, Max-Change: 0.00296\nIteration: 26, Log-Lik: -17247.767, Max-Change: 0.00066\nIteration: 27, Log-Lik: -17247.766, Max-Change: 0.00049\nIteration: 28, Log-Lik: -17247.765, Max-Change: 0.00230\nIteration: 29, Log-Lik: -17247.763, Max-Change: 0.00020\nIteration: 30, Log-Lik: -17247.762, Max-Change: 0.00016\nIteration: 31, Log-Lik: -17247.762, Max-Change: 0.00065\nIteration: 32, Log-Lik: -17247.762, Max-Change: 0.00010\nIteration: 33, Log-Lik: -17247.762, Max-Change: 0.00012\nIteration: 34, Log-Lik: -17247.762, Max-Change: 0.00010\nIteration: 35, Log-Lik: -17247.762, Max-Change: 0.00008\n\n# Factor score\nfs3 &lt;- fscores(mg_irtfit)\n\n\n\nCompare Coefficients\n\n# The numbers are virtually the same; however, the single-group approach \n# standardizes on the combined data, whereas the MIMIC and the multiple-group\n# approaches standardize on just the first group. Therefore, a scale adjustment\n# will be needed to put the parameters on the same scale\n\n# Scale adjustment factor:\ntotal_sd &lt;- sqrt(1 + coef(mg_irtfit)$D2$GroupPars[1, \"MEAN_1\"]^2 / 4)\nsg_pars &lt;- coef(sg_irtfit, simplify = TRUE)$items  # single-group (unadjusted)\nsg_pars[, 1] / total_sd  # discriminations with an approximate scale adjustment\n\n   Item_1    Item_2    Item_3    Item_4    Item_5    Item_6    Item_7    Item_8 \n1.1279379 1.2803483 0.9396957 0.8741506 1.1847194 0.4434912 1.2258472 0.9952438 \n   Item_9   Item_10   Item_11   Item_12   Item_13   Item_14   Item_15 \n1.0056366 0.6777604 0.8802975 1.4708279 1.2180373 1.1271193 0.7513079 \n\ncoef(mimic_irtfit, simplify = TRUE)$items  # single-group with covariates\n\n               a1           d g u\nItem_1  1.1344745  0.56141379 0 1\nItem_2  1.2748144 -0.71750014 0 1\nItem_3  0.9263239 -0.23168942 0 1\nItem_4  0.8788699  0.85739018 0 1\nItem_5  1.2010639  0.18724951 0 1\nItem_6  0.4345348  0.62056936 0 1\nItem_7  1.2176948  0.96014551 0 1\nItem_8  0.9742907 -0.42310233 0 1\nItem_9  1.0080648 -1.06816100 0 1\nItem_10 0.6773441 -1.05699733 0 1\nItem_11 0.8848418  1.22591144 0 1\nItem_12 1.4724849 -0.23745600 0 1\nItem_13 1.2160712  0.44539675 0 1\nItem_14 1.1248698  0.45726162 0 1\nItem_15 0.7457826 -0.06616818 0 1\n\ncoef(mg_irtfit, simplify = TRUE)$D1$items  # multiple-group\n\n               a1           d g u\nItem_1  1.1344531  0.56249099 0 1\nItem_2  1.2744706 -0.71613006 0 1\nItem_3  0.9262275 -0.23080999 0 1\nItem_4  0.8788613  0.85822186 0 1\nItem_5  1.2009517  0.18839934 0 1\nItem_6  0.4345115  0.62096775 0 1\nItem_7  1.2178070  0.96135430 0 1\nItem_8  0.9741636 -0.42215719 0 1\nItem_9  1.0078107 -1.06705954 0 1\nItem_10 0.6772300 -1.05630348 0 1\nItem_11 0.8848795  1.22678453 0 1\nItem_12 1.4721600 -0.23594789 0 1\nItem_13 1.2160331  0.44655427 0 1\nItem_14 1.1248339  0.45832468 0 1\nItem_15 0.7457149 -0.06547177 0 1"
  },
  {
    "objectID": "posts/2021-09-13-irt-scoring-with-covariates/index.html#comparing-the-factor-scores",
    "href": "posts/2021-09-13-irt-scoring-with-covariates/index.html#comparing-the-factor-scores",
    "title": "IRT Scoring With Covariates",
    "section": "Comparing the Factor Scores",
    "text": "Comparing the Factor Scores\nAs shown below, the single-group approach gives different results then the MIMIC and the multiple-group approaches.\n\nhead(cbind(fs1, fs2, fs3))\n\n           F1        F1        F1\n1 -0.02550602 0.3402118 0.3394847\n2  0.77540259 1.1504818 1.1497899\n3  0.36027615 0.7367433 0.7361048\n4  0.65733506 1.0313568 1.0307768\n5  0.69447276 1.0671403 1.0665020\n6  0.53033799 0.9113015 0.9106491\n\n\n\nplot(fs1, fs2)\n\n\n\n\n\nplot(fs1, fs3)\n\n\n\n\nThis is particularly problematic when looking at the mean differences across groups:\n\ntapply(fs1, sim_dat$group, mean)  # single-group; shrinkage applies to differences\n\n        D1         D2 \n-0.3367292  0.3359305 \n\ntapply(fs2, sim_dat$group, mean)  # MIMIC; shrinkage does not apply to differences\n\n          D1           D2 \n0.0001759437 0.9643186768 \n\ntapply(fs3, sim_dat$group, mean)  # MGIRT; shrinkage does not apply to differences\n\n           D1            D2 \n-0.0005464251  0.9634912005 \n\n\nIn mirt, one can change the prior to get factor scores for a pooled population:\n\nfs1_new &lt;- fscores(sg_irtfit,\n                   # Use the mean implied from MGIRT\n                   mean = 0,\n                   cov = total_sd)\nfs2_new &lt;- fscores(mimic_irtfit, mean = 0, cov = 1)\nfs3_new &lt;- fscores(mg_irtfit, \n                   mean = c(0, 0), cov = c(1, 1))\ntapply(fs1_new, sim_dat$group, mean)  # single-group; shrinkage applies to differences\n\n        D1         D2 \n-0.3428453  0.3502421 \n\ntapply(fs2_new, sim_dat$group, mean)  # MIMIC; shrinkage STILL does not apply to differences\n\n          D1           D2 \n0.0001759437 0.9643186768 \n\ntapply(fs3_new, sim_dat$group, mean)  # MGIRT; shrinkage applies to differences\n\n           D1            D2 \n-0.0005464251  0.6886014934 \n\n\nNow that the single-group and the multiple-group analyses are much closer (other than the difference in the means, as the single-group analysis sets the grand mean to 0, whereas the multiple-group analysis sets the mean of the first group to 0):\n\nplot(fs1_new, fs3_new)\n\n\n\n\nHowever, it looks like with MIMIC needs a different kind of priors to do scoring.\n\nplot(fs1_new, fs2_new)"
  },
  {
    "objectID": "posts/2021-09-13-irt-scoring-with-covariates/index.html#conclusion",
    "href": "posts/2021-09-13-irt-scoring-with-covariates/index.html#conclusion",
    "title": "IRT Scoring With Covariates",
    "section": "Conclusion",
    "text": "Conclusion\nAs discussed in Davoudzadeh et al. (2021), the default options in getting factor scores in a multiple-group analysis may not be appropriate as it assumes different priors for different groups. This also happens when treating the grouping variable as a covariate, as in the MIMIC (multiple-indicator-multiple-causes) model, which is the basis of the moderated nonlinear factor analysis (Curran et al., doi: 10.1080/00273171.2014.889594)—an approach commonly used for integrative data analysis. This deserves attentions as if one is going to use factor scores to estimate differences among certain subgroups—either the original grouping variable (\\(G\\)) in factor score estimation or some other variables related to \\(G\\), one gets different estimates depending on the different factor score approaches."
  },
  {
    "objectID": "course.html",
    "href": "course.html",
    "title": "Courses",
    "section": "",
    "text": "Workshops:\n\nAdvancing Quantitative Science with Monte Carlo Simulation\n\nCourses I teach at USC:\n\nPSYC 314: Experimental Research Methods\nPSYC 621: Seminar in Quantitative Psychology (Multilevel Modeling)\nPSYC 621: Seminar in Quantitative Psychology (Bayesian Data Analysis)\n\nCourse notes\n\nPSYC 575: Multilevel Modeling\n\nCourse site (GitHub materials) for 2020 Fall\nCourse site for 2021 Fall\nCourse site for 2022 Fall\n\nPSYC 573: Bayesian Data Analysis\n\nCourse site for 2022 Spring\n\nPSYC 520: Fundamentals of Psychological Measurement\n\nCourse site for 2023 Spring\n\n\nCourses I taught at UC:\n\nEDST 7010: Statistical Data Analysis I\nEDST 7011: Statistical Data Analysis II\nEDST 8075: Bayesian Data Analysis\nEDST 7082: Getting Started With Multilevel Modeling\nEDST 8087: Multilevel Modeling for Educational Research"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hok Chio (Mark) Lai 黎學昭",
    "section": "",
    "text": "I am an Associate Professor of Psychology (Quantitative Methods) in the Department of Psychology at the University of Southern California. I am from Macau and speaks Cantonese as my native language. Mathematics has always been a favorite subject to me, but my growing interest in understanding the human mind lead me to pursue a bachelor degree in Psychology at the University of Macau, where I discovered that a career of developing and applying quantitative methods in the social sciences would be a very good fit to me.\nI am interested in all kinds of statistical methods, but currently my research areas include multilevel modeling, structural equation modeling, effect size statistics, and measurement invariance. I also have collaborations on projects studying the prevalence, antecedents and consequences of gambling disorder, and in other areas such as evolutionary psychology, social psychology, developmental sciences, health sciences, etc."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Hok Chio (Mark) Lai 黎學昭",
    "section": "Education",
    "text": "Education\nTexas A&M University | College Station, TX PhD in Educational Psychology (Research, Measurement and Statistics) | Aug 2011 - Aug 2015\nUniversity of Macau | Macau BSSc in Psychology | Sept 2007 - May 2011"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Hok Chio (Mark) Lai 黎學昭",
    "section": "Experience",
    "text": "Experience\nUniversity of Southern California | Assisstant Professor of Psychology | August 2018 - 2023\n— Associate Professor of Psychology | August 2023 -\nUniversity of Cincinnati | Assistant Professor | Aug 2015 - Aug 2018"
  },
  {
    "objectID": "posts/2022-05-28-estimating-a-2-pl-model-in-julia-part-2/index.html",
    "href": "posts/2022-05-28-estimating-a-2-pl-model-in-julia-part-2/index.html",
    "title": "Estimating a 2-PL Model in Julia (Part 2)",
    "section": "",
    "text": "\\[\n  \\newcommand{\\bv}[1]{\\boldsymbol{\\mathbf{#1}}}\n\\]\nThe EM algorithm is usually used for estimation problems that involve some latent variables \\(\\bv Z\\) and parameters \\(\\bv \\omega\\), where the conditional likelihood \\(L(\\bv \\omega; \\bv Y, \\bv Z)\\) is relatively easy to solve, but the marginal likelihood, which requires integrating \\(\\bv Z\\) out, is intractable. In the 2-PL estimation problem, we consider the item parameters \\(\\bv a\\) and \\(\\bv d\\) as \\(\\bv \\omega\\), and the person parameters \\(\\bv \\theta\\) as \\(\\bv Z\\).\nIn a general IRT model, the marginal likelihood function \\(L(\\bv \\omega; \\bv Y)\\) requires integrating out the person parameters \\(\\bv \\theta\\), which is usually difficult. We can instead use the EM algorithm. First, we can define the complete-data likelihood\n\\[L(\\bv \\omega; \\bv Y, \\bv \\theta) = P(\\bv Y \\mid \\bv \\theta, \\bv \\omega) P(\\bv \\theta \\mid \\bv \\omega)\\]\nassuming that observations are independently and identically distributed given \\(\\bv \\theta\\), and local independence such that the item responses are independent when conditioning on \\(\\bv \\theta\\), we have\n\\[L(\\bv \\omega; \\bv Y, \\bv \\theta) = \\prod_i^N \\prod_j L_{ij} P(\\theta_i \\mid \\bv \\omega_j)\\]\nwhere \\(L_{ij} = P(y_{ij} \\mid \\theta_i, \\bv \\omega_j)\\). Thus, the complete-data loglikelihood is\n\\[\\ell(\\bv \\omega; \\bv Y, \\bv \\theta) = \\log L(\\bv \\omega; \\bv Y, \\bv \\theta) = \\sum_i^N \\sum_j \\left[\\ell_{ij} + \\log P(\\theta_i \\mid \\bv \\omega_j)\\right],\\]\nwhere \\(\\ell_{ij} = \\log L_{ij}\\)\nWith the EM algorithm, we update our parameter estimates by iterating between two steps:\nNote: the setup of the problem in this post deviates a bit from the one in the original Bock and Aitkin paper, which assumes a multinomial distribution of the sample counts of each response pattern. The resulting estimating equations should be the same."
  },
  {
    "objectID": "posts/2022-05-28-estimating-a-2-pl-model-in-julia-part-2/index.html#e-step",
    "href": "posts/2022-05-28-estimating-a-2-pl-model-in-julia-part-2/index.html#e-step",
    "title": "Estimating a 2-PL Model in Julia (Part 2)",
    "section": "E-Step",
    "text": "E-Step\nFirst consider the E-step for the 2-PL model, which has\n\\[\n  \\begin{aligned}\n    \\ell_{ij} & = y_{ij} \\log \\eta_{ij} - \\log[1 + \\exp(\\eta_{ij})] \\\\\n    \\eta_{ij} & = a_j \\theta_i + d_j\n  \\end{aligned}.\n\\]\nIn addition, in IRT, because item responses are discrete, the \\(N\\) observations can usually be reduced to a smaller number of \\(l\\) response patterns. For example, with the LSAT data, which has 1,000 observations, it only has 30 response patterns of 5 binary items:\nusing RCall\nlsat = rcopy(R\"mirt::LSAT6\")\n30×6 DataFrame\n Row │ Item_1  Item_2  Item_3  Item_4  Item_5  Freq\n     │ Int64   Int64   Int64   Int64   Int64   Int64\n─────┼───────────────────────────────────────────────\n   1 │      0       0       0       0       0      3\n   2 │      0       0       0       0       1      6\n   3 │      0       0       0       1       0      2\n   4 │      0       0       0       1       1     11\n   5 │      0       0       1       0       0      1\n   6 │      0       0       1       0       1      1\n   7 │      0       0       1       1       0      3\n   8 │      0       0       1       1       1      4\n  ⋮  │   ⋮       ⋮       ⋮       ⋮       ⋮       ⋮\n  24 │      1       1       0       0       1     56\n  25 │      1       1       0       1       0     21\n  26 │      1       1       0       1       1    173\n  27 │      1       1       1       0       0     11\n  28 │      1       1       1       0       1     61\n  29 │      1       1       1       1       0     28\n  30 │      1       1       1       1       1    298\n                                      15 rows omitted\nSo instead of computing the loglikelihood for 1,000 observations, we only need to do it for \\(s = 30\\) response patterns. So the complete-data loglikelihood can be written as\n\\[\\ell(\\bv \\omega; \\bv Y, \\bv \\theta) = \\sum_{l = 1}^s \\sum_j n_l \\ell_{lj} + \\sum_{i = 1}^n \\sum_j \\log P(\\theta_i \\mid \\bv \\omega_j),\\]\nwith \\(\\ell_{lj} = \\log L_{lj} = \\log P(y_{lj} \\mid \\bv \\omega_j, \\theta)\\) for a fixed \\(\\theta\\) value.\nFor the E-step, we first need the conditional distribution \\(P(\\theta \\mid \\bv Y, \\bv a^{(t)}, \\bv d^{(t)})\\) with some fixed values of \\(\\bv a^{(t)}\\) and \\(\\bv d^{(t)}\\), which can be obtained with Bayes’ theorem\n\\[P(\\theta_i \\mid \\bv y_l, \\bv a^{(t)}, \\bv d^{(t)}) = \\frac{P(\\bv y_l \\mid \\theta_i, \\bv a^{(t)}, \\bv d^{(t)})P(\\theta_i)}{\\int P(\\bv y_l \\mid \\theta, \\bv a^{(t)} \\bv d^{(t)})P(\\theta) d\\theta}\\]\nWe need a prior distribution \\(P(\\theta)\\), which is commonly chosen as a normal distribution (and standard normal for single group analysis). Using Gaussian-Hermite (GH) quadrature to replace \\(P(\\theta_k)\\) by \\(w_k\\), where \\(k\\) being one of the \\(q\\) quadrature points, and replacing the intergral with the finite sum, we have:\n\\[\n  \\begin{aligned}\n  P(\\theta_k \\mid \\bv y_l, \\bv a^{(t)}, \\bv d^{(t)}) & \\approx \\frac{P(\\bv y_l \\mid \\theta_i, \\bv a, \\bv d) w_k}{\\sum_{k = 1}^q P(\\bv y_l \\mid \\theta, \\bv a \\bv d) w_k} \\\\\n  & = \\frac{L^{(t)}_l(\\theta_k)w_k}{\\sum_k L^{(t)}_l(\\theta_k) w_k} \\\\\n  & = \\frac{L^{(t)}_l(\\theta_k)w_k}{P_l}\n  \\end{aligned}\n\\]\nNote the subscript \\((t)\\) indicating that \\(L^{(t)}_l(\\cdot)\\) is a function of \\(\\theta\\) with fixed values of the item parameters. Now we can taking the expectation of the complete-data loglikelihood with respect to the conditional distribution we just saw:\n\\[\n  \\begin{aligned}\n  E_{\\bv \\theta \\mid \\bv Y, \\bv \\omega^{(t)}}[\\ell(\\bv \\omega; \\bv Y, \\bv \\theta)]\n  & = E_{\\bv \\theta \\mid \\bv Y, \\bv \\omega^{(t)}}\\left[\\sum_l \\sum_j n_l \\ell_{lj} \\right] \\\\\n  & \\quad + E_{\\bv \\theta \\mid \\bv Y, \\bv \\omega^{(t)}} \\left[\\sum_l \\sum_j \\log P(\\theta_i \\mid \\bv \\omega^{(t)})\\right],\n  \\end{aligned}\n\\]\nNote that the second expectation term is not a function of the item parameters (it is a function of \\(\\theta\\) treating the item parameters as fixed), so we can write it as \\(h(\\bv \\theta)\\), which is a constant for the maximization step. So using G-H quadrature to approximate the integral when taking the expectation, we have\n\\[\n  \\begin{aligned}\n  E_{\\bv \\theta \\mid \\bv Y, \\bv \\omega^{(t)}}[\\ell(\\bv \\omega; \\bv Y, \\bv \\theta)]\n  & = \\int \\left[\\sum_l \\sum_j n_l \\ell_{lj} P(\\theta \\mid \\bv y_l, \\bv \\omega^{(t)})\\right] d \\theta + h(\\bv \\theta) \\\\\n  & = \\sum_k \\sum_l \\sum_j \\frac{ n_l \\ell_{lj} L^{(t)}_l(\\theta_k)w_k}{P_l} + h(\\bv \\theta)\n  \\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/2022-05-28-estimating-a-2-pl-model-in-julia-part-2/index.html#m-step",
    "href": "posts/2022-05-28-estimating-a-2-pl-model-in-julia-part-2/index.html#m-step",
    "title": "Estimating a 2-PL Model in Julia (Part 2)",
    "section": "M-Step",
    "text": "M-Step\nIn the M-step, we find new values \\(\\bv \\omega^{(t + 1)}\\) that maximize the expected loglikelihood above. To find the maximizer, we take the derivative of the expected loglikelihood with respect to each parameter, and set the derivative to zero. This gives us a set of estimating equations. First, it can be verified that\n\\[\n  \\begin{aligned}\n    \\frac{\\partial \\ell_{lj}}{\\partial a_j} & = \\left[y_{lj} - P_j(\\theta_k)\\right] \\theta_k \\\\\n    \\frac{\\partial \\ell_{lj}}{\\partial d_j} & = y_{lj} - P_j(\\theta_k),\n  \\end{aligned}\n\\]\nwhere \\(P_j(\\theta_k) = \\eta_{kj} / (1 + \\eta_{kj})\\). So the derivative of the expected loglikelihood with respect to \\(a_j\\) is\n\\[\n  \\begin{aligned}\n  \\frac{\\partial}{\\partial a_j} E_{\\bv \\theta \\mid \\bv Y, \\bv \\omega^{(t)}}[\\ell(\\bv \\omega; \\bv Y, \\bv \\theta)]\n  & = \\sum_k \\sum_l \\frac{n_l [y_{lj} - P_j(\\theta_k)] \\theta_k L^{(t)}_l(\\theta_k)w_k}{P_l}.\n  \\end{aligned}\n\\]\nFollowing the previous literature, let\n\\[\n  \\begin{aligned}\n  \\bar r_{jk} & = \\sum_l \\frac{n_l y_{lj} L^{(t)}_l(\\theta_k)w_k}{P_l} \\\\\n  \\bar n_k & = \\sum_l \\frac{n_l L^{(t)}_l(\\theta_k)w_k}{P_l},\n  \\end{aligned}\n\\]\nwhere \\(n_k\\) is the expected number of participants with ability level \\(\\theta_k\\), and \\(r_{jk}\\) is the expected number of participants with ability level \\(\\theta_k\\) endorsing item \\(j\\). Then the estimating equations are\n\\[\n  \\begin{aligned}\n  \\sum_k [\\bar r_{jk} - \\bar n_k P_j(\\theta_k)] = 0 \\\\\n  \\sum_k [\\bar r_{jk} - \\bar n_k P_j(\\theta_k)] \\theta_k = 0\n  \\end{aligned}\n\\]\nfor \\(j = 1, \\ldots, p\\) where \\(p\\) is the number of items. These can be solved using non-linear solvers."
  },
  {
    "objectID": "posts/2022-05-28-estimating-a-2-pl-model-in-julia-part-2/index.html#estimating-a-2-pl-model-with-em-in-julia",
    "href": "posts/2022-05-28-estimating-a-2-pl-model-in-julia-part-2/index.html#estimating-a-2-pl-model-with-em-in-julia",
    "title": "Estimating a 2-PL Model in Julia (Part 2)",
    "section": "Estimating a 2-PL Model with EM in Julia",
    "text": "Estimating a 2-PL Model with EM in Julia\nHere is my attempt to implement the EM algorithm in Julia, following the steps laid out in Harwell (1988). First, load the packages\nusing LinearAlgebra, LogExpFunctions\nusing FastGaussQuadrature: gausshermite\nusing NLsolve\nusing BenchmarkTools\n\nFind \\(\\bar r_{jk}\\) and \\(\\bar n_k\\)\n# Helper for computing logits: ηᵢⱼ = aⱼθ + dⱼ\nfunction compute_logits(θ, a, d)\n    [θ[i] * a[j] + d[j]\n     for i = eachindex(θ), j = eachindex(a)]\nend\ncompute_logits (generic function with 1 method)\nfunction eloglik_2pl_em(y, n, θ, w, parₜ)\n    num_items = size(y, 2)\n    aₜ = parₜ[1:num_items]\n    dₜ = parₜ[num_items+1:end]\n    ηₜ = compute_logits(θ, aₜ, dₜ)\n    sum1pexpη = sum(log1pexp, ηₜ, dims=2)\n    wpy_given_θ = Matrix{eltype(aₜ)}(undef, length(θ), length(n))\n    for l in eachindex(n)\n        wpy_given_θ[:, l] = w .* exp.(ηₜ * view(y, l, :) .- sum1pexpη)\n    end\n    pθ_given_y = wpy_given_θ ./ sum(wpy_given_θ, dims=1)\n    (bar_nₖ=pθ_given_y * n,\n        bar_rⱼₖ=pθ_given_y * (n .* y))\nend\neloglik_2pl_em (generic function with 1 method)\n# Test:\ngh15 = gausshermite(15)  # 15 quadrature points\n([-4.499990707309391, -3.669950373404453, -2.9671669279056054, -2.3257324861738606, -1.7199925751864926, -1.136115585210924, -0.5650695832555779, -3.552713678800501e-15, 0.5650695832555779, 1.136115585210924, 1.7199925751864926, 2.3257324861738606, 2.9671669279056054, 3.669950373404453, 4.499990707309391], [1.5224758042535368e-9, 1.0591155477110773e-6, 0.00010000444123250024, 0.0027780688429127607, 0.030780033872546228, 0.15848891579593563, 0.41202868749889865, 0.5641003087264175, 0.41202868749889865, 0.15848891579593563, 0.030780033872546228, 0.0027780688429127607, 0.00010000444123250024, 1.0591155477110773e-6, 1.5224758042535368e-9])\ngh15_node = gh15[1] .* √2\n15-element Vector{Float64}:\n -6.363947888829838\n -5.190093591304782\n -4.196207711269019\n -3.289082424398771\n -2.4324368270097634\n -1.6067100690287344\n -0.7991290683245511\n -5.0242958677880805e-15\n  0.7991290683245511\n  1.6067100690287344\n  2.4324368270097634\n  3.289082424398771\n  4.196207711269019\n  5.190093591304782\n  6.363947888829838\ngh15_weight = gh15[2] ./ √π\n15-element Vector{Float64}:\n 8.589649899633383e-10\n 5.975419597920666e-7\n 5.642146405189039e-5\n 0.0015673575035499477\n 0.01736577449213769\n 0.08941779539984435\n 0.23246229360973225\n 0.31825951825951826\n 0.23246229360973225\n 0.08941779539984435\n 0.01736577449213769\n 0.0015673575035499477\n 5.642146405189039e-5\n 5.975419597920666e-7\n 8.589649899633383e-10\nexp1 = eloglik_2pl_em(Matrix(lsat[:, 1:5]), lsat[:, 6],\n    gh15_node, gh15_weight,\n    [ones(5); zeros(5)])\n(bar_nₖ = [2.7564518778742383e-8, 2.0114861555321952e-5, 0.002128243156556154, 0.07560094100777943, 1.3352261691433547, 14.249248990842812, 89.7255889231157, 278.1775106946097, 365.8604302697971, 199.39830527246278, 46.43045474007526, 4.57238810409275, 0.17124982332670236, 0.001845015100526724, 2.6708429185723037e-6], bar_rⱼₖ = [4.138800953383068e-10 4.341143688898378e-11 … 8.455317850390647e-11 2.4966992659649684e-10; 9.488948442862142e-7 1.0961092559330794e-7 … 2.0276924419430197e-7 5.791071220863187e-7; … ; 0.0018436478096575996 0.0018377274514118978 … 0.0018394578991136525 0.0018424609777163434; 2.670229820441103e-6 2.667573675000273e-6 … 2.6683500732256074e-6 2.669698207072498e-6])\nThe output is a tuple, with the first element bar_nₖ being a \\(q\\) \\(\\times\\) 1 vector and bar_rⱼₖ being a \\(q\\) \\(\\times\\) \\(p\\) matrix.\n\n\nSolve estimating equations\nfunction compute_probs(θ, a, d)\n    [logistic(θ[i] * a[j] + d[j]) for i = eachindex(θ), j = eachindex(a)]\nend\ncompute_probs (generic function with 1 method)\nfunction esteq_2pl_em(par, bar_r, bar_n, θ)\n    num_items = size(bar_r, 2)\n    a = par[1:num_items]\n    d = par[num_items+1:end]\n    rmntpθ = bar_r .- bar_n .* compute_probs(θ, a, d)\n    vec([sum(rmntpθ, dims=1) θ' * rmntpθ])\nend\nesteq_2pl_em (generic function with 1 method)\n# Test:\nroot1 = nlsolve(x -&gt; esteq_2pl_em(x, \n    exp1.bar_rⱼₖ, exp1.bar_nₖ, gh15_node),\n    [ones(5); zeros(5)])\nResults of Nonlinear Solver Algorithm\n * Algorithm: Trust-region with dogleg and autoscaling\n * Starting Point: [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n * Zero: [0.9441787368725837, 0.9412834339373086, 0.9702850411757558, 0.932179532744094, 0.9197528908890249, 2.166166737455263, 0.41020205101729856, -0.37879989990817486, 0.7262454452107591, 1.5321165384312954]\n * Inf-norm of residuals: 0.000000\n * Iterations: 6\n * Convergence: true\n   * |x - x'| &lt; 0.0e+00: false\n   * |f(x)| &lt; 1.0e-08: true\n * Function Calls (f): 7\n * Jacobian Calls (df/dx): 7\nThe solution is contained in the zero field. These will be passed back to the E-step.\n\n\nIterations\nWe can do two more iterations:\nexp2 = eloglik_2pl_em(Matrix(lsat[:, 1:5]), lsat[:, 6],\n    gh15_node, gh15_weight,\n    root1.zero)\n(bar_nₖ = [1.7202823621463788e-7, 0.00012478229640175463, 0.012917226708649957, 0.42950281900235543, 6.374685649928196, 47.54065170285358, 177.55073039082802, 324.1944490261614, 289.0821550106182, 125.99472138888558, 26.268899898287724, 2.460042585603299, 0.09015529832828216, 0.0009626597451594212, 1.3887248384670485e-6], bar_rⱼₖ = [5.372365387113749e-9 6.042186880320284e-10 … 1.1967481795709406e-9 3.692939772198374e-9; 1.1195428419388414e-5 1.486089377064305e-6 … 2.671355380687325e-6 7.62773320369415e-6; … ; 0.0009620030652981468 0.000959109431245618 … 0.0009598307020344003 0.0009612789437790556; 1.3884114755863066e-6 1.3870244275199353e-6 … 1.3873554006168715e-6 1.3880472692324014e-6])\nroot2 = nlsolve(x -&gt; esteq_2pl_em(x, \n    exp2.bar_rⱼₖ, exp2.bar_nₖ, gh15_node),\n    root1.zero)\nResults of Nonlinear Solver Algorithm\n * Algorithm: Trust-region with dogleg and autoscaling\n * Starting Point: [0.9441787368725837, 0.9412834339373086, 0.9702850411757558, 0.932179532744094, 0.9197528908890249, 2.166166737455263, 0.41020205101729856, -0.37879989990817486, 0.7262454452107591, 1.5321165384312954]\n * Zero: [0.8851376144605885, 0.8780270229700008, 0.9305712408536134, 0.8617362606683743, 0.8410113215381403, 2.5424571803032903, 0.781691569809412, -0.0034125639910551494, 1.0943152659197337, 1.8930834808194317]\n * Inf-norm of residuals: 0.000000\n * Iterations: 4\n * Convergence: true\n   * |x - x'| &lt; 0.0e+00: false\n   * |f(x)| &lt; 1.0e-08: true\n * Function Calls (f): 5\n * Jacobian Calls (df/dx): 5\nexp3 = eloglik_2pl_em(Matrix(lsat[:, 1:5]), lsat[:, 6],\n    gh15_node, gh15_weight,\n    root2.zero)\n(bar_nₖ = [4.3023109522800455e-7, 0.0003040218519114245, 0.02987372125524563, 0.9055221912858598, 11.602389316066303, 71.03280737295191, 214.59437201659995, 325.67522637877084, 253.23782066569376, 101.01070654940031, 20.01802278052005, 1.826159972919833, 0.06609181187521956, 0.000701760614451534, 1.0099632324488185e-6], bar_rⱼₖ = [2.0531055047839344e-8 2.5014192320684056e-9 … 4.965693882437507e-9 1.5557900531690245e-8; 3.8265586589167865e-5 5.753192037170773e-6 … 1.0076467391887043e-5 2.8241538126234826e-5; … ; 0.0007012180412131716 0.0006987629848589311 … 0.0006992811247620892 0.0007004963056719968; 1.00968636885505e-6 1.0084204735673418e-6 … 1.008662595953656e-6 1.0092845394530493e-6])\nroot3 = nlsolve(x -&gt; esteq_2pl_em(x, \n    exp3.bar_rⱼₖ, exp3.bar_nₖ, gh15_node),\n    root2.zero)\nResults of Nonlinear Solver Algorithm\n * Algorithm: Trust-region with dogleg and autoscaling\n * Starting Point: [0.8851376144605885, 0.8780270229700008, 0.9305712408536134, 0.8617362606683743, 0.8410113215381403, 2.5424571803032903, 0.781691569809412, -0.0034125639910551494, 1.0943152659197337, 1.8930834808194317]\n * Zero: [0.8479491477352421, 0.8323916555812338, 0.902644467313084, 0.8111833641429015, 0.7863596508859064, 2.6865158580988453, 0.9283333274643143, 0.1559502636058875, 1.2352944393730914, 2.0242412851515406]\n * Inf-norm of residuals: 0.000000\n * Iterations: 4\n * Convergence: true\n   * |x - x'| &lt; 0.0e+00: false\n   * |f(x)| &lt; 1.0e-08: true\n * Function Calls (f): 5\n * Jacobian Calls (df/dx): 5\n\n\nStopping criteria\nOne way to stop the iteration is when the absolute change in the parameter estimates is less than a certain threshold (e.g., 0.00001). For example, the following shows the maximum absolute change in the parameter estimates from the second and the third iteration:\nmaximum(abs.(root3.zero .- root2.zero))\n0.15936282759694267\nwhich is pretty large. So we should do more iterations."
  },
  {
    "objectID": "posts/2022-05-28-estimating-a-2-pl-model-in-julia-part-2/index.html#benchmarking",
    "href": "posts/2022-05-28-estimating-a-2-pl-model-in-julia-part-2/index.html#benchmarking",
    "title": "Estimating a 2-PL Model in Julia (Part 2)",
    "section": "Benchmarking",
    "text": "Benchmarking\nWe can wrap the steps into a function\nfunction estimate_2pl_em(y, n, init, \n    n_quadpts=101, par_tol=1e-5, rtol=1e-5, max_iter=1000)\n    parₜ = init\n    parₜ₊₁ = parₜ\n    # Convert y to matrix\n    y = Matrix(y)\n    # Obtain quadrature nodes and weights\n    ghq = gausshermite(n_quadpts)\n    ghq_θ = ghq[1] .* √2\n    ghq_w = ghq[2] ./ √π\n    i = 1\n    while i &lt; max_iter\n        expₜ = eloglik_2pl_em(y, n,\n    ghq_θ, ghq_w, parₜ)\n        root = nlsolve(x -&gt; esteq_2pl_em(x, \n    expₜ.bar_rⱼₖ, expₜ.bar_nₖ, ghq_θ),\n            parₜ, autodiff=:forward)\n        parₜ₊₁ = root.zero\n        if maximum(abs.(parₜ₊₁ - parₜ)) &lt; par_tol\n            break\n        else\n            parₜ = parₜ₊₁\n            i += 1\n        end\n    end\n    (estimate=parₜ₊₁, num_iter=i)\nend\nestimate_2pl_em (generic function with 5 methods)\n@btime est_em = estimate_2pl_em(lsat[:, 1:5], lsat[:, 6], [ones(5); zeros(5)])\n  16.968 ms (16212 allocations: 29.76 MiB)\n\n(estimate = [0.8256464036061023, 0.7227767138869371, 0.8907854481666115, 0.6883896452213724, 0.6568975184341934, 2.773226187628446, 0.9902095352382095, 0.24914112339506816, 1.284763785097769, 2.0532889359951128], num_iter = 66)\nCompare to mirt\ndata(\"LSAT\", package = \"ltm\")\nlibrary(mirt)\nbench::mark(\n    mirt = mirt(LSAT,\n                verbose = FALSE, quadpts = 101,\n                TOL = 1e-5)\n)\n# A tibble: 1 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 mirt          107ms    107ms      9.38    22.4MB     28.1"
  },
  {
    "objectID": "posts/2022-05-28-estimating-a-2-pl-model-in-julia-part-2/index.html#remark",
    "href": "posts/2022-05-28-estimating-a-2-pl-model-in-julia-part-2/index.html#remark",
    "title": "Estimating a 2-PL Model in Julia (Part 2)",
    "section": "Remark",
    "text": "Remark\nMy implementation of the EM takes a shorter time to run than direct MML (see my post in Part 1), but it does not compute the standard errors. Also, it probably uses a different convergence criterion than direct MML using Optim.jl, so it’s hard to say which one is faster."
  },
  {
    "objectID": "posts/2017-08-01-bayesian-mlm-with-group-mean-centering/index.html",
    "href": "posts/2017-08-01-bayesian-mlm-with-group-mean-centering/index.html",
    "title": "Bayesian MLM With Group Mean Centering",
    "section": "",
    "text": "This post is re-rendered on 2023-06-13 with cleaner and more efficient STAN code.\nIn the past week I was teaching a one-and-a-half-day workshop on multilevel modeling (MLM) at UC, where I discussed the use of group mean centering to decompose level-1 and level-2 effects of a predictor. In that session I ended by noting alternative approaches that reduce bias (mainly from Lüdtke et al. 2008). That lead me also consider the use of Bayesian methods for group mean centering, which will be demonstrated in this post. (Turns out Zitzmann, Lüdtke, and Robitzsch 2015 has already discussed something similar, but still a good exercise.)"
  },
  {
    "objectID": "posts/2017-08-01-bayesian-mlm-with-group-mean-centering/index.html#the-problem",
    "href": "posts/2017-08-01-bayesian-mlm-with-group-mean-centering/index.html#the-problem",
    "title": "Bayesian MLM With Group Mean Centering",
    "section": "The Problem",
    "text": "The Problem\nIn some social scienc research, a predictor can have different meanings at different levels. For example, student’s competence, when aggregated to the school level, becomes the competitiveness of a school. As explained in the big-fish-little-pond effect, whereas the former can help an individual’s self-concept, being in a more competitive environment can hurt one’s self-concept. Traditionally, and still popular nowadays, we investigate the differential effects of a predictor, \\(X_{ij}\\), on an outcome at different levels by including the group means, \\(\\bar X_{.j}\\), in the equation.\nThe problem, however, is that unless each cluster (e.g., school) has a very large sample size, the group means will not be very reliable. This is true even when the measurement of \\(X_{ij}\\) is perfectly reliable. This is not difficult to understand: just remember in intro stats we learn that the standard error of the sample mean is \\(\\sigma / \\sqrt{N}\\), so with limited \\(N\\) our sample (group) mean can be quite different from the population (group) mean.\nWhat’s the problem when the group means are not reliable? Regression literature has shown that unreliability in the predictors lead to downwardly biased coefficients, which is what happen here. The way to account for that, in general, is to treat the unreliable as a latent variable, which is the approach discussed in Lüdtke et al. (2008) and also demonstrated later in this post."
  },
  {
    "objectID": "posts/2017-08-01-bayesian-mlm-with-group-mean-centering/index.html#demonstration",
    "href": "posts/2017-08-01-bayesian-mlm-with-group-mean-centering/index.html#demonstration",
    "title": "Bayesian MLM With Group Mean Centering",
    "section": "Demonstration",
    "text": "Demonstration\nLet’s compare the results using the well-known High School and Beyond Survey subsample discussed in the textbook by Raudenbush and Bryk (2002). To illustrate the issue with unreliability of group means, I’ll use a subset of 800 cases, with a random sample of 5 cases from each school.\n\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(lme4)\nlibrary(brms)\nlibrary(rstan)\n# rstan_options(auto_write = TRUE)\n# options(mc.cores = 2L)\n\n\nhsb &lt;- haven::read_sas('https://stats.idre.ucla.edu/wp-content/uploads/2016/02/hsb.sas7bdat')\n# Select a subsample\nset.seed(123)\nhsb_sub &lt;- hsb %&gt;% group_by(ID) %&gt;% sample_n(5)\nhsb_sub\n\n# A tibble: 800 × 12\n# Groups:   ID [160]\n   ID     SIZE SECTOR PRACAD DISCLIM HIMINTY MEANSES MINORITY FEMALE    SES\n   &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 1224    842      0   0.35   1.60        0  -0.428        0      1 -0.478\n 2 1224    842      0   0.35   1.60        0  -0.428        0      0  0.332\n 3 1224    842      0   0.35   1.60        0  -0.428        0      1 -0.988\n 4 1224    842      0   0.35   1.60        0  -0.428        0      0 -0.528\n 5 1224    842      0   0.35   1.60        0  -0.428        0      1 -1.07 \n 6 1288   1855      0   0.27   0.174       0   0.128        0      1  0.692\n 7 1288   1855      0   0.27   0.174       0   0.128        0      0 -0.118\n 8 1288   1855      0   0.27   0.174       0   0.128        0      0  1.26 \n 9 1288   1855      0   0.27   0.174       0   0.128        1      1 -1.47 \n10 1288   1855      0   0.27   0.174       0   0.128        0      0  0.222\n# ℹ 790 more rows\n# ℹ 2 more variables: MATHACH &lt;dbl&gt;, `_MERGE` &lt;dbl&gt;\n\n\nWith the subsample, let’s study the association of socioeconomic status (SES) with math achievement (MATHACH) at the student level and the school level. The conventional way is to do group mean centering of SES, by computing the group means and the deviation of each SES score from the corresponding group mean.\n\nhsb_sub &lt;- hsb_sub %&gt;% ungroup() %&gt;% \n  mutate(SES_gpm = ave(SES, ID), SES_gpc = SES - SES_gpm)\n\nTo make things simpler, the random slope of SES is not included. Also, one can study differential effects with grand mean centering (Enders and Tofighi 2007), but still the group means should be added as a predictor, so the same issue with unreliability still applies.\n\nGroup mean centering with lme4\n\nm1_mer &lt;- lmer(MATHACH ~ SES_gpm + SES_gpc + (1 | ID), data = hsb_sub)\nsummary(m1_mer)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: MATHACH ~ SES_gpm + SES_gpc + (1 | ID)\n   Data: hsb_sub\n\nREML criterion at convergence: 5188.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.74545 -0.75449  0.04934  0.75875  2.64005 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept)  3.13    1.769   \n Residual             35.81    5.984   \nNumber of obs: 800, groups:  ID, 160\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  12.8714     0.2536   50.75\nSES_gpm       4.6564     0.4840    9.62\nSES_gpc       2.4229     0.3481    6.96\n\nCorrelation of Fixed Effects:\n        (Intr) SES_gpm\nSES_gpm -0.005        \nSES_gpc  0.000  0.000 \n\n\nSo the results suggested that one unit difference SES is associated with 4.7 (SE = 0.48) units difference in mean MATHACH at the school level, but 2.4 (SE = 0.35) units difference in mean MATHACH at the student level.\nWe can get the contextual effect by substracting the fixed effect coefficient of SES_gpm from that of SES_gpc, or by using the original SES variable together with the group means:\n\nm1_mer2 &lt;- lmer(MATHACH ~ SES_gpm + SES + (1 | ID), data = hsb_sub)\nsummary(m1_mer2)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: MATHACH ~ SES_gpm + SES + (1 | ID)\n   Data: hsb_sub\n\nREML criterion at convergence: 5188.7\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.74545 -0.75449  0.04934  0.75875  2.64005 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept)  3.13    1.769   \n Residual             35.81    5.984   \nNumber of obs: 800, groups:  ID, 160\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  12.8714     0.2536  50.750\nSES_gpm       2.2336     0.5962   3.746\nSES           2.4229     0.3481   6.960\n\nCorrelation of Fixed Effects:\n        (Intr) SES_gp\nSES_gpm -0.004       \nSES      0.000 -0.584\n\n\nThe coefficient for SES_gpm is now the contextual effect. We can get a 95% confidence interval:\n\nconfint(m1_mer2, parm = \"beta_\")\n\n                2.5 %    97.5 %\n(Intercept) 12.374410 13.368315\nSES_gpm      1.066932  3.400232\nSES          1.740067  3.105664\n\n\n\n\nSame analyses with Bayesian using brms\nI use the brms package with the default priors:\n\\[\\begin{align*}\n  Y_{ij} & \\sim N(\\mu_j + \\beta_1 (X_{ij} - \\bar X_{.j}), \\sigma^2) \\\\\n  \\mu_j & \\sim N(\\beta_0 + \\beta_2 \\bar X_{.j}, \\tau^2) \\\\\n  \\beta & \\sim N(0, \\infty) \\\\\n  \\sigma & \\sim t_3(0, 10) \\\\\n  \\tau & \\sim t_3(0, 10)\n\\end{align*}\\]\n\nm1_brm &lt;- brm(MATHACH ~ SES_gpm + SES_gpc + (1 | ID), data = hsb_sub, \n              control = list(adapt_delta = .90))\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 8.5e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.85 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 1.231 seconds (Warm-up)\nChain 1:                0.585 seconds (Sampling)\nChain 1:                1.816 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 4.5e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.45 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 1.235 seconds (Warm-up)\nChain 2:                0.583 seconds (Sampling)\nChain 2:                1.818 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 4.5e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.45 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 1.193 seconds (Warm-up)\nChain 3:                0.585 seconds (Sampling)\nChain 3:                1.778 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 8e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.8 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 1.487 seconds (Warm-up)\nChain 4:                0.585 seconds (Sampling)\nChain 4:                2.072 seconds (Total)\nChain 4: \n\nsummary(m1_brm)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: MATHACH ~ SES_gpm + SES_gpc + (1 | ID) \n   Data: hsb_sub (Number of observations: 800) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nGroup-Level Effects: \n~ID (Number of levels: 160) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.73      0.37     0.95     2.41 1.00     1035     1171\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    12.87      0.25    12.38    13.37 1.00     3461     3012\nSES_gpm       4.65      0.49     3.70     5.60 1.00     3831     2324\nSES_gpc       2.43      0.35     1.75     3.12 1.00     6509     3278\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     6.00      0.17     5.67     6.36 1.00     3018     2438\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nWith Bayesian, the results are similar to those with lme4.\nWe can summarize the posterior of the contextual effect:\n\npost_fixef &lt;- posterior_samples(m1_brm, pars = c(\"b_SES_gpm\", \"b_SES_gpc\"))\n\nWarning: Method 'posterior_samples' is deprecated. Please see ?as_draws for\nrecommended alternatives.\n\npost_contextual &lt;- with(post_fixef, b_SES_gpm - b_SES_gpc)\nc(mean = mean(post_contextual), median = median(post_contextual), \n  quantile(post_contextual, c(.025, .975)))\n\n    mean   median     2.5%    97.5% \n2.223545 2.237711 1.017085 3.437801 \n\nggplot(aes(x = post_contextual), data = data_frame(post_contextual)) + \n  geom_density(bw = \"SJ\")\n\nWarning: `data_frame()` was deprecated in tibble 1.1.0.\nℹ Please use `tibble()` instead.\n\n\n\n\n\n\n\nGroup mean centering treating group means as latent variables\nInstead of treating the group means as known, we can instead treat them as latent variables: \\[X_{ij} \\sim N(\\mu_{Xj}, \\sigma^2_X)\\]\nand we can set up the model with the priors:\n\\[\\begin{align*}\n  Y_{ij} & \\sim N(\\mu_j + \\beta_1 (X_{ij} - \\mu_{Xj}), \\sigma^2) \\\\\n  X_{ij} & \\sim N(\\mu_{Xj}, \\sigma^2_X) \\\\\n  \\mu_j & \\sim N(\\beta_0 + \\beta_2 \\mu_{Xj}, \\tau^2) \\\\\n  \\mu_{Xj} & \\sim N(0, \\tau^2_X) \\\\\n  \\sigma & \\sim t_3(0, 10) \\\\\n  \\tau & \\sim t_3(0, 10) \\\\\n  \\sigma_X & \\sim t_3(0, 10) \\\\\n  \\tau_X & \\sim t_3(0, 10) \\\\\n  \\beta & \\sim N(0, 10) \\\\\n\\end{align*}\\]\nNotice that here we treat \\(X\\) as an outcome variable with a random intercept, just like the way we model \\(Y\\) when there is no predictor.\nBelow is the STAN code for this model:\n\n\ndata { \n  int&lt;lower=1&gt; N;  // total number of observations \n  int&lt;lower=1&gt; J;  // number of clusters\n  int&lt;lower=1, upper=J&gt; gid[N]; \n  vector[N] y;  // response variable \n  int&lt;lower=1&gt; K;  // number of population-level effects \n  matrix[N, K] X;  // population-level design matrix \n  int&lt;lower=1&gt; q;  // index of which column needs group mean centering\n} \ntransformed data { \n  int Kc = K - 1; \n  matrix[N, K - 1] Xc;  // centered version of X \n  vector[K - 1] means_X;  // column means of X before centering\n  vector[N] xc;  // the column of X to be decomposed\n  for (i in 2:K) { \n    means_X[i - 1] = mean(X[, i]); \n    Xc[, i - 1] = X[, i] - means_X[i - 1]; \n  } \n  xc = Xc[, q - 1];\n} \nparameters { \n  vector[Kc] b;  // population-level effects at level-1\n  real bm;  // population-level effects at level-2\n  real b0;  // intercept (with centered variables)\n  real&lt;lower=0&gt; sigma_y;  // residual SD \n  real&lt;lower=0&gt; tau_y;  // group-level standard deviations \n  vector[J] eta_y;  // normalized group-level effects of y\n  real&lt;lower=0&gt; sigma_x;  // residual SD \n  real&lt;lower=0&gt; tau_x;  // group-level standard deviations \n  vector[J] eta_x;  // normalized latent group means of x\n} \ntransformed parameters { \n  // group means for x\n  vector[J] theta_x = tau_x * eta_x;  // group means of x\n  // group-level effects \n  vector[J] theta_y = b0 + tau_y * eta_y + bm * theta_x;  // group intercepts of y\n  matrix[N, K - 1] Xw_c = Xc;  // copy the predictor matrix\n  Xw_c[ , q - 1] = xc - theta_x[gid];  // group mean centering\n} \nmodel {\n  // prior specifications \n  b ~ normal(0, 10); \n  bm ~ normal(0, 10); \n  sigma_y ~ student_t(3, 0, 10); \n  tau_y ~ student_t(3, 0, 10); \n  eta_y ~ std_normal(); \n  sigma_x ~ student_t(3, 0, 10); \n  tau_x ~ student_t(3, 0, 10); \n  eta_x ~ std_normal(); \n  xc ~ normal(theta_x[gid], sigma_x);  // prior for lv-1 predictor\n  // likelihood contribution \n  y ~ normal(theta_y[gid] + Xw_c * b, sigma_y); \n} \ngenerated quantities {\n  // contextual effect\n  real b_contextual = bm - b[q - 1];\n}\n\n\nAnd to run it in rstan:\n\nhsb_sdata &lt;- with(hsb_sub, \n                  list(N = nrow(hsb_sub), \n                       y = MATHACH, \n                       K = 2, \n                       X = cbind(1, SES), \n                       q = 2, \n                       gid = match(ID, unique(ID)), \n                       J = length(unique(ID))))\nm2_stan &lt;- stan(\"stan_gpc_pv.stan\", data = hsb_sdata, \n                chains = 2L, iter = 1000L, \n                pars = c(\"b0\", \"b\", \"bm\", \"b_contextual\", \"sigma_y\", \"tau_y\", \n                         \"sigma_x\", \"tau_x\"))\n\nWarning: There were 1 divergent transitions after warmup. See\nhttps://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup\nto find out why this is a problem and how to eliminate them.\n\n\nWarning: Examine the pairs() plot to diagnose sampling problems\n\n\nWarning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#bulk-ess\n\n\nWarning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#tail-ess\n\n\nThe results are shown below:\n\nprint(m2_stan, pars = \"lp__\", include = FALSE)\n\nInference for Stan model: anon_model.\n2 chains, each with iter=1000; warmup=500; thin=1; \npost-warmup draws per chain=500, total post-warmup draws=1000.\n\n              mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nb0           12.88    0.01 0.26 12.35 12.72 12.88 13.07 13.40   675 1.00\nb[1]          2.41    0.01 0.35  1.75  2.17  2.41  2.63  3.09   668 1.00\nbm            5.89    0.04 0.77  4.44  5.37  5.87  6.41  7.39   379 1.01\nb_contextual  3.49    0.05 0.90  1.72  2.87  3.49  4.08  5.28   311 1.01\nsigma_y       6.00    0.01 0.18  5.65  5.88  6.01  6.12  6.35   349 1.00\ntau_y         1.41    0.05 0.53  0.19  1.08  1.46  1.80  2.27   104 1.01\nsigma_x       0.68    0.00 0.02  0.65  0.67  0.68  0.70  0.72   721 1.00\ntau_x         0.43    0.00 0.04  0.36  0.40  0.42  0.45  0.50   302 1.02\n\nSamples were drawn using NUTS(diag_e) at Tue Jun 13 20:40:46 2023.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nNote that the coefficient of SES at level-2 now has a posterior mean of 5.9 with a posterior SD of 0.77, and the contextual effect has a posterior mean of 3.5 with a posterior SD of 0.9\n\nTwo-step approach using brms\n\nStep 1: estimating measurement error in observed means\n\nm_ses &lt;- lmer(SES ~ (1 | ID), data = hsb_sub)\nsummary(m_ses)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: SES ~ (1 | ID)\n   Data: hsb_sub\n\nREML criterion at convergence: 1830.9\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-4.8031 -0.6035  0.0062  0.6787  2.3937 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept) 0.1839   0.4289  \n Residual             0.4617   0.6795  \nNumber of obs: 800, groups:  ID, 160\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept) 0.002775   0.041555   0.067\n\n\nExtract measurement estimates:\n\n# sigma_x\nsigmax &lt;- sigma(m_ses)\n# Add to data\nhsb_sub &lt;- hsb_sub %&gt;% \n  group_by(ID) %&gt;% \n  mutate(SES_gpm_se = sigmax / sqrt(n())) %&gt;% \n  ungroup()\n\nFit MLM, incorporating measurement error in latent group means:\n\nm1_brm_lm &lt;- brm(MATHACH ~ me(SES_gpm, sdx = SES_gpm_se, gr = ID) + \n                   SES + (1 | ID), \n                 data = hsb_sub, \n                 control = list(adapt_delta = .99, \n                                max_treedepth = 15))\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 8.9e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.89 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 8.854 seconds (Warm-up)\nChain 1:                4.871 seconds (Sampling)\nChain 1:                13.725 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 8e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.8 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 9.322 seconds (Warm-up)\nChain 2:                4.803 seconds (Sampling)\nChain 2:                14.125 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 8.6e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.86 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 6.35 seconds (Warm-up)\nChain 3:                4.763 seconds (Sampling)\nChain 3:                11.113 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 8.2e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.82 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 9.541 seconds (Warm-up)\nChain 4:                3.425 seconds (Sampling)\nChain 4:                12.966 seconds (Total)\nChain 4: \n\nsummary(m1_brm_lm)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: MATHACH ~ me(SES_gpm, sdx = SES_gpm_se, gr = ID) + SES + (1 | ID) \n   Data: hsb_sub (Number of observations: 800) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nGroup-Level Effects: \n~ID (Number of levels: 160) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     1.41      0.49     0.26     2.25 1.01      600      565\n\nPopulation-Level Effects: \n                               Estimate Est.Error l-95% CI u-95% CI Rhat\nIntercept                         12.87      0.26    12.35    13.38 1.00\nSES                                2.39      0.35     1.70     3.08 1.00\nmeSES_gpmsdxEQSES_gpm_segrEQID     3.49      0.91     1.73     5.29 1.00\n                               Bulk_ESS Tail_ESS\nIntercept                          5076     3543\nSES                                2459     2861\nmeSES_gpmsdxEQSES_gpm_segrEQID     1349     2186\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     6.01      0.17     5.68     6.33 1.00     2932     2863\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\n\n\n\n\nWith random slopes\nWith brms, we have\n\nm2_brm &lt;- brm(MATHACH ~ SES_gpm + SES_gpc + (SES_gpc | ID), data = hsb_sub, \n              control = list(max_treedepth = 15, adapt_delta = .90))\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 8.8e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.88 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 2.83 seconds (Warm-up)\nChain 1:                2.303 seconds (Sampling)\nChain 1:                5.133 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 8.3e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.83 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 3.583 seconds (Warm-up)\nChain 2:                2.586 seconds (Sampling)\nChain 2:                6.169 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 9.5e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.95 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 3.43 seconds (Warm-up)\nChain 3:                2.537 seconds (Sampling)\nChain 3:                5.967 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 9.5e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.95 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 3.48 seconds (Warm-up)\nChain 4:                2.487 seconds (Sampling)\nChain 4:                5.967 seconds (Total)\nChain 4: \n\n\nWarning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#tail-ess\n\nsummary(m2_brm)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: MATHACH ~ SES_gpm + SES_gpc + (SES_gpc | ID) \n   Data: hsb_sub (Number of observations: 800) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nGroup-Level Effects: \n~ID (Number of levels: 160) \n                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(Intercept)              1.71      0.41     0.80     2.41 1.01      636\nsd(SES_gpc)                1.00      0.65     0.04     2.38 1.00      970\ncor(Intercept,SES_gpc)     0.26      0.48    -0.81     0.96 1.00     2805\n                       Tail_ESS\nsd(Intercept)               345\nsd(SES_gpc)                1607\ncor(Intercept,SES_gpc)     2472\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    12.87      0.25    12.39    13.38 1.00     5543     3367\nSES_gpm       4.66      0.47     3.72     5.58 1.00     4872     3029\nSES_gpc       2.41      0.37     1.68     3.15 1.00     7791     2632\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     5.97      0.18     5.62     6.33 1.00     1590     1556\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nBelow is the STAN code for this model incorporating the unreliability of group means:\n\n\ndata { \n  int&lt;lower=1&gt; N;  // total number of observations \n  int&lt;lower=1&gt; J;  // number of clusters\n  int&lt;lower=1, upper=J&gt; gid[N]; \n  vector[N] y;  // response variable \n  int&lt;lower=1&gt; K;  // number of population-level effects \n  matrix[N, K] X;  // population-level design matrix \n  int&lt;lower=1&gt; q;  // index of which column needs group mean centering\n} \ntransformed data { \n  int Kc = K - 1; \n  matrix[N, K - 1] Xc;  // centered version of X \n  vector[K - 1] means_X;  // column means of X before centering\n  vector[N] xc;  // the column of X to be decomposed\n  for (i in 2:K) { \n    means_X[i - 1] = mean(X[, i]); \n    Xc[ , i - 1] = X[ , i] - means_X[i - 1]; \n  } \n  xc = Xc[ , q - 1];\n} \nparameters { \n  vector[Kc] b;  // population-level effects at level-1\n  real bm;  // population-level effects at level-2\n  real b0;  // intercept (with centered variables)\n  real&lt;lower=0&gt; sigma_y;  // residual SD \n  vector&lt;lower=0&gt;[2] tau_y;  // group-level standard deviations \n  matrix[2, J] z_u;  // normalized group-level effects \n  real&lt;lower=0&gt; sigma_x;  // residual SD \n  real&lt;lower=0&gt; tau_x;  // group-level standard deviations\n  cholesky_factor_corr[2] L_1; // Cholesky correlation factor of lv-2 residuals\n  vector[J] eta_x;  // unscaled group-level effects \n} \ntransformed parameters { \n  // group means for x\n  vector[J] theta_x = tau_x * eta_x;  // group means of x\n  // group-level effects of y\n  matrix[J, 2] u = (diag_pre_multiply(tau_y, L_1) * z_u)';\n} \nmodel { \n  // prior specifications \n  b ~ normal(0, 10); \n  bm ~ normal(0, 10); \n  sigma_y ~ student_t(3, 0, 10); \n  tau_y ~ student_t(3, 0, 10); \n  L_1 ~ lkj_corr_cholesky(2);\n  // z_y ~ normal(0, 1);\n  to_vector(z_u) ~ normal(0, 1);\n  sigma_x ~ student_t(3, 0, 10); \n  tau_x ~ student_t(3, 0, 10); \n  eta_x ~ std_normal(); \n  xc ~ normal(theta_x[gid], sigma_x);  // prior for lv-1 predictor\n  // likelihood contribution \n  {\n    matrix[N, K - 1] Xw_c = Xc;  // copy the predictor matrix\n    vector[N] x_gpc = xc - theta_x[gid]; \n    vector[J] beta0 = b0 + theta_x * bm + u[ , 1];\n    Xw_c[ , q - 1] = x_gpc;  // group mean centering\n    y ~ normal(beta0[gid] + Xw_c * b + u[gid, 2] .* x_gpc, \n               sigma_y); \n  }\n} \ngenerated quantities {\n  // contextual effect\n  real b_contextual = bm - b[q - 1];\n}\n\n\nAnd to run it in rstan:\n\nhsb_sdata &lt;- with(hsb_sub, \n                  list(N = nrow(hsb_sub), \n                       y = MATHACH, \n                       K = 2, \n                       X = cbind(1, SES), \n                       q = 2, \n                       gid = match(ID, unique(ID)), \n                       J = length(unique(ID))))\nm3_stan &lt;- stan(\"stan_gpc_pv_ran_slp.stan\", data = hsb_sdata, \n                chains = 2L, iter = 1000L, \n                control = list(adapt_delta = .99, max_treedepth = 15), \n                pars = c(\"b0\", \"b\", \"bm\", \"b_contextual\", \"sigma_y\", \"tau_y\", \n                         \"sigma_x\", \"tau_x\"))\n\nWarning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#bulk-ess\n\n\nWarning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#tail-ess\n\n\nThe results are shown below:\n\nprint(m3_stan, pars = \"lp__\", include = FALSE)\n\nInference for Stan model: anon_model.\n2 chains, each with iter=1000; warmup=500; thin=1; \npost-warmup draws per chain=500, total post-warmup draws=1000.\n\n              mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nb0           12.89    0.01 0.26 12.39 12.72 12.87 13.06 13.40   959 1.00\nb[1]          2.39    0.02 0.36  1.68  2.15  2.38  2.64  3.09   545 1.01\nbm            5.86    0.05 0.79  4.33  5.31  5.81  6.40  7.46   244 1.01\nb_contextual  3.47    0.07 0.93  1.75  2.79  3.45  4.11  5.29   189 1.01\nsigma_y       5.98    0.01 0.17  5.64  5.85  5.98  6.10  6.30   749 1.00\ntau_y[1]      1.40    0.04 0.46  0.45  1.14  1.43  1.73  2.18   130 1.02\ntau_y[2]      0.83    0.04 0.55  0.02  0.37  0.78  1.23  1.99   205 1.00\nsigma_x       0.68    0.00 0.02  0.64  0.67  0.68  0.69  0.72  1175 1.00\ntau_x         0.43    0.00 0.04  0.36  0.40  0.43  0.45  0.50   437 1.01\n\nSamples were drawn using NUTS(diag_e) at Tue Jun 13 20:44:14 2023.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\n\nUsing two-step with brms\n\nm3_brm_lm &lt;- brm(MATHACH ~ me(SES_gpm, sdx = SES_gpm_se, gr = ID) + \n                   SES + \n                   (SES_gpc | ID), \n                 data = hsb_sub, \n                 control = list(adapt_delta = .99, \n                                max_treedepth = 15))\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000136 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.36 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 19.781 seconds (Warm-up)\nChain 1:                7.743 seconds (Sampling)\nChain 1:                27.524 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0.000133 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 1.33 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 16.357 seconds (Warm-up)\nChain 2:                7.146 seconds (Sampling)\nChain 2:                23.503 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0.000131 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 1.31 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 17.364 seconds (Warm-up)\nChain 3:                7.121 seconds (Sampling)\nChain 3:                24.485 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0.00044 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 4.4 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 21.034 seconds (Warm-up)\nChain 4:                7.144 seconds (Sampling)\nChain 4:                28.178 seconds (Total)\nChain 4: \n\nsummary(m3_brm_lm)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: MATHACH ~ me(SES_gpm, sdx = SES_gpm_se, gr = ID) + SES + (SES_gpc | ID) \n   Data: hsb_sub (Number of observations: 800) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nGroup-Level Effects: \n~ID (Number of levels: 160) \n                       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\nsd(Intercept)              1.43      0.50     0.28     2.28 1.01      522\nsd(SES_gpc)                1.03      0.65     0.04     2.36 1.00      779\ncor(Intercept,SES_gpc)     0.23      0.49    -0.81     0.96 1.00     1761\n                       Tail_ESS\nsd(Intercept)               557\nsd(SES_gpc)                1634\ncor(Intercept,SES_gpc)     1859\n\nPopulation-Level Effects: \n                               Estimate Est.Error l-95% CI u-95% CI Rhat\nIntercept                         12.87      0.26    12.36    13.39 1.00\nSES                                2.39      0.37     1.68     3.10 1.00\nmeSES_gpmsdxEQSES_gpm_segrEQID     3.50      0.94     1.74     5.40 1.00\n                               Bulk_ESS Tail_ESS\nIntercept                          5703     2843\nSES                                2915     2288\nmeSES_gpmsdxEQSES_gpm_segrEQID     1469     2343\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     5.97      0.18     5.64     6.33 1.00     1952     2279\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1)."
  },
  {
    "objectID": "posts/2017-08-01-bayesian-mlm-with-group-mean-centering/index.html#using-the-full-data",
    "href": "posts/2017-08-01-bayesian-mlm-with-group-mean-centering/index.html#using-the-full-data",
    "title": "Bayesian MLM With Group Mean Centering",
    "section": "Using the Full Data",
    "text": "Using the Full Data\n\nWith lme4\n\nhsb &lt;- hsb %&gt;% mutate(SES_gpm = ave(SES, ID), SES_gpc = SES - SES_gpm)\nmfull_mer &lt;- lmer(MATHACH ~ SES_gpm + SES_gpc + (1 | ID), data = hsb)\nsummary(mfull_mer)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: MATHACH ~ SES_gpm + SES_gpc + (1 | ID)\n   Data: hsb\n\nREML criterion at convergence: 46568.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.1666 -0.7254  0.0174  0.7558  2.9454 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept)  2.693   1.641   \n Residual             37.019   6.084   \nNumber of obs: 7185, groups:  ID, 160\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  12.6833     0.1494   84.91\nSES_gpm       5.8662     0.3617   16.22\nSES_gpc       2.1912     0.1087   20.16\n\nCorrelation of Fixed Effects:\n        (Intr) SES_gpm\nSES_gpm 0.010         \nSES_gpc 0.000  0.000  \n\n\n\n\nWith Bayesian taking into account the unreliability\n\nhsb_sdata &lt;- with(hsb, \n                  list(N = nrow(hsb), \n                       y = MATHACH, \n                       K = 2, \n                       X = cbind(1, SES), \n                       q = 2, \n                       gid = match(ID, unique(ID)), \n                       J = length(unique(ID))))\n# This takes about 4 minutes on my computer\nm2full_stan &lt;- stan(\"stan_gpc_pv.stan\", data = hsb_sdata, \n                    chains = 2L, iter = 1000L, \n                    pars = c(\"b0\", \"b\", \"bm\", \"b_contextual\", \n                            \"sigma_y\", \"tau_y\", \"sigma_x\", \"tau_x\"))\n\nWarning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable.\nRunning the chains for more iterations may help. See\nhttps://mc-stan.org/misc/warnings.html#bulk-ess\n\n\n\nprint(m2full_stan, pars = \"lp__\", include = FALSE)\n\nInference for Stan model: anon_model.\n2 chains, each with iter=1000; warmup=500; thin=1; \npost-warmup draws per chain=500, total post-warmup draws=1000.\n\n              mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat\nb0           12.68    0.01 0.15 12.37 12.58 12.67 12.78 12.98   314 1.00\nb[1]          2.19    0.00 0.11  1.97  2.12  2.18  2.26  2.40   887 1.00\nbm            6.05    0.02 0.38  5.28  5.82  6.06  6.33  6.74   337 1.00\nb_contextual  3.86    0.02 0.40  3.02  3.62  3.88  4.14  4.61   344 1.01\nsigma_y       6.08    0.00 0.05  5.99  6.05  6.08  6.12  6.19  2004 1.00\ntau_y         1.61    0.01 0.13  1.36  1.52  1.61  1.69  1.89   342 1.00\nsigma_x       0.67    0.00 0.01  0.66  0.66  0.67  0.67  0.68  2052 1.00\ntau_x         0.40    0.00 0.02  0.36  0.39  0.40  0.42  0.45   158 1.01\n\nSamples were drawn using NUTS(diag_e) at Tue Jun 13 20:47:13 2023.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\n\nNote that with higher reliabilities, the results are more similar. Also, the results accounting for unreliability with the subset is much closer to the results with the full sample."
  },
  {
    "objectID": "posts/2022-06-17-scaling-and-standard-errors-in-sem/index.html",
    "href": "posts/2022-06-17-scaling-and-standard-errors-in-sem/index.html",
    "title": "Scaling and Standard Errors in SEM",
    "section": "",
    "text": "In this post, I demonstrate why rescaling a coefficient (i.e., multiplied/divided by a constant) is different from “standardizing” a coefficient (i.e., multiplied/divided by the sample standard deviation, which is a random variable) in SEM.\nSee also this discussion on StackExchange and the article When constraints interact: A caution about reference variables, identification constraints, and scale dependencies in structural equation modeling."
  },
  {
    "objectID": "posts/2022-06-17-scaling-and-standard-errors-in-sem/index.html#load-packages",
    "href": "posts/2022-06-17-scaling-and-standard-errors-in-sem/index.html#load-packages",
    "title": "Scaling and Standard Errors in SEM",
    "section": "Load Packages",
    "text": "Load Packages\n\nlibrary(lavaan)\n\nThis is lavaan 0.6-13\nlavaan is FREE software! Please report any bugs.\n\n# library(performance)\n# library(parameters)\nlibrary(modelsummary)"
  },
  {
    "objectID": "posts/2022-06-17-scaling-and-standard-errors-in-sem/index.html#different-scaling-choices",
    "href": "posts/2022-06-17-scaling-and-standard-errors-in-sem/index.html#different-scaling-choices",
    "title": "Scaling and Standard Errors in SEM",
    "section": "Different Scaling Choices",
    "text": "Different Scaling Choices\nIn the following, I manipulate the scaling constraints on (1) the average loading, (2) the first loading, (3) the second loading, and (4) the latent variance, so that they all have the same sample unit for the latent variable.\n\n1. Effect coding (average loading = 1)\n\nmodel &lt;- '\n  # latent variable definitions\n    ind60 =~ x1 + x2 + x3\n    dem60 =~ y1 + y2 + y3 + y4\n  # regression\n    dem60 ~ ind60\n'\nfit &lt;- sem(model, data = PoliticalDemocracy, effect.coding = \"loadings\")\nparameterEstimates(fit)\n\n     lhs op   rhs   est    se      z pvalue ci.lower ci.upper\n1  ind60 =~    x1 0.600 0.030 19.833  0.000    0.541    0.660\n2  ind60 =~    x2 1.308 0.044 29.622  0.000    1.222    1.395\n3  ind60 =~    x3 1.091 0.050 21.634  0.000    0.993    1.190\n4  dem60 =~    y1 0.809 0.072 11.309  0.000    0.669    0.950\n5  dem60 =~    y2 1.148 0.099 11.575  0.000    0.954    1.343\n6  dem60 =~    y3 0.887 0.094  9.444  0.000    0.703    1.071\n7  dem60 =~    y4 1.156 0.080 14.400  0.000    0.998    1.313\n8  dem60  ~ ind60 1.067 0.263  4.056  0.000    0.552    1.583\n9     x1 ~~    x1 0.081 0.020  4.123  0.000    0.043    0.120\n10    x2 ~~    x2 0.120 0.072  1.676  0.094   -0.020    0.261\n11    x3 ~~    x3 0.467 0.091  5.153  0.000    0.289    0.644\n12    y1 ~~    y1 2.404 0.516  4.663  0.000    1.394    3.415\n13    y2 ~~    y2 6.552 1.289  5.082  0.000    4.025    9.078\n14    y3 ~~    y3 5.363 0.996  5.384  0.000    3.410    7.315\n15    y4 ~~    y4 2.137 0.719  2.973  0.003    0.728    3.546\n16 ind60 ~~ ind60 1.245 0.215  5.778  0.000    0.823    1.667\n17 dem60 ~~ dem60 5.271 1.031  5.111  0.000    3.249    7.292\n\n\n\n\n2. Fix first loading\n\n# Get estimates from first model\ncoef(fit)[c(\"ind60=~x1\", \"dem60=~y1\")]\n\nind60=~x1 dem60=~y1 \n0.6002121 0.8094457 \n\nmodel &lt;- '\n  # latent variable definitions\n    ind60 =~ 0.6002121 * x1 + x2 + x3\n    dem60 =~ 0.8094457 * y1 + y2 + y3 + y4\n  # regression\n    dem60 ~ ind60\n'\nfit1 &lt;- sem(model, data = PoliticalDemocracy)\nparameterEstimates(fit1)\n\n     lhs op   rhs   est    se      z pvalue ci.lower ci.upper\n1  ind60 =~    x1 0.600 0.000     NA     NA    0.600    0.600\n2  ind60 =~    x2 1.308 0.084 15.633  0.000    1.144    1.472\n3  ind60 =~    x3 1.091 0.091 11.966  0.000    0.913    1.270\n4  dem60 =~    y1 0.809 0.000     NA     NA    0.809    0.809\n5  dem60 =~    y2 1.148 0.164  7.005  0.000    0.827    1.470\n6  dem60 =~    y3 0.887 0.139  6.396  0.000    0.615    1.158\n7  dem60 =~    y4 1.156 0.139  8.341  0.000    0.884    1.427\n8  dem60  ~ ind60 1.067 0.281  3.801  0.000    0.517    1.618\n9     x1 ~~    x1 0.081 0.020  4.123  0.000    0.043    0.120\n10    x2 ~~    x2 0.120 0.072  1.676  0.094   -0.020    0.261\n11    x3 ~~    x3 0.467 0.091  5.153  0.000    0.289    0.644\n12    y1 ~~    y1 2.404 0.516  4.663  0.000    1.394    3.415\n13    y2 ~~    y2 6.552 1.289  5.082  0.000    4.025    9.078\n14    y3 ~~    y3 5.363 0.996  5.384  0.000    3.410    7.315\n15    y4 ~~    y4 2.137 0.719  2.973  0.003    0.728    3.546\n16 ind60 ~~ ind60 1.245 0.241  5.170  0.000    0.773    1.717\n17 dem60 ~~ dem60 5.271 1.335  3.947  0.000    2.653    7.888\n\n\n\n\n3. Fix second loading\n\n# Get estimates from first model\ncoef(fit)[c(\"ind60=~x2\", \"dem60=~y2\")]\n\nind60=~x2 dem60=~y2 \n 1.308404  1.148334 \n\nmodel2 &lt;- '\n  # latent variable definitions\n    ind60 =~ NA * x1 + 1.308404 * x2 + x3\n    dem60 =~ NA * y1 + 1.148334 * y2 + y3 + y4\n  # regression\n    dem60 ~ ind60\n'\nfit2 &lt;- sem(model2, data = PoliticalDemocracy)\nparameterEstimates(fit2)  # SE changed!\n\n     lhs op   rhs   est    se      z pvalue ci.lower ci.upper\n1  ind60 =~    x1 0.600 0.038 15.633  0.000    0.525    0.675\n2  ind60 =~    x2 1.308 0.000     NA     NA    1.308    1.308\n3  ind60 =~    x3 1.091 0.083 13.149  0.000    0.929    1.254\n4  dem60 =~    y1 0.809 0.116  7.005  0.000    0.583    1.036\n5  dem60 =~    y2 1.148 0.000     NA     NA    1.148    1.148\n6  dem60 =~    y3 0.887 0.146  6.059  0.000    0.600    1.173\n7  dem60 =~    y4 1.156 0.151  7.674  0.000    0.860    1.451\n8  dem60  ~ ind60 1.067 0.284  3.761  0.000    0.511    1.623\n9     x1 ~~    x1 0.081 0.020  4.123  0.000    0.043    0.120\n10    x2 ~~    x2 0.120 0.072  1.676  0.094   -0.020    0.261\n11    x3 ~~    x3 0.467 0.091  5.153  0.000    0.289    0.644\n12    y1 ~~    y1 2.404 0.516  4.663  0.000    1.394    3.415\n13    y2 ~~    y2 6.552 1.289  5.082  0.000    4.025    9.078\n14    y3 ~~    y3 5.363 0.996  5.384  0.000    3.410    7.315\n15    y4 ~~    y4 2.137 0.719  2.973  0.003    0.728    3.546\n16 ind60 ~~ ind60 1.245 0.218  5.705  0.000    0.817    1.673\n17 dem60 ~~ dem60 5.271 1.447  3.642  0.000    2.434    8.107"
  },
  {
    "objectID": "posts/2022-06-17-scaling-and-standard-errors-in-sem/index.html#fix-latent-variances",
    "href": "posts/2022-06-17-scaling-and-standard-errors-in-sem/index.html#fix-latent-variances",
    "title": "Scaling and Standard Errors in SEM",
    "section": "4. Fix latent variances",
    "text": "4. Fix latent variances\n\n# Get estimates from first model\ncoef(fit)[c(\"ind60~~ind60\", \"dem60~~dem60\")]\n\nind60~~ind60 dem60~~dem60 \n    1.245068     5.270542 \n\nmodel3 &lt;- '\n  # latent variable definitions\n    ind60 =~ NA * x1 + x2 + x3\n    dem60 =~ NA * y1 + y2 + y3 + y4\n  # latent variances\n    ind60 ~~ 1.245068 * ind60\n    dem60 ~~ 5.270542 * dem60\n  # regression\n    dem60 ~ ind60\n'\nfit3 &lt;- sem(model3, data = PoliticalDemocracy)\nparameterEstimates(fit3)  # SE changed again!\n\n     lhs op   rhs   est    se      z pvalue ci.lower ci.upper\n1  ind60 =~    x1 0.600 0.058 10.340  0.000    0.486    0.714\n2  ind60 =~    x2 1.308 0.115 11.410  0.000    1.084    1.533\n3  ind60 =~    x3 1.091 0.115  9.477  0.000    0.866    1.317\n4  dem60 =~    y1 0.809 0.103  7.893  0.000    0.608    1.010\n5  dem60 =~    y2 1.148 0.158  7.285  0.000    0.839    1.457\n6  dem60 =~    y3 0.887 0.134  6.606  0.000    0.624    1.150\n7  dem60 =~    y4 1.156 0.126  9.169  0.000    0.909    1.403\n8  ind60 ~~ ind60 1.245 0.000     NA     NA    1.245    1.245\n9  dem60 ~~ dem60 5.271 0.000     NA     NA    5.271    5.271\n10 dem60  ~ ind60 1.067 0.295  3.619  0.000    0.489    1.645\n11    x1 ~~    x1 0.081 0.020  4.123  0.000    0.043    0.120\n12    x2 ~~    x2 0.120 0.072  1.676  0.094   -0.020    0.261\n13    x3 ~~    x3 0.467 0.091  5.153  0.000    0.289    0.644\n14    y1 ~~    y1 2.404 0.516  4.663  0.000    1.394    3.415\n15    y2 ~~    y2 6.552 1.289  5.082  0.000    4.025    9.078\n16    y3 ~~    y3 5.363 0.996  5.384  0.000    3.410    7.315\n17    y4 ~~    y4 2.137 0.719  2.973  0.003    0.728    3.546\n\n\n\nmsummary(list(`Effect coding` = fit,\n              `Fix item 1` = fit1,\n              `Fix item 2` = fit2,\n              `Fix variance` = fit3))\n\n\n\n\n\nEffect coding\nFix item 1\n Fix item 2\nFix variance\n\n\n\n\nind60 =~ x1\n0.600\n0.600\n0.600\n0.600\n\n\n\n(0.030)\n(0.000)\n(0.038)\n(0.058)\n\n\nind60 =~ x2\n1.308\n1.308\n1.308\n1.308\n\n\n\n(0.044)\n(0.084)\n(0.000)\n(0.115)\n\n\nind60 =~ x3\n1.091\n1.091\n1.091\n1.091\n\n\n\n(0.050)\n(0.091)\n(0.083)\n(0.115)\n\n\ndem60 =~ y1\n0.809\n0.809\n0.809\n0.809\n\n\n\n(0.072)\n(0.000)\n(0.116)\n(0.103)\n\n\ndem60 =~ y2\n1.148\n1.148\n1.148\n1.148\n\n\n\n(0.099)\n(0.164)\n(0.000)\n(0.158)\n\n\ndem60 =~ y3\n0.887\n0.887\n0.887\n0.887\n\n\n\n(0.094)\n(0.139)\n(0.146)\n(0.134)\n\n\ndem60 =~ y4\n1.156\n1.156\n1.156\n1.156\n\n\n\n(0.080)\n(0.139)\n(0.151)\n(0.126)\n\n\ndem60 ~ ind60\n1.067\n1.067\n1.067\n1.067\n\n\n\n(0.263)\n(0.281)\n(0.284)\n(0.295)\n\n\nNum.Obs.\n75\n75\n75\n75\n\n\nAIC\n1906.2\n1906.2\n1906.2\n1906.2\n\n\nBIC\n1940.9\n1940.9\n1940.9\n1940.9\n\n\n\n\n\n\n\nNote that the parameter estimates are all the same, but the standard errors are all different. This has implications for statistical power when doing latent variable analysis."
  },
  {
    "objectID": "posts/2022-06-17-scaling-and-standard-errors-in-sem/index.html#standardized-coefficients",
    "href": "posts/2022-06-17-scaling-and-standard-errors-in-sem/index.html#standardized-coefficients",
    "title": "Scaling and Standard Errors in SEM",
    "section": "Standardized Coefficients",
    "text": "Standardized Coefficients\n\nDelta method\nDirectly in lavaan\n\nmsummary(list(`Fix item 1` = fit,\n              `Fix item 2` = fit2,\n              `Fix variance` = fit3),\n         standardize = TRUE)\n\n\n\n\n\nFix item 1\n Fix item 2\nFix variance\n\n\n\n\nind60 =~ x1\n0.920\n0.920\n0.920\n\n\n\n(0.023)\n(0.023)\n(0.023)\n\n\nind60 =~ x2\n0.973\n0.973\n0.973\n\n\n\n(0.017)\n(0.017)\n(0.017)\n\n\nind60 =~ x3\n0.872\n0.872\n0.872\n\n\n\n(0.031)\n(0.031)\n(0.031)\n\n\ndem60 =~ y1\n0.804\n0.804\n0.804\n\n\n\n(0.051)\n(0.051)\n(0.051)\n\n\ndem60 =~ y2\n0.757\n0.757\n0.757\n\n\n\n(0.058)\n(0.058)\n(0.058)\n\n\ndem60 =~ y3\n0.704\n0.704\n0.704\n\n\n\n(0.066)\n(0.066)\n(0.066)\n\n\ndem60 =~ y4\n0.898\n0.898\n0.898\n\n\n\n(0.039)\n(0.039)\n(0.039)\n\n\ndem60 ~ ind60\n0.460\n0.460\n0.460\n\n\n\n(0.100)\n(0.100)\n(0.100)\n\n\nNum.Obs.\n75\n75\n75\n\n\nAIC\n1906.2\n1906.2\n1906.2\n\n\nBIC\n1940.9\n1940.9\n1940.9\n\n\n\n\n\n\n\n\n\n1. Define new parameters in the model\n\nmodel_std &lt;- '\n  # latent variable definitions\n    ind60 =~ x1 + x2 + x3\n    dem60 =~ y1 + y2 + y3 + y4\n  # latent variances\n    ind60 ~~ v1 * ind60\n    dem60 ~~ ev2 * dem60\n  # regression\n    dem60 ~ b * ind60\n  # define standardized coefficients\n    v2 := b^2 * v1 + ev2\n    beta := b * sqrt(v1 / v2)\n'\nfit_std &lt;- sem(model_std, data = PoliticalDemocracy)\nparameterEstimates(fit_std)  # same standardized coefficient\n\n     lhs op           rhs label   est    se      z pvalue ci.lower ci.upper\n1  ind60 =~            x1       1.000 0.000     NA     NA    1.000    1.000\n2  ind60 =~            x2       2.180 0.139 15.633  0.000    1.907    2.453\n3  ind60 =~            x3       1.818 0.152 11.966  0.000    1.520    2.116\n4  dem60 =~            y1       1.000 0.000     NA     NA    1.000    1.000\n5  dem60 =~            y2       1.419 0.203  7.005  0.000    1.022    1.816\n6  dem60 =~            y3       1.095 0.171  6.396  0.000    0.760    1.431\n7  dem60 =~            y4       1.428 0.171  8.341  0.000    1.092    1.763\n8  ind60 ~~         ind60    v1 0.449 0.087  5.170  0.000    0.279    0.619\n9  dem60 ~~         dem60   ev2 3.453 0.875  3.947  0.000    1.738    5.168\n10 dem60  ~         ind60     b 1.439 0.379  3.801  0.000    0.697    2.181\n11    x1 ~~            x1       0.081 0.020  4.123  0.000    0.043    0.120\n12    x2 ~~            x2       0.120 0.072  1.676  0.094   -0.020    0.261\n13    x3 ~~            x3       0.467 0.091  5.153  0.000    0.289    0.644\n14    y1 ~~            y1       2.404 0.516  4.663  0.000    1.394    3.415\n15    y2 ~~            y2       6.552 1.289  5.082  0.000    4.025    9.078\n16    y3 ~~            y3       5.363 0.996  5.384  0.000    3.410    7.315\n17    y4 ~~            y4       2.137 0.719  2.973  0.003    0.728    3.546\n18    v2 :=    b^2*v1+ev2    v2 4.382 1.089  4.024  0.000    2.248    6.517\n19  beta := b*sqrt(v1/v2)  beta 0.460 0.100  4.593  0.000    0.264    0.657\n\n\n\n\n2. Use constraints to fix variances\n\nmodel_con &lt;- '\n  # latent variable definitions\n    ind60 =~ NA * x1 + x2 + x3\n    dem60 =~ NA * y1 + y2 + y3 + y4\n  # latent variances\n    ind60 ~~ 1 * ind60\n    dem60 ~~ ev2 * dem60\n  # regression\n    dem60 ~ beta * ind60\n  # constraints\n    ev2 == 1 - beta^2\n'\nfit_con &lt;- sem(model_con, data = PoliticalDemocracy)\nparameterEstimates(fit_con)  # same standardized coefficient\n\n     lhs op   rhs label   est    se      z pvalue ci.lower ci.upper\n1  ind60 =~    x1       0.670 0.065 10.340  0.000    0.543    0.797\n2  ind60 =~    x2       1.460 0.128 11.410  0.000    1.209    1.711\n3  ind60 =~    x3       1.218 0.129  9.477  0.000    0.966    1.470\n4  dem60 =~    y1       2.093 0.260  8.049  0.000    1.584    2.603\n5  dem60 =~    y2       2.970 0.401  7.406  0.000    2.184    3.756\n6  dem60 =~    y3       2.293 0.342  6.696  0.000    1.622    2.964\n7  dem60 =~    y4       2.989 0.315  9.494  0.000    2.372    3.606\n8  ind60 ~~ ind60       1.000 0.000     NA     NA    1.000    1.000\n9  dem60 ~~ dem60   ev2 0.788 0.092  8.535  0.000    0.607    0.969\n10 dem60  ~ ind60  beta 0.460 0.100  4.593  0.000    0.264    0.657\n11    x1 ~~    x1       0.081 0.020  4.123  0.000    0.043    0.120\n12    x2 ~~    x2       0.120 0.072  1.676  0.094   -0.020    0.261\n13    x3 ~~    x3       0.467 0.091  5.153  0.000    0.289    0.644\n14    y1 ~~    y1       2.404 0.516  4.663  0.000    1.394    3.415\n15    y2 ~~    y2       6.552 1.289  5.082  0.000    4.025    9.078\n16    y3 ~~    y3       5.363 0.996  5.384  0.000    3.410    7.315\n17    y4 ~~    y4       2.137 0.719  2.973  0.003    0.729    3.546\n\n\n\n\n3. (Incorrect) constraining the first indicators\n\ncoef(fit_con)[c(\"ind60=~x1\", \"dem60=~y1\")]\n\nind60=~x1 dem60=~y1 \n0.6697334 2.0934489 \n\nmodel_conl &lt;- '\n  # latent variable definitions\n    ind60 =~ 0.6697334 * x1 + x2 + x3\n    dem60 =~ 2.0934489 * y1 + y2 + y3 + y4\n  # regression\n    dem60 ~ beta * ind60\n'\nfit_conl &lt;- sem(model_conl, data = PoliticalDemocracy)\nparameterEstimates(fit_conl)  # same standardized coefficient\n\n     lhs op   rhs label   est    se      z pvalue ci.lower ci.upper\n1  ind60 =~    x1       0.670 0.000     NA     NA    0.670    0.670\n2  ind60 =~    x2       1.460 0.093 15.633  0.000    1.277    1.643\n3  ind60 =~    x3       1.218 0.102 11.966  0.000    1.018    1.417\n4  dem60 =~    y1       2.093 0.000     NA     NA    2.093    2.093\n5  dem60 =~    y2       2.970 0.424  7.005  0.000    2.139    3.801\n6  dem60 =~    y3       2.293 0.359  6.396  0.000    1.590    2.996\n7  dem60 =~    y4       2.989 0.358  8.341  0.000    2.286    3.691\n8  dem60  ~ ind60  beta 0.460 0.121  3.801  0.000    0.223    0.698\n9     x1 ~~    x1       0.081 0.020  4.123  0.000    0.043    0.120\n10    x2 ~~    x2       0.120 0.072  1.676  0.094   -0.020    0.261\n11    x3 ~~    x3       0.467 0.091  5.153  0.000    0.289    0.644\n12    y1 ~~    y1       2.404 0.516  4.663  0.000    1.394    3.415\n13    y2 ~~    y2       6.552 1.289  5.082  0.000    4.025    9.078\n14    y3 ~~    y3       5.363 0.996  5.384  0.000    3.410    7.315\n15    y4 ~~    y4       2.137 0.719  2.973  0.003    0.728    3.546\n16 ind60 ~~ ind60       1.000 0.193  5.170  0.000    0.621    1.379\n17 dem60 ~~ dem60       0.788 0.200  3.947  0.000    0.397    1.179\n\n\n\nmsummary(list(`Define new par` = fit_std,\n              `Constraints` = fit_con,\n              `Fix loading` = fit_conl))\n\n\n\n\n\nDefine new par\nConstraints\nFix loading\n\n\n\n\nind60 =~ x1\n1.000\n0.670\n0.670\n\n\n\n(0.000)\n(0.065)\n(0.000)\n\n\nind60 =~ x2\n2.180\n1.460\n1.460\n\n\n\n(0.139)\n(0.128)\n(0.093)\n\n\nind60 =~ x3\n1.818\n1.218\n1.218\n\n\n\n(0.152)\n(0.129)\n(0.102)\n\n\ndem60 =~ y1\n1.000\n2.093\n2.093\n\n\n\n(0.000)\n(0.260)\n(0.000)\n\n\ndem60 =~ y2\n1.419\n2.970\n2.970\n\n\n\n(0.203)\n(0.401)\n(0.424)\n\n\ndem60 =~ y3\n1.095\n2.293\n2.293\n\n\n\n(0.171)\n(0.342)\n(0.359)\n\n\ndem60 =~ y4\n1.428\n2.989\n2.989\n\n\n\n(0.171)\n(0.315)\n(0.358)\n\n\ndem60 ~ ind60\n1.439\n0.460\n0.460\n\n\n\n(0.379)\n(0.100)\n(0.121)\n\n\nv2 × = b^2*v1+ev2\n4.382\n\n\n\n\n\n(1.089)\n\n\n\n\nbeta × = b*sqrt(v1/v2)\n0.460\n\n\n\n\n\n(0.100)\n\n\n\n\nNum.Obs.\n75\n75\n75\n\n\nAIC\n1906.2\n1906.2\n1906.2\n\n\nBIC\n1940.9\n1940.9\n1940.9\n\n\n\n\n\n\n\nAs can be shown, while the first two methods give the same standardized beta in terms of estimates and standard errors, the third does not as it does not account for the uncertainty in the standard deviation."
  },
  {
    "objectID": "posts/2020-12-28-unit-specific-vs-population-average-models/index.html",
    "href": "posts/2020-12-28-unit-specific-vs-population-average-models/index.html",
    "title": "Unit-Specific vs. Population-Average Models",
    "section": "",
    "text": "One thing that I always felt uncomfortable in multilevel modeling (MLM) is the concept of a unit-specific (US)/subject-specific model vs. a population-average (PA) model. I’ve come across it several times, but for some reason I haven’t really made an effort to fully understand it. I happened to come across this paper by Harring and Blozis, which I read before, and think that why not try to really understand the relationship between the coefficient estimates in a US model and in a PA model in the context of generalized linear mixed-effect model (GLMM). So I have this learning note.\nlibrary(tidyverse)\nlibrary(modelsummary)\nlibrary(glmmTMB)\n\nWarning in checkMatrixPackageVersion(): Package version inconsistency detected.\nTMB was built with Matrix version 1.5.4\nCurrent Matrix version is 1.5.4.1\nPlease re-install 'TMB' from source using install.packages('TMB', type = 'source') or ask CRAN for a binary version of 'TMB' matching CRAN's 'Matrix' package\n\n\nWarning in checkDepPackageVersion(dep_pkg = \"TMB\"): Package version inconsistency detected.\nglmmTMB was built with TMB version 1.9.2\nCurrent TMB version is 1.9.4\nPlease re-install glmmTMB from source or restore original 'TMB' package (see '?reinstalling' for more information)\n\nlibrary(geepack)\nWhile MLM/GLMM is a US model, which models the associations between predictors and the outcome for each cluster, PA models are popular in some areas of research, with the popular method of the generalized estimating equation (GEE). Whereas the fixed effect coefficients in US are the same as the coefficients in PA in linear models, when it comes to generalized linear models with nonlinear link functions, the coefficients are not the same. This is because some of the generalized linear models typically assume constant variance on the latent continuous response variable. For example, in a single-level logistic model and a GEE model, the latent response \\(Y^*\\) has a variance of \\(\\pi^2 / 3\\), but in a two-level model, the variance is \\(\\pi^2 / 3 + \\tau^2_0\\).1 Because the coefficients are in the unit of the latent response, it means that the coefficients are on different units for US vs. PA. But how are they different? I will explore four link functions: identity, log, probit, and logit. But first, some notations."
  },
  {
    "objectID": "posts/2020-12-28-unit-specific-vs-population-average-models/index.html#model-notations",
    "href": "posts/2020-12-28-unit-specific-vs-population-average-models/index.html#model-notations",
    "title": "Unit-Specific vs. Population-Average Models",
    "section": "Model Notations",
    "text": "Model Notations\nWhile in actual modeling, the distributional assumptions of the response variables are important (e.g., normal, Poisson), the comparison of US vs. PA mainly concerns the mean of the outcome and the link function. For all models, the random effects are normally distributed.\n\nConditional (US) Model\n\\[\n  \\begin{aligned}\n    \\mathop{\\mathrm{E}}(y_{ij} | u_j) & = \\mu_{ij} \\\\\n    h(\\mu_{ij}) & = \\boldsymbol{\\mathbf{x}}^\\top_{ij} \\boldsymbol{\\mathbf{\\gamma }}+ \\boldsymbol{\\mathbf{z}}^\\top_{ij} \\boldsymbol{\\mathbf{u}}_j\n  \\end{aligned}\n\\] where \\(h(\\cdot)\\) is the link function, \\(\\boldsymbol{\\mathbf{x}}_{ij}\\) and \\(\\boldsymbol{\\mathbf{z}}_{ij}\\) are the fixed and random covariates for the \\(i\\)th person in the \\(j\\)th cluster. The distributional assumption is \\(\\boldsymbol{\\mathbf{u}}_j \\sim N_q(\\boldsymbol{\\mathbf{0}}, \\boldsymbol{\\mathbf{G}})\\)\n\n\nMarginal (PA) Model\nNow one is modeling the marginal mean:\n\\[\n  \\begin{aligned}\n    \\mathop{\\mathrm{E}}(y_{ij}) & = \\mathop{\\mathrm{E}}[\\mathop{\\mathrm{E}}(y_{ij} | \\mu_{ij})] = \\mu^\\text{PA}_{ij} \\\\\n    h(\\mu^\\text{PA}_{ij}) & = \\boldsymbol{\\mathbf{x}}^\\top_{ij} \\boldsymbol{\\mathbf{\\gamma^\\text{PA}}}\n  \\end{aligned}\n\\] The above two formulas can be used to find the transformation from the unit-specific coefficients, \\(\\boldsymbol{\\mathbf{\\gamma}}\\), to the population-average coefficients, \\(\\boldsymbol{\\mathbf{\\gamma^\\text{PA}}}\\)."
  },
  {
    "objectID": "posts/2020-12-28-unit-specific-vs-population-average-models/index.html#identity-link",
    "href": "posts/2020-12-28-unit-specific-vs-population-average-models/index.html#identity-link",
    "title": "Unit-Specific vs. Population-Average Models",
    "section": "Identity Link",
    "text": "Identity Link\n\\[h(\\mu^\\text{PA}_{ij}) = \\mu_{ij}\\] From the US model \\[\n  \\begin{aligned}\n    \\mathop{\\mathrm{E}}(y_{ij}) & = \\mathop{\\mathrm{E}}[\\mathop{\\mathrm{E}}(y_{ij} | u_j)] = \\mathop{\\mathrm{E}}[\\boldsymbol{\\mathbf{x}}^\\top_{ij} \\boldsymbol{\\mathbf{\\gamma }}+ \\boldsymbol{\\mathbf{z}}^\\top_{ij} \\boldsymbol{\\mathbf{u}}_j] \\\\\n    & = \\boldsymbol{\\mathbf{x}}^\\top_{ij} \\boldsymbol{\\mathbf{\\gamma}}\n  \\end{aligned}\n\\] Compare to the PA model \\[\\mathop{\\mathrm{E}}(y_{ij}) = \\boldsymbol{\\mathbf{x}}^\\top_{ij} \\boldsymbol{\\mathbf{\\gamma^\\text{PA}}},\\] we have \\(\\boldsymbol{\\mathbf{\\gamma }}= \\boldsymbol{\\mathbf{\\gamma^\\text{PA}}}\\)\n\nPlot\n\n# Simulate predictor X ~ U(-2, 2), with ICC = 0\nnum_obs &lt;- 2e4\nnum_subjects &lt;- 200\nx &lt;- runif(num_obs, min = -1, max = 3)\n# Subject IDs\nsubject_id &lt;- rep(seq_len(num_subjects), each = num_obs / num_subjects)\n\n\n# Fixed effects\ngamma0 &lt;- -1\ngamma1 &lt;- 1\n# Random intercepts and \ntau2_u &lt;- 0.25\n# u &lt;- rnorm(num_subjects, mean = 0, sd = sqrt(tau2_u))\nu &lt;- qnorm(seq(0.01, 0.99, length.out = num_subjects), sd = sqrt(tau2_u))\n# Simulate mu_{ij}\nmu &lt;- gamma0 + gamma1 * x + u[subject_id]\n# Plot\ndf &lt;- tibble(y = mu, x = x, id = subject_id)\nggplot(df, aes(x = x, y = y)) + \n  geom_smooth(aes(group = id), \n              se = FALSE, \n              method = \"lm\",  \n              size = 0.2) + \n  geom_smooth(se = FALSE, col = \"red\") + \n  stat_function(fun = function(x) gamma0 + gamma1 * x, \n                col = \"blue\", size = 1.4)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nThe blue line represents the regression line when \\(u_j = 0\\), and the red line is the regression line for the population-average model. They are the same.\n\n\nCompare NLME with GEE\n\n# Unit-Specific\nm_us &lt;- glmmTMB(mu ~ x + (x | id), data = df, \n                family = gaussian(link = \"identity\"), \n                dispformula = ~ 0)\n# Population-Average\nm_pa &lt;- geeglm(mu ~ x, \n               family = gaussian(link = \"identity\"),\n               data = df, \n               id = id, \n               corstr = \"exchangeable\")\n# The coefficients are the same\ncbind(US = fixef(m_us)$cond, PA = coef(m_pa))\n\n                    US PA\n(Intercept) -0.9999857 -1\nx            1.0000000  1"
  },
  {
    "objectID": "posts/2020-12-28-unit-specific-vs-population-average-models/index.html#log-link",
    "href": "posts/2020-12-28-unit-specific-vs-population-average-models/index.html#log-link",
    "title": "Unit-Specific vs. Population-Average Models",
    "section": "Log Link",
    "text": "Log Link\nThe log link is commonly used in the Poisson model.\n\\[h(\\mu^\\text{PA}_{ij}) = \\log(\\mu_{ij})\\] From the US model \\[\n  \\begin{aligned}\n    \\mathop{\\mathrm{E}}(y_{ij}) & = \\mathop{\\mathrm{E}}[\\mathop{\\mathrm{E}}(y_{ij} | u_j)] = \\mathop{\\mathrm{E}}[h^{-1}(\\boldsymbol{\\mathbf{x}}^\\top_{ij} \\boldsymbol{\\mathbf{\\gamma }}+ \\boldsymbol{\\mathbf{z}}^\\top_{ij} \\boldsymbol{\\mathbf{u}}_j)] \\\\\n    & = \\mathop{\\mathrm{E}}[\\exp(\\boldsymbol{\\mathbf{x}}^\\top_{ij} \\boldsymbol{\\mathbf{\\gamma }}+ \\boldsymbol{\\mathbf{z}}^\\top_{ij} \\boldsymbol{\\mathbf{u}}_j)] \\\\\n    & = \\exp(\\boldsymbol{\\mathbf{x}}^\\top_{ij} \\boldsymbol{\\mathbf{\\gamma}}) \\mathop{\\mathrm{E}}[\\exp(\\boldsymbol{\\mathbf{z}}^\\top_{ij} \\boldsymbol{\\mathbf{u}}_j))] \\\\\n    & = \\exp(\\boldsymbol{\\mathbf{x}}^\\top_{ij} \\boldsymbol{\\mathbf{\\gamma}}) \\exp[\\boldsymbol{\\mathbf{z}}\\top_{ij} \\boldsymbol{\\mathbf{G}} \\boldsymbol{\\mathbf{z}}_{ij} / 2] \\\\\n    & = \\exp(\\boldsymbol{\\mathbf{x}}^\\top_{ij} \\boldsymbol{\\mathbf{\\gamma }}+ \\boldsymbol{\\mathbf{z}}\\top_{ij} \\boldsymbol{\\mathbf{G}} \\boldsymbol{\\mathbf{z}}_{ij} / 2)\n  \\end{aligned}\n\\] Compare to the population-average model \\[\\mathop{\\mathrm{E}}(y_{ij}) = h^{-1}(\\boldsymbol{\\mathbf{x}}^\\top_{ij} \\boldsymbol{\\mathbf{\\gamma^\\text{PA}}}) = \\exp(\\boldsymbol{\\mathbf{x}}^\\top_{ij} \\boldsymbol{\\mathbf{\\gamma^\\text{PA}}}),\\] we have \\(\\boldsymbol{\\mathbf{\\gamma^\\text{PA}}}= \\boldsymbol{\\mathbf{\\gamma }}+ [\\boldsymbol{\\mathbf{z}}^\\top_{ij} \\boldsymbol{\\mathbf{G}} \\boldsymbol{\\mathbf{z}}_{ij} / 2 \\quad \\boldsymbol{\\mathbf{0}}]^\\top\\). So the intercept has an offset, while the other coefficients stay the same.\n\nPlot\n\n# Fixed effects\ngamma0 &lt;- 1\ngamma1 &lt;- 0.2\n# Random intercepts and \ntau2_u &lt;- 0.25\n# u &lt;- rnorm(num_subjects, mean = 0, sd = sqrt(tau2_u))\nu &lt;- qnorm(seq(0.01, 0.99, length.out = num_subjects), sd = sqrt(tau2_u))\n# Simulate mu_{ij}\nmu &lt;- exp(gamma0 + gamma1 * x + u[subject_id])\n# Plot\ndf &lt;- tibble(y = mu, x = x, id = subject_id)\nggplot(df, aes(x = x, y = y)) + \n  geom_smooth(aes(group = id), \n              se = FALSE, \n              method = \"glm\",\n              method.args = list(family = gaussian(\"log\")), \n              size = 0.2) + \n  geom_smooth(se = FALSE, col = \"red\") + \n  stat_function(fun = function(x) exp(gamma0 + gamma1 * x), \n                col = \"blue\", size = 1.4)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nThe intercept for the red line has an offset of \\(\\tau^2_0 / 2 = 0.125\\).\n\n\nCompare NLME with GEE\n\n# Unit-Specific\nm_us &lt;- glmmTMB(mu ~ x + (x | id), data = df, \n                family = gaussian(link = \"log\"), \n                dispformula = ~ 0)\n# Population-Average\nm_pa &lt;- geeglm(mu ~ x, \n               family = gaussian(link = \"log\"),\n               data = df, \n               id = id, \n               corstr = \"exchangeable\")\n# The coefficients are the same\ncbind(US = fixef(m_us)$cond, PA = coef(m_pa))\n\n             US        PA\n(Intercept) 1.0 1.1178892\nx           0.2 0.1981924\n\n\nThe offset is in the intercept."
  },
  {
    "objectID": "posts/2020-12-28-unit-specific-vs-population-average-models/index.html#probit-link",
    "href": "posts/2020-12-28-unit-specific-vs-population-average-models/index.html#probit-link",
    "title": "Unit-Specific vs. Population-Average Models",
    "section": "Probit Link",
    "text": "Probit Link\nThe probit link, or the inverse normal cumulative density function, is commonly used in probit regression. \\[h(\\mu^\\text{PA}_{ij}) = \\Phi(\\mu_{ij})\\] From the US model \\[\n  \\begin{aligned}\n    \\mathop{\\mathrm{E}}(y_{ij}) & = \\mathop{\\mathrm{E}}[\\mathop{\\mathrm{E}}(y_{ij} | u_j)] = \\mathop{\\mathrm{E}}[h^{-1}(\\boldsymbol{\\mathbf{x}}^\\top_{ij} \\boldsymbol{\\mathbf{\\gamma }}+ \\boldsymbol{\\mathbf{z}}^\\top_{ij} \\boldsymbol{\\mathbf{u}}_j)] \\\\\n    & = \\mathop{\\mathrm{E}}[\\Phi(\\boldsymbol{\\mathbf{x}}^\\top_{ij} \\boldsymbol{\\mathbf{\\gamma }}+ \\boldsymbol{\\mathbf{z}}^\\top_{ij} \\boldsymbol{\\mathbf{u}}_j)] \\\\\n    & = P(Z \\leq \\boldsymbol{\\mathbf{x}}^\\top_{ij} \\boldsymbol{\\mathbf{\\gamma }}+ \\boldsymbol{\\mathbf{z}}^\\top_{ij} \\boldsymbol{\\mathbf{u}}_j) \\quad \\text{for } Z \\sim N(0, 1) \\\\\n    & = P(Z - \\boldsymbol{\\mathbf{z}}^\\top_{ij} \\boldsymbol{\\mathbf{u}}_j \\leq \\boldsymbol{\\mathbf{x}}^\\top_{ij} \\boldsymbol{\\mathbf{\\gamma}}),\n  \\end{aligned}\n\\] where \\(Z - \\boldsymbol{\\mathbf{z}}^\\top_{ij} \\boldsymbol{\\mathbf{u}}_j \\sim N(0, \\sqrt{1 + \\boldsymbol{\\mathbf{z}}^\\top_{ij} \\boldsymbol{\\mathbf{G}} \\boldsymbol{\\mathbf{z}}_{ij}})\\). So \\[\n  \\begin{aligned}\n    \\mathop{\\mathrm{E}}(y_{ij}) & = P\\left(\\frac{Z - \\boldsymbol{\\mathbf{z}}^\\top_{ij} \\boldsymbol{\\mathbf{u}}_j}{\\sqrt{1 + \\boldsymbol{\\mathbf{z}}^\\top_{ij} \\boldsymbol{\\mathbf{G}} \\boldsymbol{\\mathbf{z}}_{ij}}} \\leq \\frac{\\boldsymbol{\\mathbf{x}}^\\top_{ij} \\boldsymbol{\\mathbf{\\gamma}}}{\\sqrt{1 + \\boldsymbol{\\mathbf{z}}^\\top_{ij} \\boldsymbol{\\mathbf{G}} \\boldsymbol{\\mathbf{z}}_{ij}}}\\right) \\\\\n    & = \\Phi[(1 + \\boldsymbol{\\mathbf{z}}^\\top_{ij} \\boldsymbol{\\mathbf{G}} \\boldsymbol{\\mathbf{z}}_{ij})^{-1/2} \\boldsymbol{\\mathbf{x}}^\\top_{ij} \\boldsymbol{\\mathbf{\\gamma}}]\n  \\end{aligned}\n\\] So the PA coefficients shrinks by a factor of \\((1 + \\boldsymbol{\\mathbf{z}}^\\top_{ij} \\boldsymbol{\\mathbf{G}} \\boldsymbol{\\mathbf{z}}_{ij})^{-1/2}\\).\n\nRandom intercepts only\n\nPlot\n\n# Fixed effects\ngamma0 &lt;- -0.5\ngamma1 &lt;- 1\n# Random intercepts and \ntau2_u &lt;- 0.3\n# u &lt;- rnorm(num_subjects, mean = 0, sd = sqrt(tau2_u))\nu &lt;- qnorm(seq(0.01, 0.99, length.out = num_subjects), sd = sqrt(tau2_u))\n# Simulate mu_{ij}\nmu &lt;- pnorm(gamma0 + gamma1 * x + u[subject_id])\n# Plot\ndf &lt;- tibble(y = mu, x = x, id = subject_id)\nggplot(df, aes(x = x, y = y)) + \n  geom_smooth(aes(group = id), \n              se = FALSE, \n              method = \"glm\",\n              method.args = list(family = gaussian(\"probit\")), \n              size = 0.2) + \n  geom_smooth(se = FALSE, col = \"red\") + \n  stat_function(fun = function(x) pnorm(gamma0 + gamma1 * x), \n                col = \"blue\", size = 1.4)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nThe shrinkage factor is \\((1 + \\tau^2_0)^{-1/2} = 0.877058\\).\n\n\nCompare NLME with GEE\n\n# Unit-Specific\nm_us &lt;- glmmTMB(mu ~ x + (x | id), data = df, \n                family = gaussian(link = \"probit\"), \n                dispformula = ~ 0)\n# Population-Average\nm_pa &lt;- geeglm(mu ~ x, \n               family = gaussian(link = \"probit\"),\n               data = df, \n               id = id, \n               corstr = \"exchangeable\")\n# The coefficients are the same\ncbind(US = fixef(m_us)$cond, PA = coef(m_pa))\n\n                   US         PA\n(Intercept) -0.499999 -0.4458653\nx            1.000000  0.8858915\n\n\nSmaller coefficients for both the intercept and \\(x\\).\n\n\n\nRandom intercepts and slopes\n\nPlot\n\n# Add random slopes\ntau2_u1 &lt;- 0.15\n# u1 &lt;- rnorm(num_subjects, mean = 0, sd = sqrt(tau2_u1))\nu1 &lt;- qnorm(seq(0.01, 0.99, length.out = num_subjects), \n            sd = sqrt(tau2_u1))\n# permutate the random effects to make u and u1 independent\nu1 &lt;- sample(u1)\n# Simulate y\nmu2 &lt;- pnorm(gamma0 + (gamma1 + u1[subject_id]) * x + u[subject_id])\n# Plot\ndf2 &lt;- tibble(y = mu2, x = x, id = subject_id)\nggplot(df2, aes(x = x, y = y)) + \n  geom_smooth(aes(group = id), \n              se = FALSE, \n              method = \"glm\",\n              method.args = list(family = gaussian(\"probit\")), \n              size = 0.2) + \n  geom_smooth(se = FALSE, col = \"red\") + \n  stat_function(fun = function(x) pnorm(gamma0 + gamma1 * x), \n                col = \"blue\", size = 1.4)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nThe shrinking factor is \\([1 + (\\tau^2_0 + 2 \\tau_{01} \\bar x + \\tau^2_1 var(x))]^{-1/2}\\) = \\([1 + (0.3 + 0.15 \\times 4 / 3)]^{-1/2}\\) = 0.8164966.\n\n\nCompare NLME with GEE\n\n# Unit-Specific\nm_us &lt;- glmmTMB(mu2 ~ x + (x | id), data = df2, \n                family = gaussian(link = \"probit\"), \n                dispformula = ~ 0)\n# Population-Average\nm_pa &lt;- geeglm(mu2 ~ x, \n               family = gaussian(link = \"probit\"),\n               data = df2, \n               id = id, \n               corstr = \"exchangeable\")\n# The coefficients are the same\ncbind(US = fixef(m_us)$cond, PA = coef(m_pa))\n\n                   US         PA\n(Intercept) -0.499973 -0.3112705\nx            1.000002  0.8302407\n\n\nSmaller coefficients for both the intercept and \\(x\\)."
  },
  {
    "objectID": "posts/2020-12-28-unit-specific-vs-population-average-models/index.html#logit-link",
    "href": "posts/2020-12-28-unit-specific-vs-population-average-models/index.html#logit-link",
    "title": "Unit-Specific vs. Population-Average Models",
    "section": "Logit Link",
    "text": "Logit Link\nThe logit link is commonly used in logistic regression. \\[h(\\mu^\\text{PA}_{ij}) = \\log \\frac{\\mu_{ij}}{1 - \\mu_{ij}}\\]\nFrom the US model \\[\n  \\begin{aligned}\n    \\mathop{\\mathrm{E}}(y_{ij}) & = \\mathop{\\mathrm{E}}[\\mathop{\\mathrm{E}}(y_{ij} | u_j)] = \\mathop{\\mathrm{E}}[h^{-1}(\\boldsymbol{\\mathbf{x}}^\\top_{ij} \\boldsymbol{\\mathbf{\\gamma }}+ \\boldsymbol{\\mathbf{z}}^\\top_{ij} \\boldsymbol{\\mathbf{u}}_j)] \\\\\n    & = \\mathop{\\mathrm{E}}[\\mathop{\\mathrm{logit}}^{-1}(\\boldsymbol{\\mathbf{x}}^\\top_{ij} \\boldsymbol{\\mathbf{\\gamma }}- \\boldsymbol{\\mathbf{z}}^\\top_{ij} \\boldsymbol{\\mathbf{u}}_j)\n  \\end{aligned}\n\\]\nThe integral does not have a closed-form expression and cannot be expressed as a logistic function. However, one can approximates a normal cdf using a logistic function, and vice versa, and there are several ways to do it.2 Zeger, Liang, and Albert (1988, p. 1054) provides one approximation that results in \\[\\mathop{\\mathrm{E}}(y_{ij}) \\approx \\mathop{\\mathrm{logit}}^{-1}[a_l(\\boldsymbol{\\mathbf{G}}) \\boldsymbol{\\mathbf{x}}^\\top_{ij} \\boldsymbol{\\mathbf{\\gamma}}],\\] where \\(a_l(\\boldsymbol{\\mathbf{G}}) = (1 + c^2 \\boldsymbol{\\mathbf{z}}^\\top_{ij} \\boldsymbol{\\mathbf{G}} \\boldsymbol{\\mathbf{z}}_{ij})^{-1/2}\\) and \\(c^2 = \\left(\\frac{16}{15}\\right)^2 \\frac{3}{\\pi^2} \\approx 1 / 1.7^2 = 0.3460208\\), which was used in Allison (2009). Some other authors, like Snijders and Bosker (2012), use a simpler approximation with \\(c^2 = \\frac{3}{\\pi^2} \\approx 0.3039636\\).\n\nRandom intercepts only\n\nPlot\n\n# Fixed effects\ngamma0 &lt;- -1\ngamma1 &lt;- 2\n# Random intercepts and subject IDs\ntau2_u &lt;- 1\n# u &lt;- rnorm(num_subjects, mean = 0, sd = sqrt(tau2_u))\nu &lt;- qnorm(seq(0.01, 0.99, length.out = num_subjects), sd = sqrt(tau2_u))\n# Simulate y\nmu &lt;- plogis(gamma0 + gamma1 * x + u[subject_id])\n# Plot\ndf &lt;- tibble(y = mu, x = x, id = subject_id)\nggplot(df, aes(x = x, y = y)) + \n  geom_smooth(aes(group = id), \n              se = FALSE, \n              method = \"glm\", \n              method.args = list(family = gaussian(\"logit\")), \n              size = 0.2) + \n  geom_smooth(se = FALSE, col = \"red\") + \n  stat_function(fun = function(x) plogis(gamma0 + gamma1 * x), \n                col = \"blue\", size = 1.4)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nThe shrinkage factor is \\((1 + \\tau^2_0 / 1.7^2)^{-1/2} = 0.8619342\\).\n\n\nCompare NLME with GEE\n\n# Unit-Specific\nm_us &lt;- glmmTMB(mu ~ x + (x | id), data = df, \n                family = gaussian(link = \"logit\"), \n                dispformula = ~ 0)\n# Population-Average\nm_pa &lt;- geeglm(mu ~ x, \n               family = gaussian(link = \"logit\"),\n               data = df, \n               id = id, \n               corstr = \"exchangeable\")\n# The coefficients are the same\ncbind(US = fixef(m_us)$cond, PA = coef(m_pa))\n\n                    US        PA\n(Intercept) -0.9999994 -0.876522\nx            2.0000000  1.711613\n\n\nSmaller coefficients for both the intercept and \\(x\\).\n\n\n\nRandom intercepts and slopes\n\n# Add random slopes\ntau2_u1 &lt;- 0.5\n# u1 &lt;- rnorm(num_subjects, mean = 0, sd = sqrt(tau2_u1))\nu1 &lt;- qnorm(seq(0.01, 0.99, length.out = num_subjects), \n            sd = sqrt(tau2_u1))\n# permutate the random effects to make u and u1 independent\nu1 &lt;- sample(u1)\n# Simulate y\nmu2 &lt;- plogis(gamma0 + (gamma1 + u1[subject_id]) * x + u[subject_id])\n# Plot\ndf2 &lt;- tibble(y = mu2, x = x, id = subject_id)\nggplot(df2, aes(x = x, y = y)) + \n  geom_smooth(aes(group = id), \n              se = FALSE, \n              method = \"glm\", \n              method.args = list(family = gaussian(\"logit\")), \n              size = 0.2) + \n  geom_smooth(se = FALSE, col = \"red\") + \n  stat_function(fun = function(x) plogis(gamma0 + gamma1 * x), \n                col = \"blue\", size = 1.4)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nThe shrinking factor is approximately \\([1 + (\\tau^2_0 + \\tau^2_1 var(x)) / 1.7^2]^{-1/2}\\) = \\([1 + (1 + 0.5 \\times 4 / 3) / 1.7^2]^{-1/2}\\) = 0.7963891.\n\nCompare NLME with GEE\n\n# Unit-Specific\nm_us &lt;- glmmTMB(mu2 ~ x + (x | id), data = df2, \n                family = gaussian(link = \"logit\"), \n                dispformula = ~ 0)\n# Population-Average\nm_pa &lt;- geeglm(mu2 ~ x, \n               family = gaussian(link = \"logit\"),\n               data = df2, \n               id = id, \n               corstr = \"exchangeable\")\n# The coefficients are the same\ncbind(US = fixef(m_us)$cond, PA = coef(m_pa))\n\n                    US        PA\n(Intercept) -0.9999936 -0.701903\nx            1.9999867  1.585452"
  },
  {
    "objectID": "posts/2020-12-28-unit-specific-vs-population-average-models/index.html#footnotes",
    "href": "posts/2020-12-28-unit-specific-vs-population-average-models/index.html#footnotes",
    "title": "Unit-Specific vs. Population-Average Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSnijders & Bosker (2012), chapter 17.↩︎\nhttps://www.johndcook.com/blog/2010/05/18/normal-approximation-to-logistic/↩︎"
  },
  {
    "objectID": "posts/2020-04-10-cfa-standard-error/index.html",
    "href": "posts/2020-04-10-cfa-standard-error/index.html",
    "title": "Asymptotic Standard Errors in CFA",
    "section": "",
    "text": "In our lab meeting I’m going to present the article by Maydeu-Olivares (2017), which talked about standard errors (SEs) of the maximum likelihood estimators (MLEs) in SEM models. As I’m also working on something relevant, and I haven’t found some good references (I’m sure there are some, just too lazy to do a deep search), I decided to write this demo to show how the SEs can be obtained in R and compared that to the lavaan package, which automates these computations."
  },
  {
    "objectID": "posts/2020-04-10-cfa-standard-error/index.html#define-true-model-and-simulate-some-data",
    "href": "posts/2020-04-10-cfa-standard-error/index.html#define-true-model-and-simulate-some-data",
    "title": "Asymptotic Standard Errors in CFA",
    "section": "Define True Model and Simulate Some Data",
    "text": "Define True Model and Simulate Some Data\nTo keep things simple, I’ll use a one-factor model with four indicators (without mean structure):\n\\[\\mathbf{y} = \\boldsymbol{\\Lambda} \\boldsymbol{\\eta} + \\boldsymbol{\\varepsilon}\\]\n\n\\(\\mathbf{y}\\) = observed scores\n\\(\\boldsymbol{\\Lambda}\\) = loading matrix (vector in this case)\n\\(\\boldsymbol{\\eta}\\) = latent score (true score)\n\\(\\boldsymbol{\\varepsilon}\\) = random measurement error/unique factor scores\n\n\nlibrary(lavaan)\n\n&gt;# This is lavaan 0.6-13\n&gt;# lavaan is FREE software! Please report any bugs.\n\nLAM &lt;- rep(.7, 4)  # loading vector\nTHETA &lt;- diag(1 - LAM^2)\nN &lt;- 100\nset.seed(1)  # Set seed\n\nI’ll simulate data that follows the normal distribution:\n\ny &lt;- mvnfast::rmvn(n = N, mu = rep(0, 4), sigma = tcrossprod(LAM) + THETA)\n# # Check kurtosis\n# apply(y, 2, e1071::kurtosis)"
  },
  {
    "objectID": "posts/2020-04-10-cfa-standard-error/index.html#define-log-likelihood-function",
    "href": "posts/2020-04-10-cfa-standard-error/index.html#define-log-likelihood-function",
    "title": "Asymptotic Standard Errors in CFA",
    "section": "Define Log-Likelihood Function",
    "text": "Define Log-Likelihood Function\nThe model parameters are \\((\\boldsymbol{\\lambda}, \\boldsymbol{\\theta})\\), which include four loadings and four uniquenesses, with latent factor variance fixed to 1. Assuming normality, the sample covariance matrix, \\(\\mathbf{S}\\) (Sy in the code), is sufficient statistic:\n\n# Sample covariance S\n(Sy &lt;- cov(y) * (N - 1) / N)\n\n&gt;#           [,1]      [,2]      [,3]      [,4]\n&gt;# [1,] 0.9962082 0.4808498 0.3465193 0.3733755\n&gt;# [2,] 0.4808498 1.0076039 0.4208120 0.5062814\n&gt;# [3,] 0.3465193 0.4208120 0.9332648 0.3749744\n&gt;# [4,] 0.3733755 0.5062814 0.3749744 0.9388328\n\n\nNote that I divided by \\(N\\) instead of \\(N - 1\\), which is the default for lavaan (\\(N - 1\\) corresponds to Wishart likelihood). When normality holds, \\(\\mathbf{S}\\) follows a Wishart distribution with degrees of freedom \\(N\\). Denote \\(\\bv \\Sigma = \\bv \\Sigma(\\boldsymbol{\\lambda}, \\boldsymbol{\\theta})\\) as the model-implied covariance matrix. The likelihood function is \\[\\mathcal{L}(\\bv \\Sigma; S) \\propto\n  \\frac{\\exp\\left[- \\frac{1}{2}\\tr(\\bv \\Sigma^{-1} N \\bv S)\\right]}\n       {|\\Sigma|^\\frac{N}{2}}\\] and so the log-likelihood function is \\[\\mathcal{l}(\\bv \\Sigma; S) =\n  - \\frac{N}{2}\\left[\\tr(\\bv \\Sigma^{-1} \\bv S) +\n                     \\log |\\Sigma|\\right] + C\\] where \\(C\\) is some constant that does not involve \\(\\bv \\Sigma\\).\n\nDefining \\(\\mathcal{l}(\\bv \\Sigma; S)\\) in R:\nThis is pretty straight forward in R\n\nloglikSigma &lt;- function(pars, S = Sy, N = 100) {\n  # pars: First four as lambdas, then thetas\n  Sigma &lt;- tcrossprod(pars[1:4]) + diag(pars[5:8])\n  - N / 2 * (determinant(Sigma)$modulus[1] + \n       sum(diag(solve(Sigma, S))))\n}"
  },
  {
    "objectID": "posts/2020-04-10-cfa-standard-error/index.html#mle",
    "href": "posts/2020-04-10-cfa-standard-error/index.html#mle",
    "title": "Asymptotic Standard Errors in CFA",
    "section": "MLE",
    "text": "MLE\nIn maximum likelihood estimation, we find the parameter values that would maximize the above log-likelihood function (as log is monotonic). As an example, we consider these two sets of estimates:\n\n# Set 1\nlam1 &lt;- c(.6, .6, .8, .7); theta1 &lt;- c(.5, .4, .4, .5)\nloglikSigma(c(lam1, theta1))\n\n&gt;# [1] -158.8054\n\n# Set 2\nlam2 &lt;- c(.7, .7, .8, .7); theta2 &lt;- c(.5, .5, .4, .3)\nloglikSigma(c(lam2, theta2))\n\n&gt;# [1] -161.6866\n\n\nThe first set gives higher log-likelihood (i.e., less negative) value, so should be preferred. We can try many sets of values until we find the best by hand, but it is very tedious. So instead we rely on some optimizers to help us achieve that. In R, some general-purpose optimizer can be found in the package optim.\nOne thing to note before we start. Usually optimizers help us find parameters that minimizes an objective function, but here we want to maximizes it. So one trick is to instead minimize the -2 \\(\\times\\) log-likelihood:\n\nminus2lSigma &lt;- function(pars, S = Sy, N = 100) {\n  # pars: First four as lambdas, then thetas\n  Sigma &lt;- tcrossprod(pars[1:4]) + diag(pars[5:8])\n  N * (determinant(Sigma)$modulus[1] + \n         sum(diag(solve(Sigma, S))))\n}\n\n\nminus2lSigma(c(lam1, theta1))\n\n&gt;# [1] 317.6109\n\nminus2lSigma(c(lam2, theta2))\n\n&gt;# [1] 323.3733\n\n\nSo again the first set is preferred with lower -2 log-likelihood. Now we use optim, with the “L-BFGS-B” algorithm. It requires some starting values, so I put it the values of set 1 (you can verify that the results are the same with set 2 as starting values):\n\nopt_info &lt;- optim(c(lam1, theta1), minus2lSigma, \n                  lower = c(rep(-Inf, 4), rep(0, 4)), \n                  method = \"L-BFGS-B\", hessian = TRUE)\n# Maximum likelihood estimates:\nopt_info$par\n\n&gt;# [1] 0.6068613 0.7783132 0.5576544 0.6467392 0.6279234 0.4018284 0.6222836\n&gt;# [8] 0.5205601\n\n\nSo in terms of point estimates, that’s it! Let’s compare the results to lavaan:\n\nfit1 &lt;- lavaan::cfa(model = 'f =~ y1 + y2 + y3 + y4', \n                    std.lv = TRUE, \n                    # Note: lavaan transform S automatically by multiplying \n                    # (N - 1) / N\n                    sample.cov = `colnames&lt;-`(cov(y), paste0(\"y\", 1:4)), \n                    sample.nobs = 100)\n\n\ncbind(optim = opt_info$par, \n      `lavaan::cfa` = lavaan::coef(fit1))\n\n&gt;#            optim lavaan::cfa\n&gt;# f=~y1  0.6068613   0.6068697\n&gt;# f=~y2  0.7783132   0.7783153\n&gt;# f=~y3  0.5576544   0.5576568\n&gt;# f=~y4  0.6467392   0.6467406\n&gt;# y1~~y1 0.6279234   0.6279174\n&gt;# y2~~y2 0.4018284   0.4018292\n&gt;# y3~~y3 0.6222836   0.6222836\n&gt;# y4~~y4 0.5205601   0.5205593\n\n\nSo they are essentially the same (the difference in rounding is mainly due to different optimizer and starting values)."
  },
  {
    "objectID": "posts/2020-04-10-cfa-standard-error/index.html#asymptotic-standard-errors",
    "href": "posts/2020-04-10-cfa-standard-error/index.html#asymptotic-standard-errors",
    "title": "Asymptotic Standard Errors in CFA",
    "section": "Asymptotic Standard Errors",
    "text": "Asymptotic Standard Errors\nNow onto standard errors, which is the main focus of Maydeu-Olivares (2017). The paper also talk about sandwich estimators which are robust (to some degree) to nonnormality, but I will focus on the ones with observed and expected information first, and then show how to obtain MLM and MLR standard errors in plain R (to some degree).\nAlthough in the statistics literature, generally observed information is preferred over the use of expected information (both of which related to the second partial derivatives of the log-likelihood function, see here), in lavaan the default is to use the expected information, and so I will demonstrate that first.\n\nUsing expected information\nAs described in the paper, the asymptotic covariance matrix of the MLE, which estimates how the estimates vary and covary across repeated samples, has this formula: \\[\\acov(\\hat{\\bv \\lambda}, \\hat{\\bv \\theta}) =\n  N^{-1} (\\bv \\Delta' \\bv \\Gamma^{-1}_E \\bv \\Delta)^{-1}\\] where \\[\\bv \\Gamma^{-1}_E = \\frac{1}{2} (\\bv \\Sigma^{-1} \\otimes \\bv \\Sigma^{-1})\\] is the expected information matrix (which will be evaluated as the MLE). Note that the paper also talked about the duplication matrix (which potentially increases computational efficiency), but for simplicity I left it out as we don’t have a huge matrix here.\n\nTODO: Add the explanation on the terms\nThe asymptotic standard errors are the square root values of the diagonal elements in \\(\\acov(\\hat{\\bv \\lambda}, \\hat{\\bv \\theta})\\). After we obtained the MLEs, we can easily construct the model-implied covariance \\(\\Sigma\\), and let R do the inverse and the Kronecker product (there are computational shortcuts in this example, but not essential in this demo). The \\(\\bv \\Delta\\) matrix contains partial derivatives. As an example, with \\[\\bv \\Sigma = \\begin{bmatrix}\n                 \\lambda_1^2 + \\theta_1 & & & \\\\\n                 \\lambda_1 \\lambda_2 & \\lambda_2^2 + \\theta_2 & & \\\\\n                 \\lambda_1 \\lambda_3 & \\lambda_2 \\lambda_3 &\n                   \\lambda_3^2 + \\theta_3 & \\\\\n                 \\lambda_1 \\lambda_4 & \\lambda_2 \\lambda_4 &\n                   \\lambda_3 \\lambda_4 & \\lambda_4^2 + \\theta_4 \\\\\n               \\end{bmatrix},\\] if we align the elements in \\(\\Sigma\\) to be a vector, denoted as \\(\\vect(\\Sigma)\\), we have \\[\\vect(\\Sigma) = [\\lambda_1^2 + \\theta_1, \\lambda_1 \\lambda_2, \\lambda_1 \\lambda_3, \\cdots]', \\] so \\[\\bv \\Delta = \\begin{bmatrix}\n                 \\frac{\\partial (\\lambda_1^2 + \\theta_1)}{\\partial \\lambda_1} &\n                 \\frac{\\partial (\\lambda_1^2 + \\theta_1)}{\\partial \\lambda_2} & \\cdots \\\\\n                 \\frac{\\partial (\\lambda_1 \\lambda_2)}{\\partial \\lambda_1} &\n                 \\frac{\\partial (\\lambda_1 \\lambda_2)}{\\partial \\lambda_2} & \\cdots \\\\\n                 \\vdots & \\ddots & \\vdots \\\\\n               \\end{bmatrix}, \\] which is a \\(p^2 \\times r\\) matrix where \\(r\\) is the number of parameters (8 in this case). This can be obtained by hand, but we can also use the numDeriv::jacobian() function like:\n\n# Using parameter estimates from optim to construct Sigma\n(Sigmahat &lt;- tcrossprod(opt_info$par[1:4]) + diag(opt_info$par[5:8]))\n\n&gt;#           [,1]      [,2]      [,3]      [,4]\n&gt;# [1,] 0.9962040 0.4723281 0.3384188 0.3924810\n&gt;# [2,] 0.4723281 1.0075997 0.4340297 0.5033656\n&gt;# [3,] 0.3384188 0.4340297 0.9332620 0.3606569\n&gt;# [4,] 0.3924810 0.5033656 0.3606569 0.9388317\n\n# Make it into a function, outputting vec(Sigma)\nvecSigma &lt;- function(pars) {\n  as.vector(tcrossprod(pars[1:4]) + diag(pars[5:8]))\n}\n# Numeric derivative:\n(Delta &lt;- numDeriv::jacobian(vecSigma, opt_info$par))\n\n&gt;#            [,1]      [,2]      [,3]      [,4] [,5] [,6] [,7] [,8]\n&gt;#  [1,] 1.2137226 0.0000000 0.0000000 0.0000000    1    0    0    0\n&gt;#  [2,] 0.7783132 0.6068613 0.0000000 0.0000000    0    0    0    0\n&gt;#  [3,] 0.5576544 0.0000000 0.6068613 0.0000000    0    0    0    0\n&gt;#  [4,] 0.6467392 0.0000000 0.0000000 0.6068613    0    0    0    0\n&gt;#  [5,] 0.7783132 0.6068613 0.0000000 0.0000000    0    0    0    0\n&gt;#  [6,] 0.0000000 1.5566263 0.0000000 0.0000000    0    1    0    0\n&gt;#  [7,] 0.0000000 0.5576544 0.7783132 0.0000000    0    0    0    0\n&gt;#  [8,] 0.0000000 0.6467392 0.0000000 0.7783132    0    0    0    0\n&gt;#  [9,] 0.5576544 0.0000000 0.6068613 0.0000000    0    0    0    0\n&gt;# [10,] 0.0000000 0.5576544 0.7783132 0.0000000    0    0    0    0\n&gt;# [11,] 0.0000000 0.0000000 1.1153087 0.0000000    0    0    1    0\n&gt;# [12,] 0.0000000 0.0000000 0.6467392 0.5576544    0    0    0    0\n&gt;# [13,] 0.6467392 0.0000000 0.0000000 0.6068613    0    0    0    0\n&gt;# [14,] 0.0000000 0.6467392 0.0000000 0.7783132    0    0    0    0\n&gt;# [15,] 0.0000000 0.0000000 0.6467392 0.5576544    0    0    0    0\n&gt;# [16,] 0.0000000 0.0000000 0.0000000 1.2934784    0    0    0    1\n\n\nNow, we’re ready to compute the asymptotic standard errors:\n\ninvSigmahat &lt;- solve(Sigmahat)\n# Expected Fisher information\nexp_I &lt;- invSigmahat %x% invSigmahat / 2\n# Asymptotic covariance\nacov_exp &lt;- solve(crossprod(Delta, exp_I) %*% Delta) / N\n# Asymptotic SE\nase_exp &lt;- sqrt(diag(acov_exp))\n\nCompared to lavaan:\n\ncbind(optim = ase_exp, \n      `lavaan::cfa ('expected')` = sqrt(diag(lavaan::vcov(fit1))))\n\n&gt;#             optim lavaan::cfa ('expected')\n&gt;# f=~y1  0.10400371               0.10400353\n&gt;# f=~y2  0.10242249               0.10242227\n&gt;# f=~y3  0.10139392               0.10139383\n&gt;# f=~y4  0.09991077               0.09991052\n&gt;# y1~~y1 0.10889211               0.10889173\n&gt;# y2~~y2 0.10757581               0.10757529\n&gt;# y3~~y3 0.10420294               0.10420288\n&gt;# y4~~y4 0.09956021               0.09955978\n\n\nSo that matches pretty well!\n\n\nObserved information (using Hessian)\nWhile Maydeu-Olivares (2017) also presented the formula (in matrix form) for the asymptotic covariance estimates using the expected information, in practice such a matrix is usually obtained based on the Hessian, which usually is part of the output in the maximization algorithm in MLE. For example, going back to the output of the optim() call,\n\n# As we've multiplied by -2 before, we need to divide by 2 now.\n# The Hessian is not affected by multiplication of -1\nacov_obs &lt;- solve(opt_info$hessian / 2)\n(ase_obs &lt;- sqrt(diag(solve(opt_info$hessian / 2))))\n\n&gt;# [1] 0.10381614 0.10240102 0.10189012 0.09983351 0.10862886 0.10752628 0.10480306\n&gt;# [8] 0.09943057\n\n\nAnd compare to lavaan using the information = 'observed' option:\n\nfit1_obs &lt;- lavaan::update(fit1, information = 'observed')\ncbind(optim = ase_obs, \n      `lavaan::cfa ('obs')` = sqrt(diag(lavaan::vcov(fit1_obs))))\n\n&gt;#             optim lavaan::cfa ('obs')\n&gt;# f=~y1  0.10381614          0.10381636\n&gt;# f=~y2  0.10240102          0.10240110\n&gt;# f=~y3  0.10189012          0.10189033\n&gt;# f=~y4  0.09983351          0.09983363\n&gt;# y1~~y1 0.10862886          0.10862830\n&gt;# y2~~y2 0.10752628          0.10752643\n&gt;# y3~~y3 0.10480306          0.10480358\n&gt;# y4~~y4 0.09943057          0.09943066\n\n\nSo again it’s pretty close. You can try using the formula in Maydeu-Olivares (2017) to verify the results."
  },
  {
    "objectID": "posts/2020-04-10-cfa-standard-error/index.html#mlmmlmv",
    "href": "posts/2020-04-10-cfa-standard-error/index.html#mlmmlmv",
    "title": "Asymptotic Standard Errors in CFA",
    "section": "MLM/MLMV",
    "text": "MLM/MLMV\n\n# Asymptotic covariance of S (4th moment)\nWi_adf &lt;- apply(t(y) - colMeans(y), 2, tcrossprod)\nadf_I &lt;- tcrossprod(Wi_adf - as.vector(Sy)) / N\n# Sandwich estimator:\nDeltat_exp_I &lt;- crossprod(Delta, exp_I)\nacov_mlm &lt;- acov_exp %*% Deltat_exp_I %*% adf_I %*% \n  crossprod(Deltat_exp_I, acov_exp) * N\nase_mlm &lt;- sqrt(diag(acov_mlm))\n\nCompare to lavaan with estimator = 'MLR':\n\n# lavaan\nfit1_mlm &lt;- lavaan::cfa(model = 'f =~ y1 + y2 + y3 + y4', \n                        std.lv = TRUE, \n                        # Note: Full data needed for MLM\n                        data = `colnames&lt;-`(y, paste0(\"y\", 1:4)), \n                        estimator = \"MLM\")\n\n\ncbind(optim = ase_mlm, \n      `lavaan::cfa ('MLM')` = sqrt(diag(lavaan::vcov(fit1_mlm))))\n\n&gt;#             optim lavaan::cfa ('MLM')\n&gt;# f=~y1  0.12965429          0.12965371\n&gt;# f=~y2  0.10522671          0.10522608\n&gt;# f=~y3  0.08901649          0.08901603\n&gt;# f=~y4  0.08981597          0.08981560\n&gt;# y1~~y1 0.10278055          0.10278061\n&gt;# y2~~y2 0.10252608          0.10252560\n&gt;# y3~~y3 0.09792141          0.09792136\n&gt;# y4~~y4 0.10608980          0.10608957"
  },
  {
    "objectID": "posts/2020-04-10-cfa-standard-error/index.html#mlr",
    "href": "posts/2020-04-10-cfa-standard-error/index.html#mlr",
    "title": "Asymptotic Standard Errors in CFA",
    "section": "MLR",
    "text": "MLR\n\n# Cross-product matrix (without mean structure)\n# First, need the deviation/residual of covariance for each individual\n# observation\nSdev &lt;- apply(t(y) - colMeans(y), 2, tcrossprod) - as.vector(Sigmahat)\ng_score &lt;- crossprod(Sdev, invSigmahat %x% invSigmahat) / 2\nxp_I &lt;- crossprod(g_score) / N  # Gamma^-1_XP\n# Sandwich estimator:\nacov_mlr &lt;- acov_obs %*% crossprod(Delta, xp_I %*% Delta) %*% acov_obs * N\nase_mlr &lt;- sqrt(diag(acov_mlr))\n\nCompare to lavaan with estimator = 'MLR':\n\n# lavaan\nfit1_mlr &lt;- lavaan::cfa(model = 'f =~ y1 + y2 + y3 + y4', \n                        std.lv = TRUE, \n                        # Note: Full data needed for MLR\n                        data = `colnames&lt;-`(y, paste0(\"y\", 1:4)), \n                        estimator = \"MLR\")\n\n\ncbind(optim = ase_mlr, \n      `lavaan::cfa ('MLR, Hessian')` = sqrt(diag(lavaan::vcov(fit1_mlr))))\n\n&gt;#             optim lavaan::cfa ('MLR, Hessian')\n&gt;# f=~y1  0.12884960                   0.12885037\n&gt;# f=~y2  0.10504885                   0.10504891\n&gt;# f=~y3  0.08977431                   0.08977434\n&gt;# f=~y4  0.08883634                   0.08883676\n&gt;# y1~~y1 0.10203109                   0.10203105\n&gt;# y2~~y2 0.10139778                   0.10139847\n&gt;# y3~~y3 0.09785923                   0.09786031\n&gt;# y4~~y4 0.10566978                   0.10567060"
  },
  {
    "objectID": "posts/2017-11-30-model-selection-for-multilevel-modeling/index.html",
    "href": "posts/2017-11-30-model-selection-for-multilevel-modeling/index.html",
    "title": "Model Selection for Multilevel Modeling",
    "section": "",
    "text": "In social sciences, many times we use statistical methods to answer well-defined research questions that are derived from some theory or previous research. For example, theory may suggest that interventions to improve students’ self-efficacy may help benefit their academic performance, so we would like to test a mediation model of intervention –&gt; self-efficacy –&gt; academic performance. We may also learn from previous studies that the intervention may work differently for different genders, so we would like to include a intervention \\(\\times\\) gender interaction.\nHowever, innovations often arise from exploratory data analysis where existing theory may provide only partial or little guidance to understand our data. This is especially true for multilevel modeling (MLM), as theories that are truly multilevel are relatively rare. The additional model choices in MLM also contribute to this, as theories seldom tell whether a relationship between two variables are the same or different in different levels, or whether there are heterogeneity of level-1 relationship across level-2 units, or whether there are specific cross-level interactions. One takeaway from this of course is we need better theories in our disciplines. But for research with a more exploratory focus and in the absence of established theories, we want to fully explore our data while having some measures to save us from over-interpreting the noise in a single data set."
  },
  {
    "objectID": "posts/2017-11-30-model-selection-for-multilevel-modeling/index.html#complexity-of-mlm",
    "href": "posts/2017-11-30-model-selection-for-multilevel-modeling/index.html#complexity-of-mlm",
    "title": "Model Selection for Multilevel Modeling",
    "section": "Complexity of MLM",
    "text": "Complexity of MLM\nWith single-level analyses, if one has one predictor, call it \\(X\\), one only needs to estimate the coefficient of that predictor and perhaps evaluates whether there is statistical evidence that the predictor has predictive power (e.g., with hypothesis tests), assuming the assumptions of the statistical model is satisfied. However, with a two-level analyses, especially if the predictor is at level 1, then things can already get complicated. For example, one can ask these questions:\n\nIs \\(X\\) related to the outcome overall?\nDoes \\(X\\) has both a lv-1 effect and a lv-2 effect?\nIf yes, are the effects at different levels the same or different?\nIf \\(X\\) has a lv-1 effect, is there a random slope?\n\nJust imagine the complexity with more predictors and the potential for different interactions and cross-level interaction effects. Such complexity has two major consequences:\n\nIf we were to do a statistical test for all of the fixed and random effects, we run into risks of huge Type I error inflation (just like post hoc testing in ANOVA). \\(P\\)-values are not trustworthy!\nPerhaps more importantly, unless one has a very large sample size, the parameter estimates are highly unstable with a complex model and can be completely not reproducible."
  },
  {
    "objectID": "posts/2017-11-30-model-selection-for-multilevel-modeling/index.html#information-criteria",
    "href": "posts/2017-11-30-model-selection-for-multilevel-modeling/index.html#information-criteria",
    "title": "Model Selection for Multilevel Modeling",
    "section": "Information Criteria",
    "text": "Information Criteria\nIf statistical significance and \\(p\\)-values cannot do the job, what can? How about effect size like \\(R^2\\) we discussed before? Unfortunately, \\(R^2\\) is not designed for that purpose, and choosing models that have the largest \\(R^2\\) will generally also result in models that are unstable and not reproducible.\nIn statistics, there are indices that are specifically designed for comparing different models and evaluating their reproducibility, and by far two of the most common indices are the Akaike Information Criterion (AIC) and the so called Bayesian Information Criterion (BIC) (also called the Schwarz Criterion).\nThere are different ways to understand these information criteria, and AIC and BIC are actually developed with very different motivation and background, despite how commonly they are mentioned together. Nevertheless, the most classic way, at least for AIC, is that it is a measure of the fit of the model to a different and independent sample. In other words, if you come up with a model using the data you have now, you then collect a new sample, how good can your model describe what is happening in the new sample? The smaller the AIC/BIC, the better the fit of the model to a new sample. All information criteria follow a general form:\n\\[\\mathrm{IC} = \\text{Deviance} + \\text{Penalty}\\]\nThe Penalty term is always a function of the complexity of the model, usually measured by the number of parameters estimated. Remember that\n\nThe smaller the deviance, the better the model fit, and\nA more complex model almost always gives a smaller deviance.\n\nTherefore, without the Penalty one always selects the most complex model, which may be never reproducible in an independent sample. Instead, AIC and BIC usually are formulated as \\[\\begin{align*}\n  \\mathrm{AIC} & = \\text{Deviance} + 2 q \\\\\n  \\mathrm{BIC} & = \\text{Deviance} + q \\log N,\n\\end{align*}\\] where \\(q\\) is the number of estimated parameters (both fixed and random). Note, however, that the computation of AIC and BIC may be different for different software, especially for BIC as most software packages define \\(N\\) as the number of groups, but some other packages define \\(N\\) as the number of units at the lowest level. Nevertheless, regardless of the definitions, they tend to work fine for the general purpose of comparing models, and are generally better than using deviance or other significance tests. We will look at the models we have previously fitted in the HSB data set with SES and SECTOR to predict MATHACH.\n\nExample\n\nlibrary(tidyverse)\nlibrary(haven)\nlibrary(lme4)\n\n\nhsb &lt;- haven::read_sas('https://stats.idre.ucla.edu/wp-content/uploads/2016/02/hsb.sas7bdat')\n\nThe following models are fitted:\n\nM0: Random-intercept only\nM1: Adding SES\nM2: M1 + random slope of SES\nM3: M2 + MEANSES\nM4: M3 + SECTOR\nM5: M4 + SECTOR \\(\\times\\) SES interaction\nM6: M5 + SECTOR \\(\\times\\) MEANSES interaction\n\n\n# It's generally recommended to use ML instead of REML to get ICs\nm0 &lt;- lmer(MATHACH ~ (1 | ID), data = hsb, REML = FALSE)\n# You can use the `update` function to add terms\nm1 &lt;- update(m0, . ~ . + SES)\n# To add random slope, replace the original random intercept term\nm2 &lt;- update(m1, . ~ . - (1 | ID) + (SES | ID))\nm3 &lt;- update(m2, . ~ . + MEANSES)\nm4 &lt;- update(m3, . ~ . + SECTOR)\nm5 &lt;- update(m4, . ~ . + SES:SECTOR)\nm6 &lt;- update(m5, . ~ . + MEANSES:SECTOR)"
  },
  {
    "objectID": "posts/2017-11-30-model-selection-for-multilevel-modeling/index.html#selecting-fixed-effects",
    "href": "posts/2017-11-30-model-selection-for-multilevel-modeling/index.html#selecting-fixed-effects",
    "title": "Model Selection for Multilevel Modeling",
    "section": "Selecting Fixed Effects",
    "text": "Selecting Fixed Effects\nAs a starting point, one can usually first compare models with different fixed effects, especially those that are at level 1. For example, if one has three predictors: MINORITY, FEMALE, and SES, and wonder whether there are main and interaction effects from them in predicting MATHACH1 without strong prior theory, one can easily run all of the possible models without random slopes and level-2 effects by first defining the most complex model:\n\n# Need ML\nm_3wayinter &lt;- lmer(MATHACH ~ MINORITY * FEMALE * SES + (1 | ID), data = hsb, \n                    REML = FALSE)\n\nand then with the help of the MuMIn package in R:\n\nlibrary(MuMIn)\noptions(na.action = \"na.fail\")  # set the missing data handling method\ndd &lt;- dredge(m_3wayinter)\nmodel.sel(dd, rank = AIC)\n\nGlobal model call: lmer(formula = MATHACH ~ MINORITY * FEMALE * SES + (1 | ID), \n    data = hsb, REML = FALSE)\n---\nModel selection table \n    (Int)    FEM    MIN   SES FEM:MIN FEM:SES MIN:SES FEM:MIN:SES df    logLik\n56  14.07 -1.211 -3.084 2.152          0.3740 -0.8009              8 -23184.72\n64  14.12 -1.300 -3.272 2.119  0.3651  0.4258 -0.7901              9 -23184.20\n40  14.06 -1.214 -3.075 2.343                 -0.7846              7 -23186.63\n128 14.12 -1.300 -3.271 2.119  0.3642  0.4273 -0.7875   -0.004688 10 -23184.20\n48  14.08 -1.255 -3.161 2.340  0.1675         -0.7785              8 -23186.51\n24  14.13 -1.228 -2.967 1.909          0.3458                      7 -23191.78\n32  14.18 -1.332 -3.191 1.874  0.4312  0.4074                      8 -23191.06\n8   14.11 -1.230 -2.962 2.091                                      6 -23193.42\n16  14.14 -1.289 -3.086 2.089  0.2410                              7 -23193.17\n39  13.42        -3.058 2.395                 -0.8286              6 -23214.30\n7   13.47        -2.938 2.129                                      5 -23221.82\n22  13.29 -1.185        2.197          0.3079                      6 -23293.61\n6   13.28 -1.188        2.358                                      5 -23294.87\n5   12.66               2.391                                      4 -23320.50\n4   14.38 -1.390 -3.702                                            5 -23377.51\n12  14.43 -1.483 -3.900        0.3860                              6 -23376.92\n3   13.65        -3.688                                            4 -23411.65\n2   13.35 -1.359                                                   4 -23526.66\n1   12.64                                                          3 -23557.90\n        AIC  delta weight\n56  46385.4   0.00  0.413\n64  46386.4   0.96  0.256\n40  46387.3   1.83  0.166\n128 46388.4   2.96  0.094\n48  46389.0   3.59  0.069\n24  46397.6  12.13  0.001\n32  46398.1  12.68  0.001\n8   46398.8  13.40  0.001\n16  46400.3  14.92  0.000\n39  46440.6  55.17  0.000\n7   46453.6  68.20  0.000\n22  46599.2 213.79  0.000\n6   46599.7 214.31  0.000\n5   46649.0 263.57  0.000\n4   46765.0 379.58  0.000\n12  46765.8 380.40  0.000\n3   46831.3 445.86  0.000\n2   47061.3 675.89  0.000\n1   47121.8 736.38  0.000\nModels ranked by AIC(x) \nRandom terms (all models): \n  1 | ID\n\n\nThe above commands rank all possible models by their AIC values in ascending orders, as shown in the column AIC. As you can see, all of the best models have the main effects of FEMALE, MINORITY, and SES, and the best model also has FEMALE \\(\\times\\) MINORITY and MINORITY \\(\\times\\) SES interactions.\nYou can use BIC for the same purpose (results not shown):\n\nmodel.sel(dd, rank = BIC)\n\nHowever, the best model according to BIC does not contain the FEMALE \\(\\times\\) MINORITY interaction.\n\nHow to Choose Between AIC and BIC?\nThis is one of the most debatable issue in the field of education. The (not so) short answer is that, although AIC and BIC may give different orderings of candidate models, the set of models with lowest AIC and BIC should be similar. Indeed, it is never a good practice to only select one model out of all models, especially when two or more models have very similar AIC/BIC values. Ultimately, AIC and BIC should be used to suggest a few “best” models, and the researcher is responsible to select the one that they feel more inline with theory/literature based on their subjective judgement.\nMore technically, AIC and BIC are based on different motivations, with AIC an index based on what is called Information Theory, which has a focus on predictive accuracy, and BIC an index derived as an approximation of the Bayes Factor, which is used to find the true model if it ever exists. Practically, AIC tends to select a model that maybe slightly more complex but has optimal predictive ability, whereas BIC tends to select a model that is more parsimonius but may sometimes be too simple. Therefore, if the goal is to have a model that can predict future samples well, AIC should be used; if the goal is to get a model as simple as possible, BIC should be used. For more technical discussion, please read Vrieze (2012).\n\n\nIncluding Lv-2 Predictors\nOne can also add the contextual effects or level-2 effects of all the level-1 predictors. For example, adding MEANSES will increase the number of possible models quite a bit. The following code will select a model with all main effects, the two-way interactions of SES and FEMALE, MINORITY, and MEANSES, the MEANSES \\(\\times\\) MINORITY interaction, and the MEANSES \\(\\times\\) MINORITY \\(\\times\\) SES three-way interaction. BIC, on the other hand, would select a much simpler model with only the four main effects and the MINORITY \\(\\times\\) SES interaction. You may verify the results yourself.\n\nm_4wayinter &lt;- lmer(MATHACH ~ MINORITY * FEMALE * SES * MEANSES + (1 | ID), \n                    data = hsb, REML = FALSE)\ndd &lt;- dredge(m_4wayinter)\nmodel.sel(dd, rank = AIC)\n\n\n\nWorkflow\nAs recommended in Hox, Moerbeek, and Van de Schoot (2018), with exploratory multilevel modeling, one proceeds with the following workflow:\n\nSelect level-1 predictors\nSelect level-1 random slopes\nSelect lv-2 effects of lv-1 predictors as well as level-2 predictors\nSelect cross-level interactions\n\n\nm_bic &lt;- lmer(MATHACH ~ MINORITY + FEMALE + SES + MINORITY * SES + (1 | ID), \n              data = hsb, REML = FALSE)\nm_bic_rs1 &lt;- update(m_bic, . ~ . - (1 | ID) + (MINORITY | ID))\nm_bic_rs2 &lt;- update(m_bic, . ~ . - (1 | ID) + (FEMALE | ID))\nm_bic_rs3 &lt;- update(m_bic, . ~ . - (1 | ID) + (SES | ID))\n# BIC suggests adding the random slope for MINORITY\nBIC(m_bic, m_bic_rs1, m_bic_rs2, m_bic_rs3)\n# Now add the MEANSES, SIZE, and SECTOR variable, as well as their interactions\n# Note: because of the scale of the SIZE variable is huge, I will divide the\n#   values by 1000 so that it is measured in thousand students\nhsb &lt;- mutate(hsb, SIZE1000 = SIZE / 1000)\nm_bic_rs1_lv2 &lt;- update(m_bic_rs1, . ~ . + MEANSES * SIZE1000 * SECTOR)\n\n\ndd &lt;- dredge(m_bic_rs1_lv2, \n             fixed = ~ MINORITY + FEMALE + SES + MINORITY * SES + \n               (MINORITY | ID))\nmodel.sel(dd, rank = BIC)\n\n\n# The best model will add MEANSES and SECTOR main effects\n# Finally, let's add the possible two-way cross-level interactions:\nm_bic_rs1_lv2_cross &lt;- update(m_bic_rs1, . ~ . + MEANSES * SIZE1000 * SECTOR + \n                                MEANSES * (MINORITY + FEMALE + SES) + \n                                SIZE1000 * (MINORITY + FEMALE + SES) + \n                                SES * (MINORITY + FEMALE + SES))\n\n\ndd &lt;- dredge(m_bic_rs1_lv2_cross, \n             fixed = ~ MINORITY + FEMALE + SES + MINORITY * SES + \n               (MINORITY | ID))\nmodel.sel(dd, rank = BIC)\n# Using BIC, none of the cross-level interactions should be included. A more \n# complex model will be selected using AIC"
  },
  {
    "objectID": "posts/2017-11-30-model-selection-for-multilevel-modeling/index.html#regularization",
    "href": "posts/2017-11-30-model-selection-for-multilevel-modeling/index.html#regularization",
    "title": "Model Selection for Multilevel Modeling",
    "section": "Regularization",
    "text": "Regularization\nIn many areas research, the goal of inference is more about getting good predictive performance, and less about finding a “true” model. For example, instead of identifying whether gender and SES are true determinant of math achievement, a school administrator may instead be more interested in a predictive equation to identify which students may perform the best based on their background information. In such cases, therefore, finding the “true” model is not as important as finding a model with good predictive performance.\nHowever, a more complex model does not guarantee good predictive performance. It may be at first strange to you why adding more predictors may actually give worse predictions. The reason is that the performance of a predictive equation depends on the parameter estimates (e.g., fixed effects, random effects), and with a more complex model, one needs a much larger sample to obtain good parameter estimates. That’s why if you have a small sample size, a linear model with one or two predictors may actually give you the most stable parameter estimates with maximized predictive performance, and would be much better than some of the advanced models and analytic techniques.\nHowever, a more advanced technique generally called regularization would allow you to “simplify” a more complex model by shrinking some of the coefficients to closer to zero, which is actually the same motivation of using multilevel models as opposed to including dummy group indicators. Two methods to do regularization in R is to use model averaging and Bayesian shrinkage priors. As this is somehow beyond the scope of this course, I simply show two examples below:\n\n# Start with a complex model without cross-level interactions\ndd_complex &lt;- dredge(m_bic_rs1_lv2, \n                     fixed = ~ MINORITY + FEMALE + SES + MINORITY * SES + \n                       (MINORITY | ID), beta = \"sd\")\n# Model Averaging\nmodel.avg(dd_complex)  # by default it used AICc\n\n\nCall:\nmodel.avg(object = dd_complex)\n\nComponent models: \n'1+2+3+4+5+6+7+8+9'       '1+2+3+4+5+6+7+8+9+10'    '1+2+3+4+5+6+7+8+9+10+11' \n'1+2+3+4+5+6+9'           '1+2+3+4+5+6+7+9'         '1+2+3+4+5+6+8+9'         \n'1+2+3+4+5+6+9+10'        '1+2+3+4+5+6+7+9+10'      '1+2+3+4+5+6+8+9+10'      \n'1+2+3+4+5+7+9'           '1+2+3+4+5+9'             '1+3+4+5+6+9'             \n'1+2+3+5+9'               '1+2+3+5+6+8+9'           '1+3+4+5+6+9+10'          \n'1+2+3+5+6+9'             '1+3+4+5+9'               '1+3+5+9'                 \n'1+3+5+6+9'              \n\nCoefficients: \n       (Intercept)   MEANSES   SECTOR   SIZE1000      FEMALE   MINORITY\nfull             0 0.2340741 0.116595 0.03515988 -0.09056013 -0.2040835\nsubset           0 0.2340741 0.116595 0.03546700 -0.09056013 -0.2040835\n            SES MEANSES:SECTOR MEANSES:SIZE1000 MINORITY:SES SECTOR:SIZE1000\nfull   0.247674    -0.06486368       -0.0757840  -0.06400748      0.02279661\nsubset 0.247674    -0.08518771       -0.1042316  -0.06400748      0.04641916\n       MEANSES:SECTOR:SIZE1000\nfull               0.006670189\nsubset             0.046988987\n\n\n\n# It is usually recommended to scale the variables to have SD = 1 when doing \n#   regularization. \nhsb_s &lt;- mutate_at(hsb, \n                   vars(MATHACH, MINORITY, FEMALE, SES, MEANSES, SIZE, SECTOR), \n                   funs(. / sd(.)))\nlibrary(rstanarm)  # Bayesian multilevel modeling\noptions(mc.cores = 2L)\nm_reg &lt;- stan_lmer(MATHACH ~ MINORITY + FEMALE + SES + MINORITY * SES + \n                     MEANSES * SIZE * SECTOR + \n                     (MINORITY + FEMALE + SES | ID), \n                   data = hsb_s, prior = hs(global_scale = 0.05), \n                   prior_covariance = decov(3), iter = 800L, \n                   adapt_delta = 0.99)\nprint(m_reg, digits = 2)\n\nstan_lmer\n family:       gaussian [identity]\n formula:      MATHACH ~ MINORITY + FEMALE + SES + MINORITY * SES + MEANSES * \n       SIZE * SECTOR + (MINORITY + FEMALE + SES | ID)\n observations: 7185\n------\n                    Median MAD_SD\n(Intercept)          1.93   0.06 \nMINORITY            -0.20   0.02 \nFEMALE              -0.09   0.01 \nSES                  0.24   0.02 \nMEANSES              0.23   0.07 \nSIZE                 0.01   0.02 \nSECTOR               0.07   0.05 \nMINORITY:SES        -0.04   0.01 \nMEANSES:SIZE        -0.04   0.02 \nMEANSES:SECTOR      -0.04   0.04 \nSIZE:SECTOR          0.04   0.03 \nMEANSES:SIZE:SECTOR  0.00   0.02 \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 0.86   0.01  \n\nError terms:\n Groups   Name        Std.Dev. Corr             \n ID       (Intercept) 0.2060                    \n          MINORITY    0.0954   -0.09            \n          FEMALE      0.0613   -0.69  0.09      \n          SES         0.0530    0.08 -0.22 -0.09\n Residual             0.8651                    \nNum. levels: ID 160 \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nYou can see that the coefficients were quite similar using both methods. If you are to run a model using the regular MLM using lme4 or other software, you should see that the coefficients are less closer to zero in regular MLM than the ones reported here with regularization."
  },
  {
    "objectID": "posts/2020-12-16-piping-with-magrittr/index.html",
    "href": "posts/2020-12-16-piping-with-magrittr/index.html",
    "title": "Piping with magrittr",
    "section": "",
    "text": "I have just spent a semester teaching multilevel modeling, and in the R codes I provided, I usually use the pipe operator (%&gt;%). For example, to compute the cluster means, we can do\n\nlibrary(tidyverse)\ndata(\"Hsb82\", package = \"mlmRev\")\nHsb82 &lt;- Hsb82 %&gt;% \n  group_by(school) %&gt;% \n  mutate(ses_cm = mean(ses)) %&gt;% \n  ungroup()\n\nHowever, it’s kind of embarassing that I only recently found out the assignment pipe (%&lt;&gt;%) operator, as discussed here. For example,\n\nlibrary(magrittr)\nset.seed(123)\nx &lt;- rnorm(10)\nmean(x)\n\n[1] 0.07462564\n\n# Add 1 to x\nx %&lt;&gt;% magrittr::add(1)\nmean(x)\n\n[1] 1.074626\n\n# The above is equivalent to \n# x &lt;- x + 1\n\nFor the cluster mean example, we can do\n\nHsb82 %&lt;&gt;% \n  group_by(school) %&gt;% \n  mutate(ses_cm2 = mean(ses)) %&gt;% \n  ungroup()\nselect(Hsb82, ses_cm, ses_cm2)\n\n# A tibble: 7,185 × 2\n   ses_cm ses_cm2\n    &lt;dbl&gt;   &lt;dbl&gt;\n 1 -0.434  -0.434\n 2 -0.434  -0.434\n 3 -0.434  -0.434\n 4 -0.434  -0.434\n 5 -0.434  -0.434\n 6 -0.434  -0.434\n 7 -0.434  -0.434\n 8 -0.434  -0.434\n 9 -0.434  -0.434\n10 -0.434  -0.434\n# ℹ 7,175 more rows\n\n\nwhich saves the additional typing of Hsb82 &lt;- Hsb82 %&gt;%. That said, the %&lt;&gt;% is not commonly seen when reading other people’s code, so perhaps the R community still prefer just using the %&gt;% operator. But it’s at least good to know there is a potentially more convenient way. There is also the %$% and %T&gt;% operator, as discussed in this vignette."
  },
  {
    "objectID": "posts/2022-11-13-multilevel-composite-reliability/index.html",
    "href": "posts/2022-11-13-multilevel-composite-reliability/index.html",
    "title": "Multilevel Composite Reliability",
    "section": "",
    "text": "This is a short blog post on computing multilevel reliability based on Lai (2021)."
  },
  {
    "objectID": "posts/2022-11-13-multilevel-composite-reliability/index.html#load-packages",
    "href": "posts/2022-11-13-multilevel-composite-reliability/index.html#load-packages",
    "title": "Multilevel Composite Reliability",
    "section": "Load Packages",
    "text": "Load Packages\n\nlibrary(lavaan)\n\nThis is lavaan 0.6-13\nlavaan is FREE software! Please report any bugs.\n\nlibrary(semTools)\n\n \n\n\n###############################################################################\n\n\nThis is semTools 0.5-6\n\n\nAll users of R (or SEM) are invited to submit functions or ideas for functions.\n\n\n###############################################################################\n\n\nFirst, use demo data from lavaan\n\ndata(\"Demo.twolevel\", package = \"lavaan\")\n\nThen source a script I wrote here implementing the procedures in the paper, which defines a function multilevel_alpha():\n\nsource(\"https://github.com/marklhc/mcfa_reliability_supp/raw/master/multilevel_alpha.R\")\n\nExample syntax in R and Mplus for the paper can be found in https://github.com/marklhc/mcfa_reliability_supp. Going back to the multilevel demo data, we can consider the reliability for between-level and within-level composite scores when summing y1, y2, and y3. This can be done using\n\nmultilevel_alpha(Demo.twolevel[c(\"y1\", \"y2\", \"y3\")],\n                 id = Demo.twolevel$cluster)\n\nLoading required package: psych\n\n\n\nAttaching package: 'psych'\n\n\nThe following objects are masked from 'package:semTools':\n\n    reliability, skew\n\n\nThe following object is masked from 'package:lavaan':\n\n    cor2cov\n\n\nParallel analysis suggests that the number of factors =  NA  and the number of components =  1 \nParallel analysis suggests that the number of factors =  NA  and the number of components =  1 \n\n\n$alpha\n  alpha2l    alphab    alphaw \n0.7523324 0.7613526 0.6682291 \n\n$alpha_ci\n             2.5%     97.5%\nalpha2l 0.7279245 0.7732465\nalphab  0.7132990 0.7963489\nalphaw  0.6420676 0.6929901\n\n$omega\n  omega2l    omegab    omegaw \n0.7669377 0.7837490 0.6761273 \n\n$omega_ci\n             2.5%     97.5%\nomega2l 0.7425036 0.7866461\nomegab  0.7393715 0.8173138\nomegaw  0.6482381 0.6997885\n\n$ncomp\n within between \n      1       1 \n\n\nNote that the \\(\\omega\\) indices assume unidimensionality, local independence, and cross-level invariance. There were a total of six indices: three \\(\\alpha\\)s and three \\(\\omega\\)s, corresponding to raw composite (“2l”), between-level composite (“b”), and within-level composite (“w”). The 95% CIs were obtained using the Monte Carlo method by simulating from the asymptotic distribution of the model parameters. In addition, the function also implemented a parallel analysis procedure for determining the number of factors/components at each level, as discussed in this paper."
  },
  {
    "objectID": "posts/2022-11-13-multilevel-composite-reliability/index.html#using-semtoolscomprelsem",
    "href": "posts/2022-11-13-multilevel-composite-reliability/index.html#using-semtoolscomprelsem",
    "title": "Multilevel Composite Reliability",
    "section": "Using semTools::compRelSEM()",
    "text": "Using semTools::compRelSEM()\nSome of the above procedures were implemented in the semTools package. To use that, one needs to fit a two-level CFA in lavaan first (see ?compRelSEM:\n\nmcfa_mod &lt;- '\n  level: 1\n    f =~ y1 + L2*y2 + L3*y3\n  level: 2\n    f =~ y1 + L2*y2 + L3*y3\n'\nmcfa_fit &lt;- cfa(mcfa_mod, data = Demo.twolevel, cluster = \"cluster\")\n\nThen call the function\n\ncompRelSEM(mcfa_fit, config = c(\"f\"), shared = c(\"f\"))\n\n$config\n$config$f\n  omega_W  omega_2L \n0.6732269 0.7705154 \n\n\n$shared\n$shared$f\n  omega_B       IRR \n0.7972928 0.8361785"
  },
  {
    "objectID": "posts/2022-05-19-estimating-a-2-pl-model-in-julia-part-1/index.html",
    "href": "posts/2022-05-19-estimating-a-2-pl-model-in-julia-part-1/index.html",
    "title": "Estimating a 2-PL Model in Julia (Part 1)",
    "section": "",
    "text": "The semester is finally over, and for some reason, I wanted to consolidate my understanding of some psychometric models, perhaps because I’ll be teaching a graduate measurement class soon. I actually spent quite some time learning about the estimation of models from item response theory (IRT), which makes me better appreciate the training of the psychometricians from the IRT tradition.\nThis and the next blog posts focus on the estimation of a basic and commonly used IRT model: the two-parameter logistic (2-PL) model for binary responses, although the idea should generalize to similar IRT models as well. This post will be on the marginal likelihood estimation approach, which I believe is used in the R package ltm, and the next post (planned) will be on estimation with the expectation-maximization (EM) algorithm.\nThese two posts are mostly learning notes for myself and are based on the following great articles:\nAnd also, this post is my first complete post using Quarto!"
  },
  {
    "objectID": "posts/2022-05-19-estimating-a-2-pl-model-in-julia-part-1/index.html#import-data",
    "href": "posts/2022-05-19-estimating-a-2-pl-model-in-julia-part-1/index.html#import-data",
    "title": "Estimating a 2-PL Model in Julia (Part 1)",
    "section": "Import Data",
    "text": "Import Data\nlibrary(dplyr, include.only = \"count\")\nlibrary(ltm)\ncount(LSAT, `Item 1`, `Item 2`, `Item 3`, `Item 4`, `Item 5`)\n   Item 1 Item 2 Item 3 Item 4 Item 5   n\n1       0      0      0      0      0   3\n2       0      0      0      0      1   6\n3       0      0      0      1      0   2\n4       0      0      0      1      1  11\n5       0      0      1      0      0   1\n6       0      0      1      0      1   1\n7       0      0      1      1      0   3\n8       0      0      1      1      1   4\n9       0      1      0      0      0   1\n10      0      1      0      0      1   8\n11      0      1      0      1      1  16\n12      0      1      1      0      1   3\n13      0      1      1      1      0   2\n14      0      1      1      1      1  15\n15      1      0      0      0      0  10\n16      1      0      0      0      1  29\n17      1      0      0      1      0  14\n18      1      0      0      1      1  81\n19      1      0      1      0      0   3\n20      1      0      1      0      1  28\n21      1      0      1      1      0  15\n22      1      0      1      1      1  80\n23      1      1      0      0      0  16\n24      1      1      0      0      1  56\n25      1      1      0      1      0  21\n26      1      1      0      1      1 173\n27      1      1      1      0      0  11\n28      1      1      1      0      1  61\n29      1      1      1      1      0  28\n30      1      1      1      1      1 298"
  },
  {
    "objectID": "posts/2022-05-19-estimating-a-2-pl-model-in-julia-part-1/index.html#two-parameter-logistic-model",
    "href": "posts/2022-05-19-estimating-a-2-pl-model-in-julia-part-1/index.html#two-parameter-logistic-model",
    "title": "Estimating a 2-PL Model in Julia (Part 1)",
    "section": "Two-Parameter Logistic Model",
    "text": "Two-Parameter Logistic Model\n\\[P_{ij} = P(y_j | \\theta_i) = \\frac{\\exp(\\eta_{ij})}{1 + \\exp(\\eta_{ij})}\\]\nwhere\n\\[\\eta_{ij} = a_j (\\theta_i - b_j) = a_j \\theta_i + d_j\\]\n\nEstimation with mirt in R\nlibrary(mirt)\nm_2pl &lt;- mirt(LSAT, SE = TRUE)\nIteration: 1, Log-Lik: -2468.601, Max-Change: 0.08059\nIteration: 2, Log-Lik: -2467.278, Max-Change: 0.03446\nIteration: 3, Log-Lik: -2466.956, Max-Change: 0.02323\nIteration: 4, Log-Lik: -2466.797, Max-Change: 0.01444\nIteration: 5, Log-Lik: -2466.749, Max-Change: 0.01067\nIteration: 6, Log-Lik: -2466.721, Max-Change: 0.00781\nIteration: 7, Log-Lik: -2466.683, Max-Change: 0.00426\nIteration: 8, Log-Lik: -2466.677, Max-Change: 0.00392\nIteration: 9, Log-Lik: -2466.673, Max-Change: 0.00361\nIteration: 10, Log-Lik: -2466.657, Max-Change: 0.00235\nIteration: 11, Log-Lik: -2466.656, Max-Change: 0.00207\nIteration: 12, Log-Lik: -2466.655, Max-Change: 0.00176\nIteration: 13, Log-Lik: -2466.654, Max-Change: 0.00039\nIteration: 14, Log-Lik: -2466.654, Max-Change: 0.00026\nIteration: 15, Log-Lik: -2466.653, Max-Change: 0.00025\nIteration: 16, Log-Lik: -2466.653, Max-Change: 0.00021\nIteration: 17, Log-Lik: -2466.653, Max-Change: 0.00020\nIteration: 18, Log-Lik: -2466.653, Max-Change: 0.00018\nIteration: 19, Log-Lik: -2466.653, Max-Change: 0.00016\nIteration: 20, Log-Lik: -2466.653, Max-Change: 0.00013\nIteration: 21, Log-Lik: -2466.653, Max-Change: 0.00013\nIteration: 22, Log-Lik: -2466.653, Max-Change: 0.00010\n\nCalculating information matrix...\nm_2pl\nCall:\nmirt(data = LSAT, SE = TRUE)\n\nFull-information item factor analysis with 1 factor(s).\nConverged within 1e-04 tolerance after 22 EM iterations.\nmirt version: 1.36.1 \nM-step optimizer: BFGS \nEM acceleration: Ramsay \nNumber of rectangular quadrature: 61\nLatent density type: Gaussian \n\nInformation matrix estimated with method: Oakes\nSecond-order test: model is a possible local maximum\nCondition number of information matrix =  21.24458\n\nLog-likelihood = -2466.653\nEstimated parameters: 10 \nAIC = 4953.307\nBIC = 5002.384; SABIC = 4970.624\nG2 (21) = 21.23, p = 0.445\nRMSEA = 0.003, CFI = NaN, TLI = NaN\ncoef(m_2pl, printSE = TRUE,\n     as.data.frame = TRUE)[c(0:4 * 4 + 1,\n                             0:4 * 4 + 2), ]\n                par         SE\nItem.1.a1 0.8250552 0.25802573\nItem.2.a1 0.7230608 0.18674963\nItem.3.a1 0.8899989 0.23243973\nItem.4.a1 0.6886588 0.18521007\nItem.5.a1 0.6575904 0.21001626\nItem.1.d  2.7728009 0.20561517\nItem.2.d  0.9902689 0.09004037\nItem.3.d  0.2491043 0.07624768\nItem.4.d  1.2848516 0.09906477\nItem.5.d  2.0536381 0.13545920"
  },
  {
    "objectID": "posts/2022-05-19-estimating-a-2-pl-model-in-julia-part-1/index.html#marginal-maximum-likelihood-mml-estimation",
    "href": "posts/2022-05-19-estimating-a-2-pl-model-in-julia-part-1/index.html#marginal-maximum-likelihood-mml-estimation",
    "title": "Estimating a 2-PL Model in Julia (Part 1)",
    "section": "Marginal Maximum Likelihood (MML) Estimation",
    "text": "Marginal Maximum Likelihood (MML) Estimation\nThe main challenge of estimation in IRT, as in other latent variable models, is that there are both person (i.e., \\(\\theta\\)) and item parameters (i.e., \\(a\\) and \\(d\\) in the 2-PL model). This is the same in the common factor model with continuous responses, but there we usually marginalize out the person parameters. The idea is the same here with MML, except it’s more difficult because while we can exploit the normality with the factor model so that the marginal distribution of the data is multivariate normal, in IRT, we cannot get rid of the integral when marginalization. Let’s start with the likelihood function.\nLikelihood function for a response\n\\[L(a_j, d_j; y_{ij}, \\theta_i) = P(y_{ij} \\mid \\theta_i, a_j, d_j) = P_{ij}^{y_{ij}} (1 - P_{ij})^{1 - y_{ij}}\\]\nLog-likelihood function\n\\[\\ell(a_j, d_j; y_{ij}, \\theta_i) = y_{ij} \\log \\eta_{ij} - \\log[1 + \\exp(\\eta_{ij})]\\]\nIndividual conditional likelihood\n\\[\\ell(\\mathbf{a}, \\mathbf{d}; \\mathbf{y_i}, \\theta_i) = \\sum_j \\ell(a_j, d_j; y_{ij}, \\theta_i)\\]\nIndividual integrated loglikelihood\n\\[\n\\begin{aligned}\n\\ell_i & = \\ell(\\mathbf{a}, \\mathbf{d}; \\mathbf{y_i}) = \\log L(\\mathbf{a}, \\mathbf{d}; \\mathbf{y_i}) \\\\\n       & = \\log \\int L(\\mathbf{a}, \\mathbf{d}; \\mathbf{y_i}, \\theta) f(\\theta) d \\theta \\\\\n       & = \\log \\int \\exp [\\ell(\\mathbf{a}, \\mathbf{d}; \\mathbf{y_i}, \\theta)] f(\\theta) d \\theta\n\\end{aligned}\n\\]\nMarginal loglikelihood\n\\[\\ell(\\mathbf{a}, \\mathbf{d}; \\mathbf{y}, \\theta) = \\sum_i \\ell_i = \\sum_i \\log \\int \\exp [\\ell(\\mathbf{a}, \\mathbf{d}; \\mathbf{y_i}, \\theta)] f(\\theta) d \\theta\\]\n\nQuadrature\nWith the assumption that \\(\\theta \\sim N(0, 1)\\), we have\n\\[\\ell_i = \\log \\int \\exp[\\ell(\\mathbf{a}, \\mathbf{d}; \\mathbf{y_i}, \\theta)] \\frac{\\exp(-\\theta^2 / 2)}{\\sqrt{2 \\pi}} d \\theta,\\]\nwhich is not easy to evaluate, but can be approximated using Gaussian-Hermite (GH) quadrature. The GH quadrature is used to approximate an integral of the form\n\\[\\int_{-\\infty}^\\infty g(x) \\exp(-x^2) dx\\]\nwith\n\\[\\sum_{k = 1}^q w_k g(x_k),\\]\nwhere \\(q\\) is the number of quadrature points. For example, we can approximate \\(\\int x^2 \\exp(-x^2) dx\\) over the real line as\nusing FastGaussQuadrature: gausshermite\ngh5 = gausshermite(5)  # 5 quadrature points\n([-2.0201828704560856, -0.9585724646138196, -8.881784197001252e-16, 0.9585724646138196, 2.0201828704560856], [0.019953242059045872, 0.39361932315224074, 0.9453087204829428, 0.39361932315224074, 0.019953242059045872])\n# First element is the nodes\ngh5[1]\n5-element Vector{Float64}:\n -2.0201828704560856\n -0.9585724646138196\n -8.881784197001252e-16\n  0.9585724646138196\n  2.0201828704560856\n# Second element is the weights\ngh5[2]\n5-element Vector{Float64}:\n 0.019953242059045872\n 0.39361932315224074\n 0.9453087204829428\n 0.39361932315224074\n 0.019953242059045872\n# Approximate integral\nusing LinearAlgebra\ndot(gh5[2], gh5[1] .^ 2)\n0.8862269254527585\nWhen we have the standard normal density, the integral has the form\n\\[\\int_{-\\infty}^\\infty g(t) \\frac{\\exp(-t^2 / 2)}{\\sqrt{2\\pi}} dt,\\]\nand to use GH quadrature, we need to use a change of variable \\(x = t / \\sqrt{2}\\) so that\n\\[\n  \\begin{aligned}\n  \\int_{-\\infty}^\\infty g(t) \\frac{\\exp(-t^2 / 2)}{\\sqrt{2\\pi}} dt & = \\int_{-\\infty}^\\infty g(\\sqrt{2} x) \\frac{\\exp(-x^2)}{\\sqrt{2\\pi}} \\sqrt{2} dx \\\\\n  & \\approx \\sum_{k} \\frac{w_k}{\\sqrt{ \\pi}} g(\\sqrt{2} x).\n  \\end{aligned}\n\\]\nBecause we have \\({w_k}{\\sqrt{\\pi}}\\) before the likelihood term, we need to divide the qudrature weights by \\(\\sqrt{\\pi}\\). Similarly, because the likelihood is defined in terms of \\(\\sqrt{2} x_k\\), we need to multiply the quadrature nodes by \\(\\sqrt{2}\\). For example, to approximate \\(\\int (x^3 + 2 x^2) \\phi(x) dx\\), where \\(\\phi(\\cdot)\\) is the normal density, we need\nnew_nodes = gh5[1] .* √2\n5-element Vector{Float64}:\n -2.8569700138728056\n -1.3556261799742675\n -1.2560739669470201e-15\n  1.3556261799742675\n  2.8569700138728056\nnew_weights = gh5[2] ./ √π\n5-element Vector{Float64}:\n 0.011257411327720667\n 0.22207592200561244\n 0.5333333333333339\n 0.22207592200561244\n 0.011257411327720667\ndot(new_weights, new_nodes .^ 3 + 2 .* (new_nodes .^ 2))\n2.0000000000000018\nBack to the 2-PL model. Using a change of variable \\(x = \\theta / sqrt{2}\\), we can approximate the marginal loglikelihood\n\\[\n  \\begin{aligned}\n  \\ell(\\mathbf{a}, \\mathbf{d}; \\mathbf{y}, \\theta) & = \\sum_i \\log \\int \\exp [\\ell(\\mathbf{a}, \\mathbf{d}; \\mathbf{y_i}, \\theta)] \\frac{\\exp(-\\theta^2 / 2)}{\\sqrt{2 \\pi}} d \\theta \\\\\n          & \\approx \\sum_i \\log \\sum_k \\frac{w_k}{\\sqrt{\\pi}} \\exp [\\ell(\\mathbf{a}, \\mathbf{d}; \\mathbf{y_i}, \\sqrt{2} x_k)]\n  \\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/2022-05-19-estimating-a-2-pl-model-in-julia-part-1/index.html#implement-mml-estimation-in-julia",
    "href": "posts/2022-05-19-estimating-a-2-pl-model-in-julia-part-1/index.html#implement-mml-estimation-in-julia",
    "title": "Estimating a 2-PL Model in Julia (Part 1)",
    "section": "Implement MML Estimation in Julia",
    "text": "Implement MML Estimation in Julia\n\nImport R data\nusing RCall\nlsat = rcopy(R\"mirt::LSAT6\")\n30×6 DataFrame\n Row │ Item_1  Item_2  Item_3  Item_4  Item_5  Freq\n     │ Int64   Int64   Int64   Int64   Int64   Int64\n─────┼───────────────────────────────────────────────\n   1 │      0       0       0       0       0      3\n   2 │      0       0       0       0       1      6\n   3 │      0       0       0       1       0      2\n   4 │      0       0       0       1       1     11\n   5 │      0       0       1       0       0      1\n   6 │      0       0       1       0       1      1\n   7 │      0       0       1       1       0      3\n   8 │      0       0       1       1       1      4\n  ⋮  │   ⋮       ⋮       ⋮       ⋮       ⋮       ⋮\n  24 │      1       1       0       0       1     56\n  25 │      1       1       0       1       0     21\n  26 │      1       1       0       1       1    173\n  27 │      1       1       1       0       0     11\n  28 │      1       1       1       0       1     61\n  29 │      1       1       1       1       0     28\n  30 │      1       1       1       1       1    298\n                                      15 rows omitted\n\n\nCompute marginal loglikelihood\nusing LinearAlgebra, LogExpFunctions\n# Helper for computing logits: ηᵢⱼ = aⱼθ + dⱼ\nfunction compute_logits(θ, a, d)\n    [θ[i] * a[j] + d[j]\n     for i = eachindex(θ), j = eachindex(a)]\nend\ncompute_logits (generic function with 1 method)\n# Main function for computing marginal loglik\nfunction marginal_loglik_2pl(par, y, n, θ, logw)\n    num_items = size(y, 2)\n    a = par[1:num_items]\n    d = par[num_items+1:end]\n    η = compute_logits(θ, a, d)\n    sum1pexpη = sum(log1pexp, η, dims=2)\n    ret = zero(eltype(par))\n    for l in eachindex(n)\n        @inbounds ret += n[l] * logsumexp(logw .+ η * view(y, l, :) .- sum1pexpη)\n    end\n    ret\nend\nmarginal_loglik_2pl (generic function with 1 method)\nExample\ngh15 = gausshermite(15)  # 15 quadrature points\n([-4.499990707309391, -3.669950373404453, -2.9671669279056054, -2.3257324861738606, -1.7199925751864926, -1.136115585210924, -0.5650695832555779, -3.552713678800501e-15, 0.5650695832555779, 1.136115585210924, 1.7199925751864926, 2.3257324861738606, 2.9671669279056054, 3.669950373404453, 4.499990707309391], [1.5224758042535368e-9, 1.0591155477110773e-6, 0.00010000444123250024, 0.0027780688429127607, 0.030780033872546228, 0.15848891579593563, 0.41202868749889865, 0.5641003087264175, 0.41202868749889865, 0.15848891579593563, 0.030780033872546228, 0.0027780688429127607, 0.00010000444123250024, 1.0591155477110773e-6, 1.5224758042535368e-9])\ngh15_node = gh15[1] .* √2\n15-element Vector{Float64}:\n -6.363947888829838\n -5.190093591304782\n -4.196207711269019\n -3.289082424398771\n -2.4324368270097634\n -1.6067100690287344\n -0.7991290683245511\n -5.0242958677880805e-15\n  0.7991290683245511\n  1.6067100690287344\n  2.4324368270097634\n  3.289082424398771\n  4.196207711269019\n  5.190093591304782\n  6.363947888829838\ngh15_weight = gh15[2] ./ √π\n15-element Vector{Float64}:\n 8.589649899633383e-10\n 5.975419597920666e-7\n 5.642146405189039e-5\n 0.0015673575035499477\n 0.01736577449213769\n 0.08941779539984435\n 0.23246229360973225\n 0.31825951825951826\n 0.23246229360973225\n 0.08941779539984435\n 0.01736577449213769\n 0.0015673575035499477\n 5.642146405189039e-5\n 5.975419597920666e-7\n 8.589649899633383e-10\nθ₀ = [ones(5); zeros(5)]  # initial values [a₁, …, d₁, … ]\n10-element Vector{Float64}:\n 1.0\n 1.0\n 1.0\n 1.0\n 1.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n# Note:\n#   1. requires matrix input for y\n#   2. uses log(w) instead of w as input\nmarginal_loglik_2pl(θ₀,\n    Matrix(lsat[:, 1:5]), lsat[:, 6],\n    gh15_node, log.(gh15_weight))\n-3182.386241129594\n\n\nOptimization\nWe can now use generic optimization algorithms to search for parameter values that maximize the marginal loglikelihood function, such as those in Optim.jl. Just note that the optimize function does minimization, so we need to switch the sign.\nusing Optim\n# Use anonymous function to pass additional arguments\nopt1 = optimize(x -&gt; -marginal_loglik_2pl(x,\n        Matrix(lsat[:, 1:5]), lsat[:, 6],\n        gh15_node, log.(gh15_weight)),\n    θ₀, LBFGS())\n * Status: success\n\n * Candidate solution\n    Final objective value:     2.466653e+03\n\n * Found with\n    Algorithm:     L-BFGS\n\n * Convergence measures\n    |x - x'|               = 0.00e+00 ≤ 0.0e+00\n    |x - x'|/|x'|          = 0.00e+00 ≤ 0.0e+00\n    |f(x) - f(x')|         = 0.00e+00 ≤ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = 0.00e+00 ≤ 0.0e+00\n    |g(x)|                 = 1.13e-07 ≰ 1.0e-08\n\n * Work counters\n    Seconds run:   1  (vs limit Inf)\n    Iterations:    27\n    f(x) calls:    79\n    ∇f(x) calls:   79\nopt1.minimizer\n10-element Vector{Float64}:\n 0.8256595178534981\n 0.7227442534360561\n 0.8908743624210756\n 0.6883680043418308\n 0.6568559896469349\n 2.7732343965273114\n 0.9902012139683823\n 0.24914751010804564\n 1.2847569367785143\n 2.053270467746726\nLooks like we got similar estimates as in mirt in R!\n\n\nBenchmarking\nThe previous call of optimize uses the L-BFGS algorithm with the gradients obtained by finite differences. One nice thing in Julia is that we can use automatic differentiation. Let’s apply that. I’ll also add the standard errors from the Hessian, and also do some benchmarking.\nusing NLSolversBase\n# Wrap in a function\nfunction estimate_2pl_mml(y, n, init, n_quadpts=101)\n    # Convert y to matrix\n    y = Matrix(y)\n    # Obtain quadrature nodes and weights\n    ghq = gausshermite(n_quadpts)\n    ghq_θ = ghq[1] .* √2\n    ghq_logw = log.(ghq[2]) .- log(π) / 2\n    # Use `TwiceDifferentiable` to get Hessian\n    func = TwiceDifferentiable(\n        x -&gt; -marginal_loglik_2pl(x,\n            Matrix(lsat[:, 1:5]), lsat[:, 6],\n            ghq_θ, ghq_logw),\n        init; autodiff=:forward)\n    opt = optimize(func, init, LBFGS())\n    est = opt.minimizer\n    # Get standard error by inverting hessian\n    numerical_hessian = hessian!(func, est)\n    # Return results\n    hcat(est,\n         sqrt.(diag(inv(numerical_hessian))))\nend\nestimate_2pl_mml (generic function with 2 methods)\nWe can benchmark the implementation:\nusing BenchmarkTools\n@btime estimate_2pl_mml(lsat[:, 1:5], lsat[:, 6], θ₀)\n  25.740 ms (7931 allocations: 46.99 MiB)\n\n10×2 Matrix{Float64}:\n 0.82566   0.258115\n 0.722744  0.18668\n 0.890875  0.232764\n 0.688368  0.185143\n 0.656856  0.209909\n 2.77323   0.205744\n 0.990201  0.090019\n 0.249148  0.0762729\n 1.28476   0.0990375\n 2.05327   0.135359\nCompare to ltm and mirt in R:\nbench::mark(\n    mirt = mirt(LSAT, SE = TRUE,\n                verbose = FALSE, quadpts = 101),\n    ltm = ltm(LSAT ~ z1, control = list(GHk = 101)),\n    check = FALSE\n)\nWarning: Some expressions had a GC in every iteration; so filtering is disabled.\n\n# A tibble: 2 × 6\n  expression      min   median `itr/sec` mem_alloc `gc/sec`\n  &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n1 mirt        102.6ms    111ms      9.10    15.4MB     5.46\n2 ltm          68.1ms     74ms     13.1     53.5MB    33.6 \nSo looks like the Julia implementation is pretty fast. Of course, keep in mind that the functions in the R packages are written more generally for other types of IRT models, and that those functions may compute a few more things than just the estimates and the standard errors.\nP.S., I also tried to code the analytic gradient for the loglikelihood function with respect to the parameters, which took me quite some time, especially for getting the quadrature to work properly. However, supplying the analytic gradient seems to perform worse than automatic differentiation (AD), probably because my code is not optimized, but also perhaps AD is pretty good already. Perhaps the lesson is that with Julia and AD, I can gratefully skip the part of deriving and coding the gradients."
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "An R package for bootstrap resampling for multilevel models."
  },
  {
    "objectID": "software.html#bootmlm",
    "href": "software.html#bootmlm",
    "title": "Software",
    "section": "",
    "text": "An R package for bootstrap resampling for multilevel models."
  },
  {
    "objectID": "software.html#pinsearch",
    "href": "software.html#pinsearch",
    "title": "Software",
    "section": "pinsearch",
    "text": "pinsearch\nAn R package to automate the process of performing specification search in identifying noninvariant items."
  },
  {
    "objectID": "software.html#r2spa",
    "href": "software.html#r2spa",
    "title": "Software",
    "section": "R2spa",
    "text": "R2spa\nWith Winnie Wing-Yee Tse, Yixiao Li, & Gengrui Zhang\nA free and open-source R package that performs two-stage path analysis (2S-PA)."
  },
  {
    "objectID": "software.html#semptools",
    "href": "software.html#semptools",
    "title": "Software",
    "section": "semptools",
    "text": "semptools\nBy Shu Fai Cheung\nHelper functions for modifying (postprocessing) plots generated by the semPlot package."
  },
  {
    "objectID": "software.html#semfindr",
    "href": "software.html#semfindr",
    "title": "Software",
    "section": "semfindr",
    "text": "semfindr\nBy Shu Fai Cheung\nA find(e)r of influential cases in structural equation modeling based mainly on the sensitivity analysis procedures presented by Pek and MacCallum (2011)."
  },
  {
    "objectID": "software.html#unbiasr",
    "href": "software.html#unbiasr",
    "title": "Software",
    "section": "unbiasr",
    "text": "unbiasr\nWith Clay Cantrell, Yichi Zhang and Meltem Ozcan\nAn R package for implementing the classification accuracy analysis procedure for partial invariance.\n\nShiny application"
  },
  {
    "objectID": "presentation.html",
    "href": "presentation.html",
    "title": "Presentations",
    "section": "",
    "text": "Correcting for unreliability and partial invariance: A two-stage path analysis approach. 8th Texas Universities’ Educational Statistics and Psychometrics (TUESAP) Meeting, Texas A&M University, TX, USA (Zoom)."
  },
  {
    "objectID": "presentation.html#section",
    "href": "presentation.html#section",
    "title": "Presentations",
    "section": "",
    "text": "Correcting for unreliability and partial invariance: A two-stage path analysis approach. 8th Texas Universities’ Educational Statistics and Psychometrics (TUESAP) Meeting, Texas A&M University, TX, USA (Zoom)."
  },
  {
    "objectID": "presentation.html#section-1",
    "href": "presentation.html#section-1",
    "title": "Presentations",
    "section": "2021",
    "text": "2021\n\nWhat to Do If Measurement Invariance Does Not Hold? Let’s Look at the Practical Significance. University of Missouri, MO, USA (Zoom).\nObtaining reliability for daily diary data using multilevel factor analysis. 2021 IMPS Virtual Conference.\n[Code] [Video]\nReliability of Composite Scores With Multilevel Data. Fordham University, NY, USA (Zoom).\nA Multilevel Weighted Bootstrap Procedure to Handle Sampling Weights. 2021 AERA Virtual Meeting."
  },
  {
    "objectID": "presentation.html#section-2",
    "href": "presentation.html#section-2",
    "title": "Presentations",
    "section": "2020",
    "text": "2020\n\nInternal Consistency of Multilevel Data: Cluster Means, Centering, and Construct Meanings. Arizona State University, AZ, USA (Zoom).\nMultilevel Bootstrap Confidence Intervals for Standardized Effect Size. 2020 IMPS Virtual Conference."
  },
  {
    "objectID": "presentation.html#section-3",
    "href": "presentation.html#section-3",
    "title": "Presentations",
    "section": "2019",
    "text": "2019\n\nAdvancing Quantitative Science with Monte Carlo Simulation. Texas A&M University, College Station, TX.\n[GitHub]"
  },
  {
    "objectID": "presentation.html#section-4",
    "href": "presentation.html#section-4",
    "title": "Presentations",
    "section": "2018",
    "text": "2018\n\nMultilevel Modeling: Introduction and Recent Advances for Behavioral Research. University of Macau, Macau SAR.\nIntroduction to Multilevel Modeling. Guest lecture for PSYC 503 at the University of Southern California, CA, USA."
  },
  {
    "objectID": "presentation.html#section-5",
    "href": "presentation.html#section-5",
    "title": "Presentations",
    "section": "2017",
    "text": "2017\n\nBayesian Data Analysis Workshop. Texas A&M University, TX, USA."
  }
]
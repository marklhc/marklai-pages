---
title: "Longitudinal Data Analysis in R (Part 1)"
output:
  html_document:
    df_print: paged
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = ">#")
```

The data come from Fisher & Boswell (2016) in a study of 10 participants diagnosed with clinically generalized anxiety disorder. The paper can be found here:
https://journals.sagepub.com/doi/10.1177/1073191116638735. The data is obtained from http://www.dynamicpsychlab.com/data

## Load Packages and Import Data

```{r load-pkg, message=FALSE}
# To install a package, run the following ONCE (and only once on your computer)
# install.packages("psych")  
library(here)  # makes reading data more consistent
library(tidyverse)  # for data manipulation and plotting
library(haven)  # for importing SPSS/SAS/Stata data
library(brms)  # for Bayesian multilevel analysis
library(lattice)  # for dotplot (working with lme4)
library(sjPlot)  # for plotting effects
library(broom.mixed)  # for summarizing results
library(interactions)  # for plotting interactions
library(modelsummary)  # for making tables
# Add the following so that the LOO will be included in the msummary table
msummary_mixed <- function(models, coef_map = NULL, add_rows = NULL, ...) {
  if (is.null(coef_map)) {
    if (!"list" %in% class(models)) {
      models <- list(models)
    }
    for (model in models) {
      coef_map <- union(coef_map, tidy(model)$term)
    }
  }
  ranef_index <- grep("^(sd|cor)__", x = coef_map)
  coef_map <- c(coef_map[-ranef_index], coef_map[ranef_index])
  names(coef_map) <- coef_map
  rows <- data.frame(term = c("Fixed Effects", "Random Effects"))
  rows <- cbind(rows, rbind(rep("", length(models)), 
                            rep("", length(models))))
  length_fes <- length(coef_map) - length(ranef_index)
  attr(rows, 'position') <- c(1, (length_fes + 1) * 2)
  modelsummary::msummary(models, coef_map = coef_map, add_rows = rows, ...)
}
theme_set(theme_bw())  # Theme; just my personal preference
```

```{r setup-brms}
# (Optional) Set multiple cores in global option
options(mc.cores = min(parallel::detectCores() - 1, 2L))
```

This time, we'll use R to download the data (in ZIP) from the Internet (the link seems to have expired). 

```{r download_data, eval=FALSE}
download.file("http://dynamicpsychlab.com/download/tk3gypdr", 
              # save in the `data_files` folder
              here("data_files", "Fisher_2015_Data.zip"))
# unzip
unzip(here("data_files", "Fisher_2015_Data.zip", 
      exdir = "data_files")  # extract to the `data_files` directory
```

The data are in separate files for different persons, so we need to read all of
them in and combine them:

```{r fisher_data, message=FALSE}
# The data are all in CSV format, so let's get the list of file names:
(csv_names <- dir(here("data_files", "Fisher_2015_Data"), 
                  pattern = ".csv",  # find all files with '.csv' extension
                  full.names = TRUE,  # show the full directory
                  recursive = TRUE))  # search subfolders too
# Try read in one file
read_csv(csv_names[1], na = "999")  # 999 is missing
# Looks good. But it needs an additional ID column because we have 10 of them.
# We can use the folder name as the person ID, which can be obtained as:
csv_names[1] %>% 
  dirname() %>% 
  basename()
# Add this to the data:
read_csv(csv_names[1], na = "999") %>% 
  mutate(id = csv_names[1] %>% 
           dirname() %>% 
           basename())
# Now we need to do all. You can write a for loop, or use the `purrr::map()`
# function in tidyverse
fisher_data <- 
  map(csv_names, 
      ~ read_csv(.x, na = "999") %>% 
        mutate(id = .x %>% 
                 dirname() %>% 
                 basename(), 
               # add row number to indicate time
               time = row_number() - 1)) %>% 
  # Now join them together with the `bind_rows()` function
  bind_rows() %>% 
  # Rearrange the columns so that `id` is the first
  select(id, everything())
fisher_data
```

## Computing Scale Score

To simplify the analysis in this illustrative example, we'll average out the
scores for `happy`, `content`, `relaxed`, and `positive` for positive affect, 
and `angry`, `afraid`, `sad`, and `lonely` for negative affect. 

```{r scale-score}
fisher_data <- fisher_data %>% 
  mutate(pa = (happy + content + relaxed + positive) / 4, 
         na = (angry + afraid + sad + lonely) / 4)
```

We also need to do cluster-(person-)mean centering. **This is the current 
standard in intensive longitudinal data analysis!!**

```{r pmc}
fisher_data <- fisher_data %>% 
  group_by(id) %>% 
  mutate(across(c(pa, na), 
                # The `.` means the variable to be operated on
                list("pm" = ~ mean(., na.rm = TRUE), 
                     "pmc" = ~ . - mean(., na.rm = TRUE)))) %>% 
  ungroup()
```

## Visualize the Data

We can explore the data by plotting the individual trajectories

```{r traj}
fisher_data %>% 
  ggplot(aes(x = time)) + 
  geom_line(aes(y = pa), col = "blue") + 
  geom_line(aes(y = na), col = "red") + 
  facet_wrap( ~ id, ncol = 4)
```

We can also explore the within-person and between-person associations:

```{r p1}
p1 <- fisher_data %>% 
  ggplot(aes(x = na, y = pa, col = id)) + 
  geom_point(size = 0.7, alpha = 0.5) + 
  geom_smooth(se = FALSE) + 
  geom_point(aes(x = na_pm, y = pa_pm, fill = id), 
             shape = 24, col = "black", size = 2) + 
  geom_smooth(aes(x = na_pm, y = pa_pm, group = 1), 
              method = "lm", linetype = "dashed", fullrange = TRUE)
p1
```

# MLM with NA Predicting PA (Concurrently)

With the small sample size of 10 people , you'll need either Bayesian analysis
or small sample correction. I'll use `brms` below, with PA and NA rescaled 
to 0 to 5 by dividing by 20. 

```{r m1_brm, cache=TRUE, message=FALSE, results='hide'}
fisher_data_rescaled <- fisher_data %>% 
  # perform the same operation on multiple variables
  mutate(across(pa:na_pmc, ~ . / 20))
# First, check whether there's a trend in the data:
m0 <- brm(pa ~ time + (time | id), data = fisher_data_rescaled, 
          seed = 2313, 
          control = list(adapt_delta = .90))
# Use NA to predict PA
m1 <- brm(pa ~ na_pm + na_pmc + (na_pmc | id), data = fisher_data_rescaled, 
          seed = 2313, 
          control = list(adapt_delta = .90))
```

```{r msummary-m0-to-m2}
msummary_mixed(list("Trend" = m0, 
                    "NA --> PA" = m1), 
               statistic = "conf.int", 
               looic = TRUE)
```

## Visualize the results

```{r plot-predicted-m1}
# Show the predicted level one relationship
augment(m1) %>% 
  mutate(na = na_pm + na_pmc) %>% 
  ggplot(aes(x = na, y = pa, col = id)) + 
  geom_point(size = 0.7, alpha = 0.5) +
  geom_smooth(aes(y = .fitted)) +
  stat_summary(aes(x = na_pm, y = .fitted, fill = id), 
               shape = 24, 
               col = "black", 
               size = 2, 
               geom = "point")
```

```{r plot-m1, fig.width=12, fig.height=14}
# Show the posterior distributions of the parameters:
plot(m1, N = 7)
```

```{r mcmcplot-m1}
# Show the coefficients and credible intervals (useful for large number of them):
mcmc_plot(m1)
```

# Autoregression and Lagged Predictor

An autoregressive model asks the question: does observation at time $t$ 
predicts observation at time $t + 1$? This can be used, for example, to 
see whether people's mood linger for a while, or whether they quickly change. 
Below is a simple example using `pa` at previous time points to predict `pa` at
the current time point:

```{r lag-pa_pmc}
# Create lagged predictors
fisher_data_rescaled <- fisher_data_rescaled %>% 
  group_by(id) %>% 
  mutate(pa_pmc_lag1 = lag(pa_pmc)) %>% 
  ungroup()
# Confirm it's indeed a lag variable:
fisher_data_rescaled %>% 
  group_by(id) %>% 
  select(pa_pmc, pa_pmc_lag1) %>% 
  # First five rows for each person
  slice(1:5)
```

```{r m_ar, cache=TRUE, message=FALSE, results='hide'}
# Now, perform an autoregressive model
# 1. no random slope
m_ar <- brm(pa ~ pa_pmc_lag1 + (1 | id), 
            data = fisher_data_rescaled, 
            control = list(adapt_delta = .90), 
            seed = 2313)
# 2. with random slope
m_ar_rs <- brm(pa ~ pa_pmc_lag1 + (pa_pmc_lag1 | id), 
               data = fisher_data_rescaled, 
               control = list(adapt_delta = .90), 
               seed = 2313)
```

```{r loo-ar-rs}
# If you compare the LOO, there wasn't strong evidence for the random slopes
loo(m_ar, m_ar_rs)
```

## Adding negative affect (and lagged)

```{r m_ar_na, cache=TRUE, message=FALSE, results='hide'}
# Create lagged predictors for NA
fisher_data_rescaled <- fisher_data_rescaled %>% 
  group_by(id) %>% 
  mutate(na_pmc_lag1 = lag(na_pmc)) %>% 
  ungroup()
m_ar_na <- brm(pa ~ pa_pmc_lag1 + na_pm + na_pmc_lag1 +
                 (1 | id), 
               data = fisher_data_rescaled, 
               control = list(adapt_delta = .90))
# Add interactions between `na_pm` and `pa_pmc_lag1`
m_ar_na2 <- brm(pa ~ pa_pmc_lag1 * (na_pm + na_pmc_lag1) +
                  (1 | id), 
                data = fisher_data_rescaled, 
                control = list(adapt_delta = .90))
# Autoregression of PA is weaker for people with higher general negative
# affect
```

You can use the `conditional_effects()` function to plot the effects:

```{r plot-m_ar_na2}
plot(
  conditional_effects(m_ar_na2),
  points = TRUE,
  point_args =  list(
    width = 0.02,
    height = 0.02,
    alpha = 0.3,
    size = 0.6
  ),
  ask = FALSE
)
```

## Model Checking

This is to see whether the simulated data based on the model look similar to 
the observed data:

```{r ppcheck}
pp_check(m_ar_na2)
# Not too bad; missing the peak a little bit
```

```{r ppcheck-ribbon, fig.width=12}
# Let's check the time series:
pp_check(m_ar_na2, group = "id", type = "ribbon_grouped")
# Some extreme states have not been accounted for, for individual P009, 
# individual P022, for example
```

## Coefficient Table

```{r msummary-all}
msummary_mixed(list("M1: AR1" = m_ar, 
                    "M2: AR1 with NA" = m_ar_na, 
                    "M3: AR1 with NA interaction" = m_ar_na2), 
               statistic = "conf.int", looic = TRUE)
```

# Location-Scale Model

Sometimes one may not only be interested in whether mean negative affect predicts mean positive affect. In some area of research, the amount of fluctuation may be as important or even more important than the mean level (e.g., bipolar disorder). In `brms`, it is easy to also model the variation, using the **location-scale model** (see [Hedeker et al., 2008](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1541-0420.2007.00924.x)). In a model without predictors, we can write

Within-person level:
$$\text{PA}_{ti} = \beta_{0i} + e_{ti}, \; e_{ti} \sim N(0, \sigma_{i})$$
Notice now $\sigma_i$ has an $i$ subscript, meaning that each person can have a different amount of fluctuation over time. Then, at the between-person level, 
$$
  \begin{aligned}
    \beta_{0i} & = \gamma_{00} + u_{0i}  \\
    \log(\sigma_{i}) & = \gamma^{(\sigma)}_{00} + u^{(\sigma)}_{0i}, 
  \end{aligned}
$$
where $\gamma^{(\sigma)}_{00}$ is the average of the logarithm of $\sigma_i$s, and $u^{(\sigma)}_{0i}$ is the person-level deviation. We need the logarithm because $\sigma$ is non-negative. We then assume $u_{0i}$ and $u^{(\sigma)}_{0i}$ are jointly normal. 

To fit the model in `brms`, we use `bf()` to set multiple equations
```{r m0_ls, cache=TRUE, message=FALSE, results='hide'}
m0_ls<-
  brm(bf(pa ~ (1 | p | id),  # include the `p` for the covariance
         sigma ~ (1 | p | id)),  # predict standard deviation
      data = fisher_data_rescaled, 
      control = list(adapt_delta = .90), 
      seed = 2313)
# Compared to mean only
m0 <-
  brm(pa ~ (1 | id),  
      data = fisher_data_rescaled, 
      control = list(adapt_delta = .90), 
      seed = 2313)
```

LOO would strongly suggest that $\sigma$ is different across persons:
```{r loo-ls}
loo(m0_ls, m0)
```

Now let's include `na_pm` to predict $\sigma$

```{r m_ar_na2_ls, cache=TRUE, message=FALSE, results='hide'}
m_ar_na2_ls <-
  brm(bf(pa ~ pa_pmc_lag1 * (na_pm + na_pmc_lag1) + (1 | p | id), 
         sigma ~ na_pm + (1 | p | id)),  # predict standard deviation
      data = fisher_data_rescaled, 
      seed = 2313, 
      control = list(adapt_delta = .95))
```

```{r summary-m_ar_na2_ls}
summary(m_ar_na2_ls)
```

Not sufficient evidence that a person's mean negative affect relates to the amount of fluctuation. 

# Final Remark

In the data description, it was mentioned that the researchers collected the
data four times a day on four-hour intervals. Therefore, the last observation
from the previous day and the first observation on the next day has an 8 hour
interval, and it's probably not reasonable to expect the autoregressive effect
will be the same, as is assumed in our model. Also, after sleep it is possible
that the carry over from the previous day may fade. 

Relatedly, there can be time-of-day effect where positive affect may be higher
at a certain time of day, which can be modelled by creating a categorical 
predictor. There can also be day of week effect. 

# Additional Resources

Book by Bolger and Laurenceau (2013), Intensive Longitudinal Methods:
An Introduction to Diary and Experience Sampling Research: http://www.intensivelongitudinal.com/

Tutorials on "Intensive Longitudinal Data: Analysis of Experience Sampling and
EMA Data": https://quantdev.ssri.psu.edu/resources/intensive-longitudinal-data-analysis-experience-sampling-and-ema-data

